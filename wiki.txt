Category:coordination
 Liste des listes de diffusion 

 Emplacement 
Toutes les listes de diffusion sont décrites dans le document Listes de diffusion efluid.xlsm disponible dans la room efluid - Gestion de Projet

 Fonctionnement 
Le contenu de ce document est généré à la demande par requête sur le LDAP d'UEM, ce qui implique que :
 il faut être dans le réseau UEM pour le mettre à jour,
 il ne se met pas à jour tout seul, pensez à le faire quand vous demandez des modifications de liste de diffusion et/ou constatez qu'il commence à se faire vieux.

 Mise à jour 

Difficile de faire plus simple :
 télécharger puis ouvrir le document,
 activez les macros,
 sélectionner la feuille "Listes de diffusion" et cliquer sur "Actualiser les données" :
Fichier:Actualiser_la_liste_des_listes_de_diffusion.png
 sauvegarder,
 uploader vers l'emplacement de départ.<!!!>Category:coordination

 Procédure de lotissement générique d'une anomalie 

 un évènement d'anomalie est crée (client, recette, même développeur, peu importe),
 dans un domaine / contrat / version donnée
 avec pour statut ouvert ("ouvert interne" ou par un client)
 une double qualification est réalisée :
 de portée fonctionnelle (gravité) par l’expertise,
 de portée technique (nb de classes, risque de regr.) par le développement,
 avec ces deux éléments, la coordination réalise le lotissement, annonce la version prévisionnelle et affecte au développement.

 Cas d'un évènement ouvert par un client 
Ces évènements sont qualifiés par l'expertise; se reporter au paragraphe correspondant.

 Cas d'un évènement ouvert par l’expertise 
 l’expertise fait la qualification fonctionnelle (gravité) et contacte la coordination,
 la coordination relaie vers le développement pour qualification technique (classes)
 le développement transmet son évaluation,
 la coordination loti, affect à une équipe de développement 

 Cas d'un évènement ouvert par le dev 
 le développement fait la qualification technique (classes, risque) et contacte la coordination,
 la coordination relaie vers l’expertise pour qualification technique (classes)
 le développement transmet son évaluation,
 la coordination loti, affect à une équipe de développement<!!!>Catégorie:Coordination

 projets et plannings clients
 tableaux de bord
 logistique
 communication
 formations
 organisation
 procédures
 chantiers
 ramassages<!!!>Catégorie:Coordination
 Planning 
 Diagramme des versions (eroom) 
 Planning Client
 Liste des clients

Catégorie:Projets

 Tirs à Blanc et Mises en production 
Lors de chaque Tir à Blanc / Mise en Production, le pôle développement propose la mise à disposition d'une cellule d'assistance dédiée.

En cours :

 MULTIELD
SALLANCHES/BONNEVILLE - Tir à blanc Bascule
MULTIELD - Tir à blanc Migration n°3
MULTIELD - TAB MEP 1 - Sallanches et Bonneville
MULTIELD - Tir à blanc Migration n°4
MULTIELD - Tir à blanc Migration n°5 interne
MULTIELD - TAB MEP 3 - Enercom connexion
MULTIELD - Tir à blanc Migration n°6 interne
MULTIELD - Tir à blanc Migration n°7
MULTIELD - Tir à blanc efluid.net

 VIALIS
 Vialis - Tir à blanc lot 2 n°1
 Vialis - Tir à blanc lot 2 n°2
 Vialis - Tir à blanc lot 2 n°3 et n°4
 Vialis - Tir à blanc lot 2 MEP

Terminé :

 GEDIA
GEDIA - Tir à blanc V12 à isofonctionnalité

 RSEIPC
RSEIPC - Tir à blanc V12 à isofonctionnalité
RSEIPC - Tir à blanc Bascule n°1
RSEIPC - Tir à blanc Bascule n°2
RSEIPC - Mise en production Bascule

 VIALIS
 Vialis - Tir à blanc lot 1 n°1
 Vialis - Tir à blanc lot 1 n°2
 Vialis - Tir à blanc lot 1 n°3

 GEDIA EAU
 GEDIA EAU - Tir à blanc
 GEDIA EAU - Tir à blanc 2
 GEDIA EAU - Mise en production
 ES
 ES - Montée de version efluid 11.11
 GEG
 GEG - Recette RECOFLUX GAZ du 4 et mars 2013
 GEG - Tir à blanc lot 2 n°2
 GEG - Tir à blanc lot 2 n°3
 GEG - Tir à blanc lot 2 n°4
 GEG - Tir à blanc lot 2 n°5
 GEG - Mise en production lot 2
 UEM
 UEM - Tir à blanc SEPA

 Autres 

La :Catégorie:Projets contient également l'intégralité des pages référencées dans cette catégorie.<!!!>Catégorie:Coordination
 projet efluid 

 Nouveaux développement et capacitaire 

Ces tableaux sont orientés groupe de développement : le but est de visualiser et de suivre l'activité d'un groupe et de planifier les nouveaux développements.

Pour l'estimation du capacitaire basé sur l'évolution dans le temps de :
- la composition des groupes de développement
- l'évolution des congés
- le pourcentage d'activité associé aux nouveaux développement

Prevision_Capacitaire_efluid.xlsm

La notice du fonctionnement du fichier se trouve dans l'onglet ReadMe.
Il est mis à jour tous les matins à partir des données issues de suivefluid.

 Historique de l'activité par mois et par année 

Ces tableaux de bord permette de suivre l'activité :
 des anomalies de production par groupe, client, origine
 des anomalies par groupe
 des anomalies en cours par groupe
 des temps passés par groupe pour les activités : qualité, nouveau développement, anomalie, anomalie de production et études et prestations
 des tâches des agents du groupe

La documentation du fonctionnement du fichier se trouve dans l'onglet ReadMe.
Le tableau de bord est mis à jour toutes les fin de mois pour établir les statistiques mensuelles de suivi de l'activité.

Le tableau de bord est disponible pour :

 Le suivi de la v13 sur 2016/2017 : tdb_mensuel_efluid.xlsm 
 L'année 2013 qui est homogène avec l'année 2016 : tdb_mensuel_efluid_201312.xlsm
 L'année 2015 qui est homogène avec l'année 2018 : tdb_mensuel_efluid_201512.xlsm

 Requêtes de contrôle de cohérence 

Pour s'assurer que les données de suivefluid sont correctement renseignées, des requêtes de contrôle de cohérence sont réalisées à des rythmes hebdomadaire ou mensuel.
L'ensemble de ces requêtes est présenté dans la page suivante : 
Contrôles de cohérence<!!!>Catégorie:Coordination
 Se déplacer chez de clients 
 Voir se déplacer avec efluid SAS.pptx dans eroom 

 UEM Pontiffroy 
 Plans des locaux UEM Pontiffroy
 Salles de réunion UEM Pontiffroy

 CGI Carré Playel 
 Accès CGI Carré Pleyel

 audio et visio 
 Salles_Visio<!!!>Catégorie:Coordination
Ecrire Un Courriel - Les Bonnes Pratiques<!!!>Catégorie:Coordination
 Organigrammes  

Les organigrammes de la société efluid SAS sont disponible sur cette page.

 Responsables de projets client 

La liste des responsables des projets pour chacun des clients est disponible sur cette page.<!!!>category:coordination
category:procédure
Document pour les réserves (RESERVE) disponible sur eroom, dans Mes eRooms > efluid - Qualité Développement > Guides et procédures > procédures > coordination.<!!!>Category:Coordination
Ce modèle est à utiliser uniquement dans le cadre de la livraison de script(s) SQL ponctuel(s). Dans le cas où plusieurs scripts doivent être livrés par un même développeur, il faut envoyer un mail par script.

 Modèle de mail 

Bonjour,

pouvez vous livrer le script de référence <référence ebuild> attaché à l'événement <référence suivefluid> pour validation par la recette ?
 Niveau d’urgence : <priorité> ;
 Environnement de recette : <environnement cible : INT REC MACHIN, PROD BIDULE etc... > ;
 Volumétries attendues : <nombre de références mises à jour : 123, plusieurs milliers, etc ...> ;
 Durée prévisionnelle du script : <ordre de grandeur : quelques secondes, plusieurs heures etc ...> ;

Merci.<!!!>Category:Coordination
Category:Report de code

Vous trouverez ci-dessous une demande de report concernant l’évènement XXXXXX.

 Version(s) souhaitée(s) de correction : v12.x.y.RCz
 Classes modifiées : 
 repo toto : 
 aaa/bbb/ccc.java
 aaa/bbb/ddd.java
 repo titi : 
 eee/fff/ggg.java
 eee/fff/hhh.java
 Raisons de cette demande : 
 Impact client de l’évènement : (à compléter)
 Existence d’une solution de contournement : (à compléter)
 Volumétrie des cas concernés (le cas échéant) : (à compléter)
 Origine de la demande : (à compléter)
 Risques de régression : (à compléter)
 Concerne ERDF : Oui/Non<!!!>category:coordination
category:versions

La gestion des versions dans Maven peut se faire de plusieurs façons, voici notre manière de procéder pour la suite efluid :

 Schéma global des branches et versions 
Fichier:Gestion des branches et versions.png

Visio : http://wperoom2.uem.lan/eRoom/Production/QualiteDeveloppementEfluid/0_107a60  => Gestion_des_branches_et_versions.vsd

 Gel d'une branche 

Lorsqu'une branche est gelée (dans Gerrit le bouton submit sur le changeSet n'apparait pas), il faut suivre le procéssus suivant : 
Gerrit#Workflow_de_soumission_de_code_en_cas_de_code-freeze

Quand une branche est-elle gelée ? Généralement dans les cas de figure suivant, mais cela peut changer en concertation entre UL et coordination afin de bloquer le moins possible les commits tout en garantissant au mieux les livraisons clients.
 branche de production (exemple : maintenance_13.8.300) : gelées de manière permanente
 branche de maintenance (exemple : maintenance_13.8) dont dérivent des branches de production : gelée dès besoin de stabilisation
 branche de développement (develop) : gel dès besoin de stabilisation (souvent entre RC1 et RC2, puis dans les dernières RC)

Un gel peut être également positionné à la demande.

 Correspondance entre branche des composants et suite efluid 
 Matrice des branches utilisées dans les composants efluid

 Correspondance version lotie / branche git 
Un événement est doté d'un lot prévisionnel majeur et d'un lot prévisionnel mineur.

Le majeur indique où livrer, :
 un événement loti "efluid xxx" est à livrer sur la branche develop,
 un événement loti "efluid maint. yyy" est à livrer sur la branche de maintenance "maintenance_yyy",

Le mineur indique quand livrer :
 un événement avec version mineure doit être livré après que la version mineure précédente ait été ramassée, mais avant que la version mineur cible ne soit ramassée (évidemment).
 un événement sans version mineur peut être livré à tout moment sur la branche correspondant à la version majeur.

Les versions majeures et mineures d'une branche sont indiquée par la version renseignée dans le pom de la branche. On ne peut pas livrer sur une branche donnée des événements dont les versions prévisionnelles majeures et mineures ne sont pas exactement égales à celles du pom (ou éventuellement non précisées dans le cas de la version mineure).

 Convention de nommage des versions  
 La convention à respecter est la suivante :
 Les 3 premiers digit doivent être des entiers positifs uniquement (pas lettre, pas de 0 devant le chiffre, exemple : 01, pas de caractères spéciaux)
 A partir du 4eme digit on peut utiliser ce que l'on veut, car il s'agit du "qualifier". Exemple : 12.1.10.RC1, 12.1.10.beta1
 Les digits sont séparés par des point

 Nomenclature de version de développement 
 Une version développement est de la forme : 12.1.100.RC1-SNAPSHOT
 Les numéro sont identiques aux version release et release candidate
 On suffixe en plus par la mention -SNAPSHOT

 Nomenclature de version release candidate 
 Une version release candidate est de la forme : 12.1.100.RC1
 Les 3 premiers numéro sont identiques à la version release
 Le 4eme numéro est le numéro de release candidate, toujours préfixé de RC

 Nomenclature de version release 
 Une version release est de la forme : 12.1.100
 Le premier numéro correspond au lot Majeur
 Le second numéro correspond au lot Mineur
 Le troisième numéro correspond à la Maintenance

 Nomenclature de version proto 
 Une branche suiteEfluid pour un proto : proto_<nomProto>_<version>
 Numéro de version suiteEfluid pour un proto : <version>-PROTO-<nomProto>-SNAPSHOT
 Même chose pour les briques

 Cycle de vie d'une version 

 Version SNAPSHOT 
 Les versions SNAPSHOT sont des versions qui indiquent que l'on est en cours de développement (à la différence d'une version release)
 On nomme une version SNAPSHOT de la manière suivante : 12.1.100-SNAPSHOT
 On conserve le numéro de version dans le SNAPSHOT de manière à savoir quelle version on est en train de construire

 Version RELEASE 
 Une version release est une version ramassée et tagguée, et elle ne bougera plus
 La convention Maven indique qu'on ne modifiera jamais une version Release
 Celle-ci porte donc un numero de version unique

 Edition d'une version de production 

La création d'une nouvelle version de production se déroule en trois principales étapes :
 création de l'événement support de l'assemblage et information de l'UL pour planification
 information des développeurs pour développement / intégration des patchs selon le process de publication de code lorsqu'une branche est gelée (car une branche de production est toujours gelée); cf Gerrit#Workflow_de_soumission_de_code_en_cas_de_code-freeze pour le process détaillé
 suivi de l'avancement par la coordination puis go d'assemblage quand la version est conforme aux attentes.

Dans les détails :
Fichier:Assemblage version de production.png

(Soumission patch code freeze.vsd dans Mes eRooms > efluid - Qualité Développement > Guides et procédures > guide environnement de développement > schéma visio)

 Périmètres et responsabilités 

 Préparation 
La Coordination définis les jalons d’assemblage des versions de l’application par via une série d’événements suivefluid, en accord avec l’équipe UL (requête suivefluid 1787).
L’équipe UL est chargée de positionner le ramassage des briques correspondantes selon les pratiques habituelles.
La Coordination transmet/confirme au responsable d’application la liste des événements attendus dans le lot.

 Planification des assemblages 

 Source de l'information 
Suivefluid est réputé référence de la planification des assemblages.

Pour éviter une duplication de l’information, il est prévu d’abandonner la maintenance du planning eroom. En attenandant la mise en place d'un éventuel système de représentation du planning dans le wiki, utiliser les threads du forum dédiés aux assemblages et/ou aux points hebdomadaires.

 Diffusion de l'information 
 Périodiquement via le point hebdomadaire du lundi : réalisation des slides avec le planning UL général (toutes version) et détaillé (y.c. briques) de la semaine comme entrant, report dans le forum post tenue du point (exemple : http://eforum.uem.lan/viewtopic.php?f=42&t=1492 )
 Spécifiquement via forum / wiki pour la feuille de route d’une version donnée avec le planning UL d’une version spécifique comme entrant (exemple : http://eforum.uem.lan/viewtopic.php?f=42&t=1477 / http://wikefluid/index.php/Cycle_de_vie_des_versions http://wikefluid/index.php/Tableaux_de_bord_D%26T) 

 Maintenance de l'informaiton 
Réalisation par l’UL et/ou Coordination.
L’UL est le point d’entrée « planification » vis-à-vis de la Coordination, et communique avec NRU en cas de création/modification/annulation.

 Suivi 
Le responsable d’application réalise la supervision de l’avancement du lot.
 Assemblage 
Synchronisation préalable entre Coordination et le responsable d’application pour évaluer la complétion du lot.
La Coordination demande l’assemblage à l’UL via l’événement d’assemblage suivefluid. Assemblage demandé de manière à ce qu’il soit réalisé au plus prêt de l’heure prévue.

 Cas des ramassages exceptionnels 
Il s’agit des versions demandées pour déploiement immédiat (anomalie bloquante en recette, etc …).
La synchronisation est gérée en direct entre UL et Coordination :
 La Coordination se rapproche de l’UL pour présenter le contexte et définir la stratégie de ramassage
 La Coordination formalise le besoin via un événement d’assemblage confirmant la stratégie et le périmètre
 L’UL s’assure de la livraison des changetSets attendus, assemble dès que le périmètre est complété

 Points à remettre dans la documentation 
 Le nommage du numéro de maintenance débute toujours par 100
 La suite du numéro de maintenance 900 est 1000
 Exemple : on release 12.10.900, alors la prochaine version est 12.10.1000, puis la prochaine sera 12.10.1100 et ainsi de suite.
 On conserve 2 niveaux de branches de maintenance maximum
 Le second niveau de branche de maintenance ne doit être utilisé qu'en cas de force majeure :
 Événement bloquant (qui bloque le fonctionnement nominal de l'application)
 Le client ne souhaite pas la version de maintenance suivante
 Si un client possède la version 12.10.100 et un autre la version 12.10.101, alors si les deux souhaitent une correction de bug bloquant, ils ne pourront avoir que la version 12.10.102, il n'est pas possible d'avoir une version intermédiaire. (Note : ils peuvent aussi avoir une 12.10.200)
 Schéma à ajouter<!!!> les modèles de documents sont disponible sous eRoom
 diagrammes fréquemment utilisés
 diagramme des robinets
Catégorie:Coordination<!!!>Category:coordination

[DEPRECATED]

Les contraintes guidant la conception du planning de ramassage eroom.

 Découpage des journées 

Chaque journée est découpée en 8 créneau d'assemblage :
 matin
 8h -> 9h
 9h -> 10h
 10h -> 11h
 11h -> 12h
 après-midi
 14h -> 15h
 15h -> 16h
 16h -> 17h
 17h -> 18h

Il ne peut y avoir qu'un assemblage par créneau.

 Demande d'assemblage 

Les assemblages passent par un événement suivefluid dédié. Ces événements peuvent être retrouvés via la requête n°1787 "SMC - livraison à venir".

 Planification des assemblages spécifiques à une version 

On appelle :
 jalon principal : la RC1 d'une version.
 jalon de stabilisation : les RCx d'une version.
 jalon briques : toute release de brique en lien avec un jalon primaire ou secondaire.

 Timing 
 timing de positionnement des jalons principaux 

On positionne dès qu'ils sont connus. Horaires selon ce qui est décrit dans la section "Ramassages périodiques".

 timing de positionnement des jalons de stabilisation 

On positionne dès qu'ils sont connus, ce qui arrive généralement une à deux semaine avant l’occurrence du jalon principal de référence. Horaires selon ce qui est décrit dans la section "Ramassages périodiques".
Les jalons de stabilisation se terminent tous en "RCx" de manière à ne pas avoir à les modifier en cas de ramassage non programmé d'une RC.

 timing de positionnement des jalons brique 

On ne positionne les jalons briques (décrits ci dessous) que pour la prochaine RC1 d'une branche donnée de manière à ne pas trop anticiper la planification. Exemple sur la develop : la prochaine RC1 est la 12.13.100.RC1, je positionne les jalons secondaires de cette RC1 , mais pas ceux de la 12.14.100.RC1 ni de la 12.15.100.RC1.

 Planification 

 jalons pré RC1 

 toute version 

Positionnement de l'intégralité des jalons des briques selon la section "Ramassages périodiques".

 cas RC1 sur develop 
 J-1 : jalon "nettoyage de suivefluid (cohérence événements livrés / version prévisionelle)" et "stabilisation de l'appli (TI, NB)". 
 J-2 : jalon "gel des dépots de code" (18h00). 
 J-3 : jalon "ramassage des briques embarquées dans la version" selon les horaires suivants :
 archi/fmk : 9h00
 EDK : 11h00
 ecore : 14h00

 cas RC1 sur la maintenance 
Pas de jalon spécifique, juste préciser que les dernier ramassages de briques concernant la branche seront celles embarquées dans la version.

 cas RC1 sur une branche de production 
Pas de jalon spécifique, ramassage des briques selon les besoins et disponibilités.

 jalons post RC1 

 toute version 

Positionnement de l'intégralité des jalons des briques selon la section "Ramassages périodiques" jusqu'à la dernière RCx ("FINALE") de la branche.
Positionnement de l'intégralité des jalons des briques selon la section "Ramassages périodiques" jusqu'à la prochaine RC1 de la branche d'où est issue la RC1 courante.

 Ramassages périodiques 
Rappel du cycle de vie des RC sur la page dédiée Gestion des versions.

 efluid 

 par composants
 archi : lundi
 EDK : mardi
 ecore : mercredi
 efluid : jeudi
 par version
 develop : 14h00
 stabilisation develop : 10h00
 maintenance_12.10 : 16h00<!!!>Category:coordination
Category:intégration
Category:procédure

Cette page décrit l'organisation des cursus d'intégration des nouveaux arrivants.

 Entrants 
 Nom, prénom
 date de début et de fin de contrat (le cas échéant)
 Cadre ou non
 entreprise (efluid sas, CGI, autre prestataire)
 Poste occupé

 Livrables 

une checklist d'intégration
Son but est de piloter les différents acteurs (sysres, 6000, formateurs) dans les créations, réservation de PC (virtuel ou pas), habilitations, opérations du nouvel arrivant.
Procédure de création : Checklist nouvel embauche.

un planning d'intégration
Il décrit jour par jour les formations suivies par le nouvel arriavnt au sein d'efluid.
Procédure de création : Planning d'intégration.<!!!>category:coordination
category:procédure

 Contexte 
La division étude et performances peut, à la demande, mettre à la disposition d'un groupe de travail une instance "privée" (les accès ne sont pas publiés) et lui permettre de réaliser des développements/tests/validation sans perturber les autres groupes. Cette facilité reste assez lourde à mettre en oeuvre, aussi elle n'est généralement réalisée que pour des développements longs, structurants ou ou ayant besoin de contextes maîtrisés.

 Interlocuteur étude et performances 
Ces mises à disposition sont gérée par Renaud Wozniak, et Mickael Back en son absence.

 Prérequis 
Les entrants de cette procédure sont les suivants :
 Le besoin précis, qui sert à évaluer la pertinence de la demande
 La source de donnée à utiliser comme base pour l'instance; il peut s'agir d'une copie de base client sous condition d'être autorisé à la sortir de chez le client, et qu'elle soit d'une taille raisonnable. 
 Le responsable de l'instance, auquel seront communiqué les accès et qui sera sollicité quand l'instance arrivera à expiration,
 La date de fin du besoin.

 Procédure à suivre 
 Créer une demande via suivefluid (voir modèle ci-dessous),
 La transmettre à un des interlocuteurs,
 Si la demande est réalisable, l'interlocuteur monte l'instance et communique au responsable les modalités d'accès,
 A la date de fin, l'interlocuteur contacte le responsable pour prolongation éventuelle, ou destruction.

 Modèle d'événement suivefluid 
 type : livraison d'environnement
 statut : affecté développement
 groupe fonctionnel : (non applicable)
 groupe technique : étude et performances
 domaine : (non applicable)
 analyste : la personne réalisant la demande
 développeur : non renseigné
 recetteur : le futur responsable de l'instance
 contrat, projet, application : selon besoin
 version prévisionnelle : sans
 description :
  <décrire le besoin de manière approfondie>
  
  Base source : <indiquer la base source>
  A héberger sur LDBDDEDT1, instance DTSTEDT.
  Durée de vie : <indiquer la date de fin du besoin>.
  Responsable : <indiquer le responsable>.
  Nom : <indiquer un nom à utiliser pour l'instance>
  Usage : <indiquer le besoin><!!!>category:procédure
category:coordination
 Procédure d'établissement du périmètre fonctionnelle d'un lotissement 

 Première phase : collecte des besoins 

Les superviseurs recette et responsables de groupes de développement soumettent à la coordination fonctionnelle les besoins de report, au fil de l'eau.

 Deuxième phase : consolidation 

La coordination fonctionnelle lotit dans suivefluid les demandes de report :
 en précisant la version prévisionnelle,
 en ajoutant un commentaire dans l'événement.

Durant cette phase qui peut durer quelques jours, des événements peuvent être lotis sans que les personnes en charge (développeurs, recetteurs) n'en soient notifiées.

 Troisième phase : validation 

La coordination extrait de suivefluid un fichier consistant un premier périmètre. Ce fichier et la date de ramassage souhaitée sont transmis aux superviseurs recette et responsables de groupe pour validation :
 de faisabilité technique,
 de livraison dans les temps impartis,
 de complétude par rapport aux attentes client. 

Coté fonctionnel, chaque superviseur est responsable de s'assurer que le périmète contient les événements attendus.

Coté développement, chaque responsable de groupe s'engage sur la faisabilité dans les temps impartis, propose un délais supplémentaire si ça ne passe pas dans les délais attendus, et les événements sur lesquels aucun engagement ne peut être pris du fait d'un obstacle technique.

Les retours sont attendus sous 24h et soumis à la coordination. Des itérations supplémentaires peuvent être programmées si besoin.

 Quatrième phase : évolution 

Des anomalies peuvent éventuellement être ajoutées au fil de l'eau post-validation. Le principe reste le même, à deux exceptions :
 le fichier de périmètre n'est pas à nouveau transmis,
 les responsables sont notifiés par email par la coordination.
Les autres points (en particulier faisabilité technique et temporelle) s'appliquent.<!!!>Category:coordination
Category:intégration
Category:procédure

 Résumé 

Ce document est édité pour chaque nouvel arrivant, et décrit les étapes à suivre pour configurer le SI pour l'accueillir.

Les fiches sont (stockées ici : Mes eRooms > efluid - Gestion de Projet > Intégration nouvelles ressources > Droits d'accès).
Les différents droits d'accès sont codifiés par une note de cadrage rédigée par le chef du service EF.

Il existe 5 répertoires dans cet emplacement :

 Principes => les process et modèles d'accueil 
 01 – à préparer => les checklists en cours de création ;
 02 – à traiter => les checklists à diffuser ;
 03 – En cours => les checklists dont il faut suivre le renseignement des dates de réalisation ;
 04 – Déjà traité => les checklists dont le suivi est terminé

Se reporter au process pour plus d'information.

 Quelques recommandations sur les champs à renseigner 

S'assurer que tous les champs sont renseignés, en particulier les dates.

 Mission 
Le trigramme final peux différer; pour limiter le risque le rechercher dans l'annuaire Exhcnage, s'il n'existe pas c'est qu'il est disponible.

 Affectation 
RAS

 Suivefluid 
CDR : dans le doute, se rapprocher du chef du service concerné.

 eldap 
Bien renseigner le profil de référence, c'est lui qui conditionne l'accès aux autres systèmes, en particulier les dépôts gerrit, les listes de diffusion.
S'il manque des groupes au profil par défaut, les indiquer via "Groupes additionnels".

 gerrit 
S'il manque des dépôts par rapport au profil par défaut (ecore, edk etc ...) les indiquer via "Dépôts additionnels".

 eroom 
S'il manque des rooms par rapport au profil par défaut (ethaque, etc ... ) les indiquer via "Rooms additionnelles".<!!!>Category:coordination
Category:intégration
Category:procédure

Le programme se déroule sur 8 semaines.

Le cursus est (stocké ici : Mes eRooms > efluid - Gestion de Projet > Intégration nouvelles ressources). Sous-répertoire "Profil *" selon le profil de la personne.

Le modèle de cursus est stocké ici (Mes eRooms > efluid - Gestion de Projet > Intégration nouvelles ressources).

 accueil 
 SPRH
 récupération / installation du poste de travail

 présentation 
sur 1 jour :
 de la société (par le directeur général)
 de l'activité (par le chef de service)
 du groupe (par le supérieur)
 des équipes (par le supérieur)

 formations fonctionnelles 
 immersion métier 
A organiser avec V. Mizzon pour compléter son inscription dans le fichier adéquat. 

sur 3.5 jours :
 Métier Interlocuteur Téléphone Bureau Consignes UEM : accueil physique et téléphonique Angélique Toutain 3186 Accueil Max 2 participants par session (deux casques d'écoute) URM : gestion du matériel Jean-Luc Schmitt 4570 309 Max 5 participants par session URM : gestion relève Jean-Luc Becker 4572 309 Max 6 participants par session, uniquement domaines techniques UEM : référentiel / mise à jour BT  Lionel Fiacre 4458 138 / URM : gestion intervention Roland Dorr 4521 307 Max 5 participants par session UEM : gestion facturation /principes généraux/facturation BT Claude Martin 3913 139 / UEM : recouvrement cac Francine Bariatti 4414 147 /

 socle commun efluid 
Compléter son inscription dans le fichier adéquat.

Modules plus ou moins obligatoires selon cursus, durée de 1 jour sauf mention contraire. Les modules en gras constituent le socle commun et sont obligatoires. Les autres modules peuvent être suivis plus tard.

12.5 jours répartis ainsi :

 ergonomie/référentiel
 contrat
 matériel/relève
 intervention
 facturation (2 jours)
 cac/réglement
 relance/contentieux (1/2 journée)
 éditions perso
 ael (1/2 journée)
 efluid.net / portail GRD
 workflow
 offre
 requeteur
 suivi via suivefluid (1/2 journée)

 immersion recette 
Une semaine en immersion dans le groupe de recette correspondant au domaine de travail.

Le point d'entrée est le superviseur du groupe recette.

 formations techniques 
 outillage 
sur 2.0 jours :
 gerrit
 maven
 suivefluid et gestion doc

 framework 
Sur 5 jour.

Fiche de formation - framework efluid

 formation tests unitaires 
Sur une journée. à réaliser après minimum 3 mois après la date d'entrée.

Fiche de formation - tests unitaires

 formation batch 
Selon besoins. Sur 3 jours. à réaliser après minimum 6 mois après la date d'entrée.

Fiche de formation - batch v3

 autoformation efluid 
Se fait en deux jours pleins, peut être fait en temps masqué (retour de formation par ex.)

Fiche de formation - autoformation efluid

 assistance dev / lecture d'AFD 
Géré par le resp. de l'équipe de dev. Entre 5 et 15 jours selon domaine.

 suivi sur 18 mois 
10 entretiens :
 entretien n°1 : date d’entrée + 1 mois
 entretien n°2 : date d’entrée + 2 mois
 entretien n°3 : date d’entrée + 3 mois
 entretien n°4 : date d’entrée + 4 mois
 entretien n°5 : date d’entrée + 5 mois
 entretien n°6 : date d’entrée + 6 mois
 entretien n°7 : date d’entrée + 9 mois
 entretien n°8 : date d’entrée + 12 mois
 entretien n°9 : date d’entrée + 15 mois
 entretien n°10 : date d’entrée + 18 mois

Les imputations se font sur un événement créé pour l'occasion :

 événement de suivi 
à créer.

 intitulé "DEV accompagnement nouveau développeur - XXX (Prénom NOM)"
 type conduite du changement
 statut ouvert en interne
 groupe technique selon groupe d'accueil
 contrat selon qui prend en charge (titulaire/régie : efluid, forfait : CGI par ex.); idem pour le client
 contenu : 
  Evénement support pour l'accompagnement et la montée en compétence de TBE (Tom BESNARD), à utiliser pendant le programme de suivi,
  c'est à dire jusqu'aux 18 mois d'ancienneté.
  
  A imputer sur cet événement
  
  - Assistance : Par exemple, pour le cas d'un événement d'une anomalie ou d'une nouvelle fonction affecté au développement accompagné,
    la charge d'assistance de l'accompagnant ne sera pas imputée sur l'anomalie ou la nouvelle fonction, mais sur l'événement d'accompagnement. 
  - La revue de code : 
    - Le relecteur imputera le temps nécessaire à la revue de code d'un nouvel arrivant sur l'événement d'accompagnement. 
    - Le temps passé par le nouvel arrivant à corriger les retours de la revue de code sera imputé comme développement sur l'anomalie ou le
      nouveau développement associé. 
  - Entretiens de suivi mensuels puis trimestriels 
    - L'encadrant imputera le temps passé à préparer et à réaliser ces entretiens sur l'événement d'accompagnement. 
    - Le nouvel arrivant imputera le temps consacré à ces entretien sur un autre événement, a priori de type pilotage 
  
  Contrexemples de travaux qui ne doivent pas être imputés sur cet événement
  - Les charges du développeur nouvellement arrivé : Cet événement est dédié aux accompagnants, il ne doit pas être utilisé par l'accompagné. 
  - La formation interne, notamment les formations du parcours d'intégration : Par exemple, la formation Git / Maven / Suivefluid, ainsi que
    les formations fonctionnelles du parcours d'intégration seront imputés parle nouvel arrivant et par le formateur sur un événement de formation,
    et non sur l'événement d'accompagnement du nouvel arrivant.
  
  cf http://WPEROOM2.uem.lan/eRoom/Prod11/DocFonctionnelleSuiteEfluid/0_f11<!!!>Category:Réunions
Category:Coordination

 Concept 
Cette réunion vise à distribuer aux différents groupes techniques :
 l'activité des branches : prochain jalon de chaque branche active,
 des nouveautés sur le développement proprement dit,
 l'actualité client,
 des informations sur l'actualité formation / RH,
 d'éventuels points d'information divers.

 Participants 
L'information est relayée par les chefs de filières ou pôles des services Développement et Technologie, qui peuvent choisir de déléguer. Actuellement (juin 2016) :
 Présence obligatoire :
 THIBAUT, Christophe
 DEBLOUWE, Nicolas
 DAMESTOY, Brice
 BACK, Mickael
 POCHAT, Benjamin
 DMYTRYK, Alexis
 PROUVE, Julie
 WEBER, Pierre
 GUILLEMOT, Christelle
 COLLIGNON, Thomas
 GRZEJSZCZAK, Didier
 Présence facultative :
 GUY, Grégoire
 BODIN, David
 ABDELKAFI, Khaled

 Préparation 
La partie "Ramassage" et la partie "Technique" du support de réunion sont rédigés par l'UL, et doivent être livrées le jour de la réunion pour 8h00.
Les autres parties (client, RH, divers) sont rédigés par NRU.
Les slides sont uploadés sur eroom (Mes eRooms > efluid - Gestion de Projet > Planification : http://wperoom2.uem.lan/eRoom/Production/GestionProjetEfluid/0_b2dab).

 Déroulement 
La réunion dure 15 minutes, le premier jour de chaque semaine, de 8h45 à 9h00. Elle a lieu à Metz, généralement en salle efluid R3 Cube.
Pour les personnes ne pouvant être présentes, une conférence téléphonique est mise à disposition : 0 800 105 080 code 29 62 56 89.
Accès organisateur : 0 800 105 080 code 40 17 58 14.
5 minutes avant le démarrage de la réunion, les slides sont transmis par email aux participants.
Les slides sont relayés sur le forum post-réunion (cf "Débrief").

La présentation est assurée par TCO (planning / actualité technique) et JLT (autres parties).

 Débrief 
Ils sont relayés avant 10h00 sur le forum (dans Index du forum ‹ équipes ‹ équipes de développement : http://eforum.uem.lan/viewforum.php?f=42), s'inspirer des postes précédents ("[POINT HEBDO] Semaine YYYY-WW) pour le formalisme.

Méthode rapide pour l'injection dans le forum :
 récupérer l'URL des slides de la semaine ("copier un lien" sur les slides dans eroom)
 créer un nouveau topic nommé "[POINT HEBDO] Semaine YYYY-WW" dans Index du forum ‹ équipes ‹ équipes de développement
 y indiquer le texte suivant : 
  Bonjour à tous,
  
  les slides de cette semaine sont disponibles sur [url=http://wperoom3.uem.lan/eRoom/Production/GestionProjetEfluid/0_b2dab]eroom[/url] (Mes eRooms > efluid - Gestion de Projet >   Planification) : <url des slides>
  
  Nicolas.
 publier

 à mettre sur les slides 

semaine 2016-05
 formation tu 2017 : http://eforum.uem.lan/viewtopic.php?f=42&t=2042
 formation batch 2017 : http://eforum.uem.lan/viewtopic.php?f=42&t=2043<!!!>Catégorie:Coordination

Requêtes définies dans le requêteur pour les contrôles de cohérence (suivi CC), suivi opérationnel (suivi OP) et pour analyse (suivi AN).

 quand requête exécution différée nom description 1er de chaque mois 2139 327 Suivi CC - livrés recette sans recetteur Recherche d'événements livrés recette avec trigramme recetteur vide hebdomadaire 3084 n'existe pas VersionsEnCours mise à jour de la liste des versions par extraction suivefluid des LivraisonEnvironnement lundi 3101 n'existe pas Suivi CC - Livraison d'AFD - 13.x Requete de suivi des livraisons d'AFD pour evts lotis 13.x : date de rédaction de l'AFD vide ou < date du jour lundi 1895 329 Suivi CC - événements avec charge à 0 - 13.x Recherche des développements lotis 13.x dont la charge prévisionnelle est à zéro. lundi 3107 322 Suivi OP - quantités imputées de J-7 à J-1 Recherche des agents ayant des trous d'imputations la semaine précédente. L'agent est indiqué s'il n'a pas imputé 5 jours complets (=> attention aux semaines ave jours fériés). Le champs "référence" contient la durée imputée. lundi 3131 330 Suivi CC - affectés dev 13 et 13plus affecté développeurs et lotis efluid-13 ou efluid-13+ => A partir du moment où ils sont affecté développeur c?est qu?on peut les développer et il faut à ce moment là les lotir. vendredi 3147 324 Suivi AN - prod bloquant  bloquants prod créés il y a moins de 7 jours qui sont au statut différent de pris en charge, abandonné ou ouvert client lundi 3148 331 Suivi OP - Reports non traités Extraction des evts livrés dans la version principale et avec un rôle statut à livrer dans d'autres versions lundi 2876 n'existe pas Suivi CC - Ano production origine non renseignee anomalies de production dont l'origine n'est pas renseignée quotidien 1661 159 Suivi OP - Congés du jour  Indique les agents absents au moins une partie de la journée. lundi 3166 333 Suivi OP - Refus de livraison Enedis Trace les refus de livraison enedis, en se basant sur les étiquettes positionnées manuellement (équipe maint.) "refus livraison enedis". quotidien 3200 337 Suivi CC - Livrés sans accord coordination ces événements correspondent à la recherche « livrés sur une version (onglet version) autre que develop sans validation coordination ». On s?est concentré sur les événements au statut « livré IT ? assemblage » dans un premier temps.  hebdomadaire 3226 342 Suivi CC - événements dev avec RAF nul et charge non nulle événements avec une charge prévi de dev non nulle (=> il a été prévu de livrer quelque chose), RAF à zero (=> il n'y a plus rien à livrer) mais restant affecté dev hebdomadaire 3322 343 Suivi CC - nc interne de plus de 7 jours Permet d'identifier les NC internes (statut transitoire) stagnants. hebdomadaire 3323 344 Suivi CC - affectés développement avec développeur Recherche les événements affectés équipe de développement avec développeur positionné. ponctuel 3391  Evenements 12-13 - chiffrage Extraction des evts de type nouveau développement lotis v13 à lotir dans sous version ou avec chiffrage prévisionnel à 0. ponctuel 3399  Evenements a livrer pour une version/ramassage Extraction des evts a livrer pour une version. quotidien 3424 360 suivi integration continue suivi des anomalies remontées par l'intégration continue QTP
 

=> 3147 : filtrer le statut pour ne pas partir trop tôt dans l'analyse.

 Tous les mois 
 Validation de la structure des groupes techniques pour les 3 mois à venir
 Validation du pourcentage affecté au nouveaux développement pour chaque groupe

 à faire 
 suivre l'évolution de la charge prévisionnelle et du RAF par version (hebdo) => besoin historisation (RAF ou charge selon):
 table d'histo : DateAnalyse / Version / Groupe / ouvert / analyse / affecte dev
 collecte : Requête insert into select… pour ajout des données dans la table toutes les semaines Lancée le lundi
 analyse : Requête select pour lecture des valeurs sur les 2 dernières semaines (where dateAanalyse = Now -14j.) lancée le mardi
 requête des événements livré avec version non à jour dans toutes les versions
 requête qui regarde quels sont les événements détectés en production qui passe par le statut « non conforme client » avec la date de passage dans ce statut 

 Idées 
 Découpages techniques dont le projet n'est pas le même que le projet de l'événement parent
 Evenements ouverts lotis dans une version qui n'est plus en prod
 au status 'à corriger en production' depuis plus d'un mois
 au statut 'non conforme client' depuis plus d'un mois
 au statut 'non conforme interne' depuis plus d'un mois
 au statut 'AFD validée' depuis plus d'un mois
 Gravité 'bloquant' depuis plus d'un mois
 Permanent dont la dernière imputation remonte à plus d'un an
 Permanent sans imputation
 Evenements ouverts interne présentant des imputations de type développement
 en cas de report : demande systématique pour maintenance_12
 livrés IT avec RAF > 0
 livrés sur une version de lotissement
 3107 : prise en compte des jours fériés
 événements avec chiffrage non détaillé<!!!>Catégorie:Coordination

 /!\ PAGE OBSOLETE /!\ - LE PROCESSUS D'ANALYSE DES ANOMALIES EST DECRIT ICI : http://wikefluid.uem.lan/index.php/Guide_Suivefluid_:_Analyse_des_anomalies 

''' - LE PROCESSUS D'ANALYSE 5 POURQUOI EST DECRIT ICI : http://wikefluid.uem.lan/index.php/Guide_Suivefluid_:_Analyse_5_pourquoi

Page décrivant la procédure d'analyse des anomalies bloquantes de production.

 Principe de base 

Le but est de comprendre l'arrivé du bloquant en production, en répondant à deux questions :
 Pourquoi l'anomalie s'est produite ?
 Pourquoi n'est elle pas apparue avant la production ?

Les questions s'analysent via la méthode des "5 pourquoi".

Les réponses permettent de mettre en oeuvre des mécanismes préventifs.

Un point d'attention particulier est accordé à la rapidité de l'analyse : on cherche à avoir un délai minimal entre l'introduction de l'anomalie et son analyse.

 Stockage 
Sur eroom : Mes eRooms > efluid - Gestion de Projet > Qualité > Gestion des non-conformités > Analyses.

 Timing 

La collecte des incidents se fait via deux canaux :
 chaque lundi matin durant la réunion de suivi des bloquants de production (CPEY) via les requêtes [A COMPLETER] et 3055 (ouverts par semaine)
 chaque matin (NRU) via la requête 3147.<!!!>Category:suivefluid

Catégorie:Coordination

 /!\ PAGE OBSOLETE /!\ - LE PROCESSUS D'ANALYSE DES ANOMALIES EST DECRIT ICI : http://wikefluid.uem.lan/index.php/Guide_Suivefluid_:_Analyse_des_anomalies 

 - LE PROCESSUS D'ANALYSE 5 POURQUOI EST DECRIT ICI : http://wikefluid.uem.lan/index.php/Guide_Suivefluid_:_Analyse_5_pourquoi

Cette page présente l'utilisation de l'onglet "anomalie production" de la page de consultation d'un événement suivefluid.

Il avait été constaté que dans de nombreux cas les champs de cet onglet n'étaient que très partiellement renseignés.
Nous avons cherché à alléger et redonner de l'intérêt à cet onglet sans pour autant trop forcer la main pour le renseigner.

L'onglet n'est désormais affiché que si l'événement a été détecté sur un environnement de production et a donc été renommé "anomalie production".
A l'intérieur, tous les champs ont été retravaillés :
 les deux champs "origine de l'erreur" ont été unifié dans un champs "type de problème" dont les valeurs se veulent plus claires,
 les champs "explication technique", "mesures correctives" et "mesures préventives" ont été unifié dans un champ "résumé de l'anomalie",
 afin de lever une ambiguïté, le champ "domaine d'origine" a été divisé en deux champs : le domaine ayant constaté l'anomalie, et le domaine l'ayant généré,
 un nouveau champ "responsable de l'anomalie" (client, éditeur ou intégrateur) fait son apparition,
 le champ "nature de la correction" affiche désormais de nouvelles valeurs

centré
               
La valorisation de ces champs n'est plus obligatoire pour pouvoir livrer l'événement.
Cependant, lorsque l'événement sera livré, si toutes les valeurs ne sont pas saisies, un EDP "suivi explication technique" sera créé. Tant que les données n'ont pas été saisies, celui-ci restera au statut "à analyser".

Pour connaitre la liste des EDP "à analyser" et renseigner l’onglet anomalie production de l’événement :
 Menu "campagne"
 Consulter la campagne "suivi secondaire explication technique"
 Consulter le lot "suivi explication technique"
 Consulter la statistique "à analyser"
 Rechercher les EDP vous concernant (par groupe technique et domaine par exemple)
 Pour consulter l’événement, cliquer sur son libellé dans le tableau de résultat

centré

Une fois les valeurs de l’onglet « anomalie production » renseignées, l'EDP passera au statut "analysé".Remarque :''' C'est au développeur de s'assurer que les EDP soient traités.
Nous conseillons de ne pas trop attendre pour renseigner ces valeurs tant que toutes les informations sont encore fraîches.<!!!>Category:portail

 
 
 
 

Guide de développement efluid<!!!>Category:portail

Informations générales
 ebuild
 suivefluid
 efluid
 Maven

 Equipe IT 

12 personnes.
L’IT s’occupe de tout ce qui concerne la génération et le déploiement d'environnements de la suite efluid. 

L'IT est aussi en contact privilégié avec les clients : c’est l’IT qui livre les applications. Les clients n’ont plus qu’à paramétrer et à déployer sur leurs plates-formes techniques.

 Contacts 

efluid.it@efluid.fr: mail pour le support sur l'activité IT
livraisonSQL@efluid.fr: mail pour les scripts ponctuels destinés à être livré aux clients.
 Support IT efluid : 01.53.45.31.99

Informations clients
 Base de données clients
 Diagramme des versions
 Plateformes clients à charge de l'IT
 Cartographie Client
 Liens aux applications non présents dans la page des environnements

Qualité
Processus IT

Processus de fabrication des livrables par "ebuild"

Le document ebuild processus IT couvre l'ensemble des opérations à effectuer par l'intégrateur pour la phase d'assemblage/reassemblage:
ebuild - Processus ITProcessus de fabrication des livrables par "maven"

A partir des versions d'applications suivantes, les livrables produit sont maintenant produit par Maven et passe sous la responsabilité de l'équipe Usine Logicielle :

- efluid 12.1.0

- ethaque 4.2.0

- enercom 2.1.0

- suivfluid 4.1.0

- eldap 11.1.0

- edoc 2.1.0

L'opération d'assemblage traitée par l'IT est décrite dans le document : ebuild - assemblage usine logicielle.doc 

Les livraisons génériques aux clients nécessitent - en pré-requis de la récupération des livrables produit directement d'artifactory - la mise à disposition par l'équipe IT des listes de montée de version SQL / LDIF : Usine logicielle - Nouveau processus pour les livraisons génériques

Le réassemblage a évolué en conséquence pour récupérer les livrables directement dans maven, tel que décrit dans la documentation : ebuild - réassemblage livrables issus artifactory.doc

Le paramétrage ebuild sera réalisé à l'aide des outils suivants :

- dictionnaire des propriétés de configuration : Dictionnaire

- script Installeur : efluid script installeur

- circuit de validation du paramétrage technique : Demandes d'évolution du paramétrage technique

Déploiement et maintenance des environnements de Nightly Build QTP:
 Environnements Nightly Builds gérés par l'Intégration Technique

Test de mise a disposition
Check-lists et Rapport de livraison
L'ensemble des checklists/rapport de livraison peuvent être consultés dans la room efluid - Intégration > Check-lists

Les checklists les plus utilisées sont les suivantes :
 CheckList Assemblage: CheckList à suivre lors de l'assemblage d'une version.
 CheckList livraison Suite efluid : OBSOLETE - CheckList à suivre lors de la livraison d'un environnement efluid à un client
 Rapport de livraison efluid: Rapport des tests à effectuer après le déploiement d'un environnement complet. Ce rapport contient les briques efluid,efluid.net, ael, suivefluid, suiveclient, ethaque, etineraire. Lors d'une mis à disposition, joindre uniquement les onglets utiles aux tests.
 Rapport de livraison suivefluid: Rapport des tests à effectuer après le déploiement d'un environnement suivefluid et suiveclient.
 Rapport de livraison ethaque: Rapport des tests à effectuer après le déploiement d'un environnement ethaque.
 Rapport de livraison eldap: Rapport des tests à effectuer après le déploiement d'un environnement eldap.
 Rapport de livraison ebuild: Rapport des tests à effectuer après le déploiement d'un environnement ebuild.
 Rapport de livraison edoc: Rapport des tests à effectuer après le déploiement d'un environnement edoc
 Rapport de livraison enercom: Rapport des test à effectuer après la mis à disposition d'un environnement enercom.

Les rapports de livraison sont à joindre à chaque mail de mis à disposition d'un environnement.

Tests unitaires
Référencement de différentes procédures devant être déroulé dans les check-list/rapport de livraison avant mise à disposition.
Tests Unitaires efluid/migefluid
Tests Unitaires efluidstateless/efluid-ws/efluidpub
Tests Unitaires efluid.net
Tests Unitaires ael/etineraire/portail partenaire
Tests Unitaires suivefluid/suiveclient
Tests Unitaires enercom
Tests Unitaires ethaque
Tests Unitaires edoc

PTI
 PTI efluid
 PTI efluid.net
 PTI ael
 PTI eldap
 Paramétrage des livrables génériques
 PTI edoc
Mails Génériques pour traitement de demandes
 liens vers les mails génériques

N.B: les mails de mise à disposition des environnements sont à envoyées directement et uniquement à la coordination. En cas de sollicitation, rediriger vers la coordination.

Suivie des anomalies
 Liens vers le fichier de suivie des anomalies suivefluid 
PTI et documentation Interne
Un ensemble de procédures et de guides qui font consensus et qui sont considérés comme indispensables dans le processus de l'IT.
 PTI Serveur NFS
 PTI Serveur SAMBA
 Administration système LINUX
 Administration réseau LINUX
 Installation Python sur serveur AIX
 Installation et configuration de VSFTPD

Bonnes Pratiques de l'IT
Bonnes Pratiques Suivefluid
Bonnes Pratiques eRoom

Guide du nouvel arrivant
Guide à suivre pour tous les nouveaux arrivant dans l'équipe IT du projet eFluid
Guide d'installation d'un poste d'intégrateur

Configuration Internet Explorer
Procédure de configuration IE pour fonctionnalités spécifiques ebuild et efluid (Windows 7)

Procédures techniques
Divers procédures techniques auxquelles peut être confronté un intégrateur
Procédure de gestion double sessions
Module PHP - Gestion des environnements clients
Création d'un partage Windows/Linux 
Procédure livraison d'un script ponctuel
Création d'une branche (git/maven/ebuild)
Utilitaires et scripts
Procédure de création d'un nouveau schéma AEL/etineraire/portail partenaire
Procédure de création d'un nouveau schéma efluid.net
ebuild - Traitements en erreur ne générant plus de logs
Déploiement - déploiement d'un environnement batchTesteur par l'IT
Installation service StreamServe persuasion
Gestion des files JMS avec Joram (JOnAS)
Configuration Jonas pour l'application "efluid-ws"
Parametrage des surcharges des ID en BDD
MAJ parametrages entreprise de toutes les applications
Activation du cache applicatif pour les versions V11 et V12
Tache planifiée ordonnanceur
Procédure de livraison générique - nouveau processus V12
Vérification du répertoire d'import export avant réinitialisation
Mise en place SATURNE
Installation du client Juniper 7.0.0 pour EDF-SEI
Récupérer et Mettre à disposition un dump
Accès FTP clients
Configurer les paramètres de BDD en cas d'une réinitialisation par une copie à froid des fichiers de la BDD de production
Modification BDD dans le cas ou un environnement utilise le LDAP d'un autre client
Administration MremoteNG
CURL - Récupération dumps FTP GEG
checkinstall
KeepassIT
Archivage des scripts de MaJ de TAPPLICATIONINFO 
Mise en œuvre de l'authentification multiple annuaires efluid / entreprise
Mise en œuvre de la solution d'archivage edoc pour efluid
Mise à jour de la configuration apache dans le cadre des communications soap 
export/import liste de tables
Supervision / Naemon
Configurer Nagstamon pour afficher la supervision naemon
Suivefluid : Procédure de migration des paramètres des environnements sur suivefluid
check liste des scripts de migration et liste des scripts d'assemblage
Nouveau processus livraison environnement
Mise à jour Annuaire LDAP
Ajout version dans ebuild
Livraison MCO Editique
Génération de Guichet de Paramétrage
Ajout d'un dbf à un tablespace
augmenter la mémoire alloué à une instance oracle
Transfere des comptes AEL dans efluid

Specificité client
Specificité ethaque
Specificité UEM
Specificité ERDF
Specificité ARGOS
Specificité EDF
Specificité ES
Specificité GEG
Specificité MELD
Specificité SEOLIS
Specificité SMEG
Specificité RSEIPC/GEDIA
Specificité RDM
Specificité migefluid (INT MIG CGI / PARAM MIG CGI)
Specificité SUIVEFLUID/SUIVECLIENT
Specificité Enercom
Specificité VIALIS

Usine logicielle
Configuration d'un poste de packaging

Ansible
Suivi : portage des environnemments dans Ansible
Configuration Oracle
Export et Import

Capitalisation des erreurs IT
La capitalisation des erreurs rencontrées par l'IT se fait par l'intermédiaire de ce fichier:
Capitalisation erreurs IT
Merci à chaque intégrateur de le maintenir à jour en l'alimentant à chaque fois qu'une erreur est rencontrée.

Pilotage
Intégration nouvelles ressources
Une formation est prévue à l'arrivée de chaque nouvel intégrateur dans le groupe IT selon ce plan d'intégration.
Les outils de suivi des formations se trouvent dans eroom : http://wperoom1/eRoom/Production/GestionProjetEfluid/0_42d0e

Arrivées prévues : aucune pour l'instant.

Congés
Les plannings des congés se trouvent dans la room efluid - Gestion de Projet > Congés.

Actuellement, le fichier ci-après permet de suivre les congés de l'équipe IT : Planning suivefluid 2013.xls.

Réunions de groupe

Les compte-rendus de réunions se trouvent dans eroom efluid - Intégration > Compte-Rendus.
Les réunions de groupe sont actuellement planifiées toutes les deux semaines en alternance :
 Réunions d'équipe : semaines impaires (lundi)
 Réunions techniques : semaines paires (mardi)

Boite à idées
idées wikefluid

Liens
 Convertir un fichier XLS en tableau mediawiki : http://excel2wiki.net/index.php
 Liste de diffusion efluid: Mes eRooms > efluid - Gestion de Projet > Listes de diffusion efluid.xls
 Création d'un point de montage entre un windows et un linux: http://www.pintaric.net/index.php?post/2009/05/08/Montage-d-un-dossier-partag%C3%A9-Windows-sous-Linux
 artifactory pour les clients http://wikefluid/index.php/Artifactory_frontal#Artifacts_disponibles
 Auto-formation GIT : http://pcottle.github.io/learnGitBranching/<!!!>Category:portail
Category:recetteur
Category:formateur
Category:Chefs de projet
Category:client

 Coordination 

 Suivi des dates de MEP clients en production
onglet "mise en production" : liste les différentes montées en version des environnements de production client (diagramme des versions client) 
onglet "situation projet client" : liste des versions de production, recette et versions à venir par client + actualités projet
 Superviseurs recette et responsable prestation client : à qui s'adresser ?

 Relations client

 Liste des chefs de projet :  Liste CDP Client
 Fiche d'identité des clients efluid  : fiche Client (en cours)

Utilisation de suivefluid

L'outil principal utilisé par le recetteur est suivefluid.
Suivefluid est un portail de suivi des travaux de l’équipe efluid SAS / CGI
Outil de gestion d’anomalies (depuis la déclaration d’une anomalie jusqu’à sa fermeture)
Outil de gestion des développements des nouvelles fonctionnalités (écarts) 
 etc.

Je suis affecté à un travail de recette 
(traitement des anomalies provenant du suivi de production client, ou test d'une nouvelle fonctionnalité)

 J'ai besoin d’accéder à un environnement de recette (depuis suivefluid, recherche environnement) mais je n'ai pas les informations
 contacter le responsable de l'environnement par mail et lui demander de mettre à jour depuis suivefluid les informations de connexion

Je suis affecté à un travail de recette en collaboration avec d'autres recetteur 

en interne

avec un client 

Je suis affecté à un travail de formation

 Formation interne et immersions 

A tout moment, je peux demander à mon N+1 de m'inscrire aux formations internes proposées par efluid.

Le planning de ces formations sont disponibles dans le calendrier Outlook : formation-interne@efluid.fr

 Procédures et documents de référence 

 Note de paramétrage
 Prestation client
Charte graphique
Liste des listes de diffusion
 Comment écrire un PPT de présentation
 modèles de documents efluid : portail UEM / modèles

 projet Enedis
 Enedis - Consignes technico-fonctionnelles pour PTI
 Règles de gestion évts de paramétrage pour le client Enedis
 Comment extraire paramétrage via ID DISPENDER

 Outils et astuces 

Besoin de réserver une conférence téléphonique? 
CONFÉRENCE EFLUID : 0 800 105 080
code organisateur : 40175814#
Le numéro de la conférence pour nous joindre : 0 800 105 080
Code d’accès : 29625689#
CONFÉRENCE UEM : 01 58 99 67 22
Code organisateur : *0967#
Le numéro de la conférence pour nous joindre : 01 58 99 67 22
CONFÉRENCE URM : 0 800 105 080
code organisateur : 49885086#
Le numéro de la conférence pour nous joindre :0 800 105 080
Code d’accès : 94537491#

rejoindre une conférence « Zoom » avec un téléphone ou une « pieuvre » téléphonique

Base de connaissance Footprint
Depuis Footprint, en cliquant sur "base de connaissance" vous avez accès à des procédures groupe du type : 
renouvellement certificat Wifi
renouvellement de mot de passe 
configuration messagerie Outlook sur Iphone
etc.

Cette base de connaissance se met à jour, n'hésitez pas à aller y faire un tour!

 Besoin d'envoyer un gros fichier - impossible par mail?

Vous avez la possibilité d'utiliser l'interface NextCloud
https://cloud.uem-metz.fr/index.php/login?

 Je n'ai plus accès à mes lecteurs réseau 
Depuis le portail UEM https://portail.uem.lan/ , je clique sur Aide : on m'indique une procédure pour les réactiver.

 J'ai besoin de réactiver mon plug in eroom 
 ouvrez le menu Démarrer de Windows
 localisez puis déployez le groupe de programmes Groupe UEM
 cliquer sur "réactivation plug in eroom"

 RH 
 accès direct à Cantoriel
 ouvrez le menu Démarrer de Windows
 localisez puis déployez le groupe de programmes Groupe UEM
 cliquer sur "Cantoriel"
 planning expertise fonctionnelle
 procédure des déplacements
 Formation professionnelle : evt 30989
 Si vous avez suivi la formation « extincteurs », il faut la noter dans suivefluid (Sheila l’a déjà saisie dans cantoriel)
 Si vous avez suivi d’autres formations « FPC », vous devez faire la saisie dans suivefluid mais également dans cantoriel comme indiqué sur votre convocation.
 A l’avenir, si vous avez d’autres FPC en interne, transmettre à Sheila la convocation car le sprh ne le fait pas.<!!!>Category:portail

 Pôle Prestation <!!!>Category:outil
Category:eclipse

 Guide d'installation développeurs 

Toute la procédure d'installation qui concerne les développeurs est située sur la page du Guide d'installation d'éclipse

 Guide d'utilisation 

Pour les néophytes qui ont toujours travaillé avec d'autres IDE où les vétérans qui ont besoin d'un rappel ; deux guides sur l'utilisation d'Eclipse sont disponibles :
 Guide d'installation d'un projet git avec eclipse
 Guide utilisateur Eclipse
 mettre à jour le JDK dans eclipse
 Guide Eclipse Photon

 Migration d'une version d'Eclipse à une autre (pour Usine Logicielle) 

La procédure de migration à été analysée et simplifiée pour le passage d'une version à un autre. Elle est décrite dans cette page wikipédia : Zip d'éclipse

 Support développeurs 

Le support est assuré par l'équipe "expert IDE". Elle peut-etre contactée via le forum : http://eforum.uem.lan/viewforum.php?f=16

 TODO Liste 

La page suivante regroupe la TODO liste des actions à effectuer sur le "packaging" eclipse développeur. Au bon vouloir de la communauté de réaliser ces actions.

 Liens externes 
  Roadmap
  aide-mémoire des raccourcis d'Eclipse<!!!>Git est un logiciel de gestion de versions décentralisée. C'est un logiciel libre créé par Linus Torvalds, le créateur du noyau Linux, et distribué sous la GNU GPL version 2.

 Prérequis et Installation  

Besoin de suivre le Guide d'installation de Git et avoir les droits sur le référentiel Git comme publicateur ou relecteur.

 Configuration de l'environnement Git 

Avant de vous lancer dans le développement, vous devez vous assurer que Git est bien opérationnel. Pour se faire, voici la liste des pages que vous devez

 Configuration générale GIT/GERRIT : La configuration globale à mettre en place ;
 Configurer le proxy :
 git config --global http.proxy http://bouthino:password@lpsrvpxy:8080

 Guides 

 Guide cas d'utilisation : liste des cas d'utilisation utiles pour les développeurs efluid ; à ajouter / modifier au fil du temps !
 Guide d'installation d'un projet Git : comment installer un projet git dans Eclipse (clone du projet, ajout dans Eclipse et configuration)
 Utiliser une branche locale
 Deplacer des sources dans un nouveau repository
 Guide d'installation sur Linux : installation de git sous Linux
 Astuces GIT astuces git pour utilisateur avancé

 Formation 

L'assistance Git est prise en charge par les référents gerrit. Ils sont indiqués dans la section "formation" de la page Gerrit.

Voici différents supports pour vous former à utiliser Git :

 Mise à jour Usine logicielle 

 déposé le tar.gz dans artifactory, ext-release-local/git/
 Mise à jour des images dans etools en commençant par socle-jenkins/base
 puis de la version de ces images dans la sharedLibrary (vars/variables.groovy)

 Liens externes 
 guide
 http://www.cheat-sheets.org/saved-copy/git-cheat-sheet.pdf
 git-cheatsheet
 Tutorial graphique
 Documentation en français
 http://gitignore.io
 Rebase

 Archives 
 Liste des actions à effectuer lors de la création d'une nouvelle branche de maintenance
 Les alias GIT<!!!>Category:maven

 Guide d'utilisation 
 Guide d'installation
 Profils et architecture Maven suite efluid
 Livraison des briques logicielles
 Le référentiel artifactory
 L'usine logicielle

 Les plugins Maven 
 Maven - Description des plugins efluid
 Synthèse des modifications effectuée sur des plugins maven existants

 La gestion des projets avec Maven 
 La gestion des versions des applications de la suite efluid
 Génération automatique des extracteur/injecteur
 Synthèse des actions réalisées
 Dépendances efluid
 Guide de migration developpeur pour les projets sur le socle technique V14

 Formation 

 TODO liste 

 Suivre l'avancée du bug suivant : http://jira.codehaus.org/browse/MASSEMBLY-578
 Suivre l'avancée de la demande pour le plugin dependency : http://jira.codehaus.org/browse/MDEP-135
 Fournir des commandes maven permettant de supprimer des artifacts du referentiel (exemple : supprimer des RC framework qui ne servent plus)
 Mettre en place une politique de suppression des artifacts snapshots qui ne servent plus sur artifactory
 Mettre en place un job de rollback pour les releases sur HUDSON (revert commit GIT, suppression du tag distant + sur local sur le serveur + suppression des artifacts déja uploadé (optionnel) )
 Gérer le lancement de map4J avec des jars de DEV (exemple : ecore-jar-DEV)

 Liens externes 
  Maven: The definitive Guide
  Sonatype Maven book reference
  Repository central Maven
  Jar Finder - plugin Eclipse, plugin Firefox

 Montée de version dans l'usine logicielle 

 télécharger le binaire tar.gz pour linux https://maven.apache.org/download.cgi?.
 déposé le binaire dans artifactory : ext-release-local/org/apache/maven/<version> + ext-release-local/socle-provisionning/archive/
 généré un repo minimal maven : https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellerelease/job/FjobsUtilitaires/job/Fmaven/job/maven.build-repo-minimal-for-applications-maintenance
 généré un rpm https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellerelease/job/Ftools/job/Fetools/job/etools-rpm-apache-maven.release/ peut nécessiter une modification dans etools)
 mettre à jour les images dans etools, par exemple https://gerrit.efluid.uem.lan/c/etools/+/299767 et https://gerrit.efluid.uem.lan/c/etools/+/299795
 mettre à jour le pom parent : https://gerrit.efluid.uem.lan/c/efluidUtilsPom/+/308542
 mettre à jour les images batch et embeddded : https://gerrit.efluid.uem.lan/c/scriptInstalleur/+/300256
 mettre à jour dans le provisionning

 Archives 
 Modification des POM avant livraison d'une application
 Livraison efluid mode IT
 La mise à jour des versions des applications après chaque ramassage par l'IT
 Matrice d'intégration de Maven dans les projets<!!!>Category:archive
Category:hudson

Hudson est un outil d'intégration continue, il permet :
 de compiler de manière régulière le code des différents applications
 de déployer automatique les envirronements nightly-build 
 de lancer de manière régulière les tests unitaires

 comment lancer le build d'un projet 

Les builds sont lancés automatiquement; il est néanmoins possible de demander un build d'une grande partie des projets de l'usine logicielle; pour celà, voici comment procéder :
 s'authentifier auprès de l'Usine Logicielle (lien S'identifier en haut à droite de la page de l'usine logicielle,
 sélectionner le projet sur lequel un build doit être lancé,
 lancer le build.
Image:Hudson Build Howto.png

 liens 
 externes 
  http://jenkins-ci.org (Suite au rachat de Sun par Oracle, il existe un fork Open Source)
  http://hudson-ci.org
  hudson book<!!!> Description 
Evènement suivefluid permanent : 152972

Sonar est une application de qualimétrie du code source : elle permet d'analyser le code source, de vérifier le respect des bonnes pratiques et d'agréger les résultats de ces analyses. 

Sonar est disponible à cette url : http://especteur/.

Sonar application permet de :
 Connaître les violations des règles de codage détectées dans le code (CheckStyle et PMD)
 Visualiser le code source où chaque violation est détectée
 Suivre dans le temps l’évolution des violations dans le temps

Vous trouverez pour chaque application et chaque domaine fonctionnel d’efluid un tableau de bord réunissant toutes les informations issues de l’analyse du code (Checkstyle et PMD). Sur cette page de synthèse, les fonctions les plus intéressantes sont les suivantes :
 Violation drilldown : qui donne la liste des violations des règles checkstyle et pmd, ainsi qu’une vue vers le code source correspondant
 Time machine : qui donne l’évolution des chiffres du projet, notamment l’évolution du nombre de violations checktyle et pmd dans le temps

Cette application propose d'autres fonctionnalités qui ne sont pas encore mises en œuvre. C’est pourquoi il n'est pas utile de prêter attention aux chiffres suivants sur la page de synthèse de chaque projet :
 Rule compliance : le chiffre avoisine toujours les 100 % quel que soit le nombre de violations
 Duplications : les copiés / collés ne sont pas comptabilisés 

Ci-dessous, voici l’exemple d’un tableau de bord avec en vers ce qui est particulièrement intéressant, et barré en rouge les chiffres à ne pas prendre en compte.

Fichier:Sonar_example.jpg

 Liste des packages par domaine 
 efluid/ecore : Efluid package/domaine

 Liste des responsables sonar 

 Projet sonar Responsable efluid efluid  efluid cac-ic XBL efluid consommation JTA efluid crm-contrat TBO efluid crm-referentiel KST efluid echange CLR efluid facturation JFL,DBO efluid intervention BDA efluid offre-moteur PWE efluid portail ADM efluid relance-contentieux  efluid technique  efluid workflow CBO Projet sonar Responsable ecore  ecore actionpredefinie  ecore campagne  ecore courbe  ecore editique CLA ecore referentiel KST ecore valorisation CFO ecore workflow CBO Projet sonar Responsable efluidEDKWarDebug CLR efluidnet CLR efluidpub CLR ael ADM ebuild  edoc ADM eldap ADM enercom  ethaque CLEM HermesArchiWarDebug  suivefluid DFE

 Evolutions Réalisées 

 Kilian Stein 28 fevrier 2013 à 12:00 (CET) : Migration sonar 2.9 serveur Windows vers 3.4 serveur Linux, voir nouvelles fonctionnalités : Annonce forum sonar 3.4
 Kilian Stein 12 mars 2013 à 09:16 (CET) : Rajout du seuil d'alerte rouge pour les violations : Permet de dissocier les violations des problèmes de Tests unitaires

 À faire 

 Projets en cours 

 Sujet Demande initiée par Personne en charge du projet Action à mener PrioritéVoir les TU qui ne sont pas à 100%ADUADUCorrection des couvertures de code à 0%ADUADUVoir si jacocco fonctionne mieuxCorrection des domaines sur ecore et énercomADUADUVoir version Sonar 3.5 pour faire les inclusions (virer les trucs du wiki)Mise à jour des noms des projets "archi" et "EDK"ADUADUAttention aux impacts !Ajouter des filtres pour les projets / sous-projetsADUADUDéjà un premier travail de fait sur le filtre avec les métriques manuelles ; à priori ça fonctionnePasser l'analyse de couverture de code sur JacocoADUADUVoir le job d'énercom qui fonctionne comme ça (d'ailleurs ça ne fonctionne pas...)Mettre en place une analyse pour les TIKSTKSTChanger le profil "test-unitaire" en "sonar"ADUADUPrésenter les revues de code via Sonar au CTADUADURajouter les utilisateurs en user dans le LDAPKSTKSTAdministrer le bouton des faux-positifKSTKSTVoir http://jira.codehaus.org/browse/SONAR-2447Faire installer le plug-in Sonar dans Eclipse (à présenter CT ?)KSTKSTPour voir les violations comme des warnings (et par la suite gérer les revues de code !?)Créer une nouvelle instance de Sonar pour s'exécuter sur les branches de devKSTKSTPour éviter de commiter des violationsTester la version 3.5KSTKSTProposer de nouvelles règles au CTKSTKST

 FAQ 

 Quèsaco qu'un responsable sonar ? 
C'est la personne qui est chargée de s'assurer de la bonne correction des nouvelles violations sonar. Il se verra octroyer le droit de déclarer dans son domaine des "faux positifs"

 Comment déclarer un faux positif ? 
Une fois connecté à sonar, un bouton "faux positif" apparaitra au niveau de la page des violations. Ne pas oublier de remplir la raison de cette déclaration (obligatoire). 

Pour les projets efluid et ecore, il ne faut pas oublier de déclarer les "faux positif" une deuxième fois sur les projets mères : "efluid efluid" et "ecore"

 Dois-je déclarer tous mes warnings sonar en tant que faux-positif pour faire baisser les statistiques de mon domaine ? 
Surtout pas, la déclaration en tant que "faux positif" doit être réalisée quand il n'y a pas d'autre solution. C'est souvent due à l’algorithme de la détection des violations : il détecte des violations là où il ne devrait pas.

 Liens 

 http://especteur/

Category:outil
Category:sonar<!!!>Category:outil
Category:selenium
Category:test

 Documentation 

La documentation pour Selenium est la suivante :

Guide utilisateur Selenium
Documents disponibles sur eRoom : http://wperoom1/eRoom/Production/QualiteDeveloppementEfluid/0_85c8a

Et pour la documentation sur l'implémentation dans ecore, concernant les classes utilitaires et les bonnes pratiques, est située sous eroom : http://wperoom1/eRoomReq/Files/Production/DocTechniqueEfluid/0_894de/DCTCPT%20-%20Ecore%20-%20Selenium.doc

Pour le forum : http://rphpbb/viewtopic.php?f=24&t=135&sid=720eb981e74fd9d3128cefa1c8b6d39e

 Plugin Firefox : Selenium IDE 

Un plugin Firefox permet d'enregistrer les clicks utilisateur dans l'IHM, puis les convertir en tests Java JUnit. Il est à noter que les tests sont enregistré dans Firefox, mais notre but est de les rejouer dans IE. Les particularités de cette pratique sont mentionnées ici-bas.

http://seleniumhq.org/projects/ide/

Pour la compatibilité du plugin sélénium IDE avec firefox 4 il faut installer le plug-in suivant : 
https://addons.mozilla.org/en-US/firefox/addon/add-on-compatibility-reporter

downloadhttp://seleniumhq.org/download/plugin eclipsehttp://cubictest.seleniumhq.org/

 Utilisation plugin 

 IE lors du lancement du test deux fois de suite

open jsp/arc/commun/images/general/accueilUEM.jpg
open jsp/arc/commun/frame.jsp

 Selectionner une Frame

selectFrame relative=top
selectFrame bas (ou haut selon le cas...)

 Attendre l'affichage du bandeau du haut

waitForElementPresent //a[@id='client']

 Sélectionner un élément du menu

selectFrame relative=top
selectFrame haut
waitForElementPresent //a[@id='menuGeneral']
click //a[@id='menuGeneral']
waitForPopup
selectPopup
clickAndWait //a[@id='rechercheContrat']
selectWindow
selectFrame relative=top
selectFrame bas

 Cliquer sur un onglet (exemple pour "service")

clickAndWait //td[@class='titreOngletOff']/a[contains(text(),'service')]

 Sélectionner un lien d'un menu caché (rôle)

clickAndWait //div[@id='Popup2']/table/tbody/tr[2]/td/a

 Emplacement dans CVS pour les fichiers Selenium JUnit

test/[package fonctionnel]/seleniumTest

 Conversion en tests unitaires 

Une fois le test enregistré, il suffit de le convertir en Java. Il est aussi possible de tout écrire les tests en Java, sans l'aide du plugin (plus rapide). Dans la classe abstraite SeleniumTestCase, il a plusieurs méthodes qui automatisent les tests. Cette classe se charge de :

 Configurer le serveur sur lequel lancer le test et avec quel navigateur
 Gérer l’authentification à l’application (le login) entre les tests : si l’utilisateur est déjà authentifié, ne refait pas l’authentification, sinon fait le login avec les valeurs définies dans le fichier de propriétés
 Fournir des méthodes utilitaires pour les traitements de base et récurrents

De plus, certaines actions ne sont pas bien gérées par l’enregistrement via le plugin Firefox. Dans ces cas, il faut utiliser ces méthodes préexistantes afin de normaliser l’application des tests. Pour choisir un menu (un bouton) dans la barre supérieure, utiliser :
 
 choisirMenu(String)

Pour choisir un cadre (cadre haut ou cadre bas), utiliser :

 choisirCadre(String)

Pour ouvrir le menu général (bouton « menu ») et naviguer à l’intérieur, utiliser :
 
 ouvrirMenuGeneral()
 choisirOngletDansPopup(String) 
 choisirLienDansPopup(String)

Pour sélectionner un objet dans une page (recherche, tableau, etc.), utiliser : 

 selectionnerBusinessObject(BusinessObject)

Pour faire des clicks sur les boutons standards de l’architecture, utiliser : 

 annuler()
 modifier()
 finaliser()

 Classe RechercherSeleniumTestCase 

Classe utilitaire fait le zoom automatique sur la bonne page de recherche automatiquement.

 Classe ZoomerSeleniumTestCase 

Cette classe utilitaire fait le zoom automatique sur l'objet en passant par la page de recherche.

 Serveur Selenium : Selenium RC 

Il existe aussi une version serveur Selenium RC (Remote control) qui permet d'exécuter les scripts sur tous les navigateurs.

http://seleniumhq.org/projects/remote-control/

 Selenium 2 

TODO ADU revoir cette page

Screenshots

 http://code.google.com/p/selenium/issues/detail?id=3536
 http://stackoverflow.com/questions/1260106/selenium-run-as-a-windows-service-to-take-screenshots-on-errors
 http://stackoverflow.com/questions/8963045/selenium-2-webdriver-taking-a-screenshot-returns-a-black-image

Ces 3 sites décrivent comment utiliser Selenium 2 avec Maven et du vieux code : 

 http://selftechy.com/2011/06/07/selenium-2-with-junit4-create-tests-generate-reports
 http://seleniumhq.wordpress.com/2010/07/30/how-to-use-selenium-2-with-maven/
 http://seleniumhq.org/download/maven.html
 http://stackoverflow.com/questions/5094598/selenesetestcase-is-deprecated-how-to-call-verify-methods

Impossible de traiter les fenêtres modales, sauf en "hackant" le code de hermes : 

 http://stackoverflow.com/questions/866856/how-do-i-test-modal-dialogs-with-selenium
 http://seleniumdeal.blogspot.com/2009/01/handling-modal-window-with-selenium.html
 Bug : http://code.google.com/p/selenium/issues/detail?id=27

Pour le clic sur le poste de BBU : 

 http://stackoverflow.com/questions/4667048/cant-click-button-which-opens-file-attachment-dialog<!!!>Category:outil
Category:eroom
Catégorie:documentation

 FAQ 
 Comment rechercher le chemin d'un fichier dans eRoom après une recherche ? 
Après avoir lancer la recherche, cliquer sur "plus d’options", puis depuis la colonne "Trouvé dans", cliquer sur le lien correspondant : 
image:eroom.png

 Comment visualiser un calendrier eroom dans outlook ? 

Pour visualiser les calendriers eroom avec Outlook :

 Tout d'abord s'assurer que le superviseur eroom est installé (installation automatique en se rendant sur eroom) et démarré (icône dans la zone de notification). S'il n'est pas démarré le démarrer. S'il ne démarre pas le désinstaller puis le réinstaller.

 Créer un calendrier dédié (la sync ne marche pas avec le calendrier principal) : en mode Calendrier dans outlook : ficher -> nouveau -> calendrier :
Fichier:2013-08-28 15 43 01-Créer un dossier.png

 Faire un clic droit sur l’icône Superviseur Eroom puis paramètres : 
image:FAQ eroom - Faire un clic droit sur l’icône Superviseur Eroom.png

 Puis ajouter et cocher le dossier événement à synchroniser de votre ou vos room(s) : 
image:FAQ eroom - Ajouter le dossier événement à synchroniser.png
image:FAQ eroom - Cocher le dossier événement à synchroniser.png

 Puis cliquer sur le bouton Avancé et remplir la partie en rouge : 
image:FAQ eroom - Cliquer sur le bouton Avancé.png

 Fermer toutes les fenêtre avec le bouton OK.

Les événements seront dupliqués dans l’agenda Outlook.

(merci à Eric Feuvraux)<!!!>Le choix de la version de tomcat à utiliser sur le poste de développement est à corréler avec la matrice de compatibilité de la version de l'application concernée.

 Parametrage du serveur tomcat pour les JSP 

 Serveur Tomcat ligne de commande 

 Démarrage et arrêt 

Pour démarrer le serveur Tomcat en ligne de commande, il suffit de naviguer jusqu'au dossier d'installation et d'exécuter "startup.sh" ; pour l'arrêter utiliser "shutdown.sh". Le serveur sera démarré par défaut sur le port 8080 (http://localhost:8080).

cd /home/alexandre.dubreuil/programs/tomcat/apache-tomcat-<TOMCAT_VERSION>/bin
./startup.sh
ps aux | grep java
     5492       1    1192       6576  pty0    17780 10:31:50 /cygdrive/d/programs/jdk/jdk<JDK_VERSION>/bin/java

 Utilisateur 

Pour utiliser l'IHM, il faut configurer un utilisateur faisant parti du groupe "manager-gui". Il faut éditer le fichier "conf/tomcat-user.xml"

cd /home/alexandre.dubreuil/programs/tomcat/apache-tomcat-<TOMCAT_VERSION>/conf
vim tomcat-user.xml

Ajouter cette ligne dans "tomcat-users" avec un nom d'utilisateur et mot de passe arbitraire.

<user username="tomcat" password="tomcat" roles="manager-gui"/>

 FAQ  

 Mise à jour de la version dans l'Usine logicielle 

 déposé le binaire dans artifactory, dépôt ext-releases-local
 Mettre à jour les licences : par exemple https://gerrit.efluid.uem.lan/c/archi/+/239072
 Mettre à jour l'image Docker : par exemple https://gerrit.efluid.uem.lan/c/etools/+/239073
 Mettre à jour le pom parent: par exemple https://gerrit.efluid.uem.lan/c/efluidUtilsPom/+/239074
 Redescendre le pom parent dans archi : par exemple https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellerelease/job/Ftools/job/FefluidUtilsPom/job/application.update-version-efluid-parent-pipeline/
 Valider les changes ainsi générés
 Mettre à jour la version dans le wiki : Mod%C3%A8le:Outil.tomcat.version

Category:outil
Category:tomcat<!!!>Category:outil
Category:Map4J

Map4J est un logiciel servant à automatiser la génération des classes de mapping. Il permet de réaliser rapidement le portage des créations/modifications d'attributs dans les classes d'objet métier efluid vers les classes du modèle Edk.
 Guide d'utilisation  
 Variables d'environnement 
Il faut positionner sur son poste les variables suivantes : 
 JAVA_HOME=D:\Programs\jdk\jdk
 --- obsolète ANT_HOME=D:\Programs\ant ---

Sinon map4J ne fonctionnera pas.

 Customisation développeur 
Si un développeur veut customiser le script de lancement de map4J, il faut créer et valoriser le fichier suivant : 
 <PROJET>/map4j/scripts2/initialiserDev.cmd

 Clone sur emplacement particulier 
Suite à un clone sur une branche de dev particulière dans un repertoire particulier (différent de developpement_dev), il faut mettre à jour :
 le fichier projet .mpj également avec le bon répertoire (13 remplacements)

 Architecture de map4J-launcher 
 Dans les projets le script Map4J.bat va récupérer la version de map4J-launcher.zip via la variable map4J.version définie dans le POM parent.
 Ensuite Map4J.bat va dézipper map4J-launcher.zip puis va lui donner la main en appelant le script map4J-run.cmd.
 A partir de la map4J-run.cmd va appeler initialiserLibMaven.cmd dans le projet afin de provisionner le dossier <PROJET>/lib dans lequel les fichiers de mapping pourront sourcer les jars (ecore, edk, etc...)
 Ensuite map4J-run.cmd va appeler initialiserSklSql.cmd dans le projet afin de récupérer les fichiers skl, sql et mapping des briques.
 Ensuite map4J-run.cmd va appeler initialiserVersionJdk.cmd permettant de valoriser la bonne version de JDK en fonction de ce qui est défini dans le POM du projet.
 Ensuite map4J-run.cmd va appeler initialiserLibMap4J.cmd afin de provisionner les jars permettant le lancement de map4J.
 Ensuite map4J-run.cmd va appeler scripts2\initialiserDev.cmd s'il existe afin de permettre une customisation du script de lancement par le développeur.
 Enfin map4J-run.cm va construitre le classpath de map4J, et va le démarrer avec les options de JVM par défaut définie dans ce scripts.

 Liens utiles 
 Tâche maven de récupération des skeleton
 Paramétrage DB
 Mapping

 Archives 
 Utiliser map4J avec Maven<!!!>Category:outil
Category:beyond compare
Comparateur de fichier.

 Utilisation de beyon compare avec msysgit 
Pour configurer Beyond compare avec msysgit, il faut ajouter les paramètres suivants dans le fichier C:\Users\<USER>\.gitconfig

Depuis la version 1.7.4 de Git (le paramétrage du merge n'est disponible qu'avec la version PRO de beyond compare) :
[diff]
	tool = bc3
[difftool "bc3"]
	path = D:/Programs/beyondcompare/bcomp.exe
[merge]
	tool = bc3
[mergetool "bc3"]
	path = D:/Programs/beyondcompare/bcomp.exe
[mergetool]
	keepBackup = false
Avec les anciennes versions de git :
[diff]
    tool = bc3
[difftool]
    prompt = false
[difftool "bc3"]
    cmd = "\"D:/programs/beyondcompare/bcomp.exe\" \"$LOCAL\" \"$REMOTE\""

 Liens externes 
  support de beyond compare<!!!>Category:outil
Category:sql developper

 Liens internes 
 Guide utilisateur SQL Developper
 zip d'installation sur eRoom
 documentation du eRoom
 Liens externes 
  installer le driver MySQL pour SQL Developper<!!!>Category:git
Category:msysgit

MSysGit est un émulateur Linux qui permet d’exécuter des commandes git dans un environnement Windows.

 Liens externes 
  guide d'écriture des scritps Bash<!!!>Category:outil
Category:artifactory

 Description 

Le logiciel Artifactory de JFrog est un gestionnaire de référentiels de binaires. C'est un des leaders de la gestion des repository pour Apache Maven, mais bien au-delà
de Maven, c'est l'entrepôt unique pour tous les artefacts d'une usine logicielle. Artifactory est capable de conserver de façon pérenne, distribuée et sécurisée tous vos
livrables. La traçabilité complète du cycle de vie des artefacts ainsi que leurs inter-dépendances est également assurée. L'ensemble des fonctions est accessible
nativement par Maven ou par des API type REST. Pour l'utilisateur et l'administrateur, Artifactory propose une application web conviviale pour effectuer toutes les 
opérations possibles sur les référentiels. 

 Inventaire 

 Hostname  Désignation  Alias & Proxy  artifactory Version  OS  Docker version  CPU  RAM  URL lpartedt4[.efluid].uem.lan  Artifactory de prod interne eartifact[.efluid].uem.lan : alias vers lrpxyifredt1.uem.lan lui même proxy (apache) vers lpartedt4.uem.lan

partifactorydocker[.efluid].uem.lan : alias de lpartedt4[.efluid].uem.lan (nginx) 6.23.42 RH 7.9 18.06.3 12 12 Go https://eartifact.uem.lan/artifactory/
https://partifactorydocker.uem.lan/artifactory/ lpartedt1[.efluid].uem.lan  Artifactory de prod interne eartifact[.efluid].uem.lan : alias vers lrpxyifredt1.uem.lan lui même proxy (apache) vers lpartedt1.uem.lan

partifactorydocker[.efluid].uem.lan : alias de lpartedt1[.efluid].uem.lan (nginx) 7.35.2 RH 8.5 NA 12 10 Go https://eartifact.uem.lan/artifactory/
https://partifactorydocker.uem.lan/artifactory/ lpartedt3[.efluid].uem.lan  Artifactory de prod public/frontal pfroart1.uem.lan & lpfrosfld1  : reverse proxy vers lpartedt3.uem.lan:8081

partifactorydockerpub.uem.lan : (uniquement UEM) alias de lpartedt3.uem.lan:443
attention : accessible depuis la DMZ MLD

depuis EDFSEI : 192.168.170.71  6.23.42 RH 7.5 18.06.3 4 10 Go http://pfroart1.uem.lan/artifactory/ 
https://partifactorydockerpub.uem.lan/ lpartedt2[.efluid].uem.lan  Artifactory de prod public/frontal lpfrosfld3 : reverse proxy client et interne vers lpartedt2.uem.lan:8081/8082

partifactorydockerpub.uem.lan : (uniquement UEM) alias de lpartedt2.uem.lan:443
attention : accessible depuis la DMZ MLD

depuis EDFSEI : 192.168.170.71  7.35.2 RH 8.5 NA 4 6 Go http://pfroart1.uem.lan/artifactory/ 
https://partifactorydockerpub.uem.lan/ lrartedt3[.efluid].uem.lan Artifactory de recette frontal/interne artifactoryrec.efluid.uem.lan : alias vers lrpxyifredt1 redirection vers lrartedt3[.efluid].uem.lan:8082 
rartifactorydocker.efluid.uem.lan : alias lrartedt3[.efluid].uem.lan:443  7.41.7 RH 8.5 NA  2 6 Go https://artifactoryrec.efluid.uem.lan/artifactory/ 
https://rartifactorydocker.efluid.uem.lan/artifactory/

 Configuration présente 

sur le nfs du cluster kub

 Exploitation 

 Arrêt 

Se connecter et passer artuser, puis lancer la commande

 docker-compose stop -f /data/artifactory-pro-nginx-derby/docker-compose.yml

 Démarrage 

Se connecter et passer artuser, puis lancer la commande

 docker-compose start -f /data/artifactory-pro-nginx-derby/docker-compose.yml

 Opérations 

sur l'usine logicielle

 Guide d'utilisation 

 Crypter son mot de passe dans Artifactory
 L'artifactory frontal (pour les clients)

 La gestion des repository 

Voici un état des lieu de la gestion des repository d'artifactory : 

 docker-release-local : contient les images docker constitué au sein d'efluid
 libs-release-local : contient les binaires de la suite efluid livrés à l'état de release (ne changeront jamais après leur release - même en cas d'erreur, il faut versionner le jar ; on risque d'avoir des problèmes si la JAR a déjà été remonté sur un autre repository, par exemple un repository local)
 libs-snapshot-local : contient les binaires de la suite efluid livrés à l'état de snapshot(en cours de développement)
 repo1-cache : contient les jars extérieurs du repo Maven Central http://mvnrepository.com/ (s'upload automatiquement si l'utilisateur possède les droits adéquats)
 jboss-cache, java.net-cache, etc... : si artifactory utilise d’autres repo distant, alors les jars vont dans les dossiers XXX–cache d’artifactory correspondants
 ext-release-local : contient les binaires de projets externe
 plugins-release-local : plugins Maven en release
 plugins-snapshot-local : plugins Maven en snapshot

La gestion de ces repositories est interne à artifactory, on ne doit pas intervenir manuellement sur ceux-ci (à part pour libs-releases-local, libs-snapshots-local ; même dans certains cas plugins-release-local et plugins-snapshot-local, puisqu'on écrit maintenant nos propres plugins)

 Cas de l'artifactory de recette : 
2 référentiels distants on été ajouté sur cette artifactory afin de récupérer automatiquement les artifacts provenant de l'artifactory de production. Ces référentiels distants sont liés aux référentiels virtuels libs-release et libs-snapshots.

 LDAP 

 LDAPS 

Il est nécessaire d'ajouter la CA de l'annuaire aux trusstore artifactory, ici cacerts

 openssl s_client -showcerts -connect <host>:<port> /dev/null > ldap.ca
 keytool -import -alias <host> -keystore cacerts -trustcacerts -file ldap.ca -storepass *** -noprompt
 rm ldap.ca

 Configuration 

 Dans l'annuaire LDAP : 
 créer l'application : eartifact
 créer les profils nécessaire :
 administrateur : Profil permettant d'administrer l'outil Artifactory
 utilisateur enercom : Profil destiné aux développeurs travaillant sur l'application enercom pour accéder en lecture sur l'outil Artifactory
 utilisateur ethaque : Profil destiné aux développeurs travaillant sur l'application ethaque pour accéder en lecture sur l'outil Artifactory
 utilisateur suivefluid : Profil destiné aux développeurs travaillant sur l'application énercom pour accéder en lecture sur l'outil Artifactory
 utilisateur suite efluid : Profil destiné aux développeurs travaillant sur les applications de la suite efluid : efluid, efluidPub, ael, efluid.net, eldap et edoc pour accéder en lecture sur l'outil Artifactory
 utilisateur ginko : Profil destiné aux intégrateurs ginko/enedis

 Dans "Security" > "LDAP Settings" :
Fichier:artifactory_LDAP_settings_1.png

 Créer le paramétrage à l'annuaire LDAP en cliquant sur "New" à côté du tableau des "LDAP Settings" :
 
Fichier:artifactory_LDAP_settings_2.png

 Puis créer le paramétrage des groupes en cliquant sur "New" à côté du tableau des "LDAP Groups" :
Fichier:artifactory_LDAP_settings_3.png

 Saisir un login d'utilisateur dans le champ "Filter by Username" puis cliquez sur "refresh", les profils préalablement créé dans l'annuaire LDAP devrait apparaître. Sélectionner l'ensemble des profils puis cliquer sur "import" pour les ajouter dans artifactory.
Fichier:artifactory_LDAP_settings_4.png

 Pour affecter des droits aux profils LDAP, aller dans la catégorie "Security > Permissions", puis selectionner une permission. Sur l'onglet "Groups", il est possible de rattacher des droits à un profil LDAP
Fichier:artifactory_LDAP_settings_5.png

 Configuration 

https://www.jfrog.com/confluence/display/RTF/Managing+Certificates

https://www.jfrog.com/confluence/display/RTF/Using+a+Self-Signed+Certificate

Mettre en place une politique de mot de passe dans artifactory

 security:
   password-policy:    # users' password policy
     uppercase: 1      # minimum number of uppercase letters that the password must contain
     lowercase: 1      # minimum number of lowercase letters that the password must contain
     digit: 1          # minimum number of digits that the password must contain
     length: 12         # minimum length of the password
     not-match-old: true # should access allow setting a new password to the same one currently set for the user
   user-lock-policy:
     attempts: 5                     # number of failed login attempts to allow before locking a user. 0 (default) means the feature is disabled
     seconds-to-unlock: 300          # number of seconds to wait before re-enabling login for a user that has been locked out
     password-expiry-days: 0         # number of days before a password expires. Set by Artifactory
     admin-password-expirable: false # does the access admin password expire

 Proxy 

https://www.jfrog.com/confluence/display/RTF/Configuring+a+Reverse+Proxy

https://www.jfrog.com/confluence/display/RTF/Configuring+Apache

 Modifier un alias DNS  

 commun 

 configuration maven cje et cjeTest
 find /nfstst1/cje-share-dir/tools/ -name 'settings*.xml' -exec grep lrsrv {} +
 find /nfs/cje-share-dir/tools/ -name 'settings*.xml' -exec grep lrsrv {} +

 recette 

 configuration du job http://lpsrvedt1.uem.lan:8280/job/Finstallation/job/FjobsCommuns/job/scriptsqllauncher.orchestrate-execute-script/configure
 configuration du job http://lpsrvedt1.uem.lan:8480/view/artifactory/job/artifactory.update-rules-non-regression/configure

 Migration 

 Migration de la version 6.x à la version 7.x 

Mise à jour de la version 6.x à la version 7.x

 Template Migration <serveur A> vers <serveur B> 

 Opérations de migration 

 Création de la VM <serveur B> (evt suivefluid ###) 
 Installation d'artifactory et nginx via le playbook
 Copie des données,
 Créer une clé ssh pour artuser (Bonnes Pratiques Sécurité)
 Echanger la clé pour la copie des données par rsync (entre user artuser)
 Arrêter artifactory et nginx
 Copie de l'arborescence /data/artifactory vers /data/artifactory (1)
 Vérifier qu'il n'y a aucune donnée nginx à récupérer
 Redémarrer artifactory et nginx
 Migration alias & proxy pour pointer dorénavant le <serveur B>
 Vérifier dans artifactory, Admin > General Configuration > Custom Base URL : http://<alias vers reverse proxy>/artifactory
 Configurer le remote repository docker efluid en https si il est déclaré, Admin > Repository / Remote > docker-remote
 Vérifier que les clients docker utilisent le bon CA
 vérification à l'aide du job de NonReg

  (1) artuser@<serveur A>$ rsync -e ssh -avz /data/artifactory/ <serveur B>:/data/artifactory

 Migration lrsrvart2 vers lrartedt1 

 Opérations pré migration 

 Modification de la configuration nginx pour utiliser le dépôt docker-release (virtuel) au lieu de docker-local => Done
 Modifier suivefluid NB et INT pour utiliser l'alias (change 88361) => Done
 Faire Modifier par UEM suivefluid SUP & VAL pour utiliser l'alias (DI-2152 & DI-2153) => Done

 Opérations de migration 

 Création de la VM lrartedt1 (266505) => Done 
 Installation d'artifactory et nginx via le playbook => Done
 Copie des données,
 Créer une clé ssh pour artuser => Done
 Echanger les clé pour la copie des données par rsync (entre user artuser) => Done
 Arrêter artifactory et nginx => Done
 Arborescence /dataCIFS10G/artifactory vers /data/artifactory (1) => Done
 Vérifier qu'il n'y a aucune donnée nginx à récupérer => Done
 Redémarrer artifactory et nginx => Done
 Migration alias & proxy => Jeudi 04/04 avec TAJ
 Modification du host dans la config apache de lrpxyifredt1 => Done
 Modification du DNS rartiafctorydocker.efluid.uem.lan => Done
 Vérifier dans artifactory, Admin > General Configuration > Custom Base URL : http://artifactoryrec.efluid.uem.lan/artifactory => Done

  (1) artuser@lrsrvart2$ rsync -e ssh -avz /dataCIFS10G/artifactory/ lrartedt1:/data/artifactory

 Anomalie non détectée plus tôt 
 correction de lrsrvart1.uem.lan dans le cjeTest : /nfstst1/cje-share-dir/tools/maven/conf/settings-recette.xml
 find /nfstst1/cje-share-dir/tools/ -name 'settings*.xml' -exec grep lrsrv {} +
 find /nfs/cje-share-dir/tools/ -name 'settings*.xml' -exec grep lrsrv {} +
 correction configuration maven de recette sur lpsrvedt1
 sudo find /opt -name 'settings*.xml' -exec grep lrsrvart {} +
 correction de l'url artifactory de recette dans les images de base CJE

 Migration lpsrvart2 vers lpartedt3 

 Opérations pré migration 

 Modification de la configuration nginx pour utiliser le dépôt docker-release (virtuel) au lieu de docker-local => Done
 Pas d'opération préalable dans suivefluid => Done

 Opérations de migration 

 Création de la VM lpartedt3 (266505) => Done
 Installation d'artifactory et nginx via le playbook => Done
 Vérifier qu'il n'y a aucune donnée nginx à récupérer => Done
 Copie des données,
 Monter le nfs lpnfssan1-edt:/NFS_LPARTEDT3_DTA_A05 sur lpsrvart2.uem.lan => Done
 Export full system sur le nfs depuis lpsrvart2 => Done
 Import full system à partir de l'export depuis lpartedt3 => Done
 Redémarrer artifactory et nginx => Done
 Migration alias & proxy => DI-3053 => Done
 alias : partifactorydockerpub.uem.lan ( attention au CA & certificats) => Done
 proxy : pfroart1.uem.lan => Done
 proxy : lpfrosfld1 => Done
 Vérifier la replication configuré dans eartifact => Done
 Vérifier que les clients docker utilisent le bon CA (disponible dans notre dépôt ansible) => Done

 Opérations post migration 

 Ne plus répliquer les images docker passer par le cache
 Configurer le remote repository docker en https, Admin > Repository / Remote > docker-remote => Done

 Migration lpartedt1 vers lpartedt4 

 Opérations pré migration 

 Pas d'opération préalable dans suivefluid => Done

 Opérations de migration 

 copie du disque /data pour teste de montée de version => DI-3052 / GLE => Done
 Création de la VM lpartedt4 => evt 279311 => Done
 Vérifier qu'il n'y a aucune donnée nginx à récupérer => Done
 jour J 
 Désactiver les réplications docker-releases-local, libs-mobefluid-local et libs-release-local => Done
 couper le container artifactory sur lpartedt1 => Done
 clone du disque avec les données de production => DI-3811 => Done
 Montée le disque pour l'export site sur (1)  => Done
 Installation d'artifactory et nginx via le playbook => Done
 Activer les réplications docker-releases-local, libs-mobefluid-local et libs-release-local => Done
 Migration alias & proxy => opération sur lrpxyfld1 fichier /etc/httpd/conf.d/eartifact.conf + redémarrage deamon httpd => Done
 Vérifier que les clients docker utilisent le bon CA => Done

 (1) lpnfssan1-edt:/NFS_LPARTEDT1_EXT_A04   /ext/export     nfs     defaults        0 0

 Interaction avec artifactory 

Les interactions avec artifactory peuvent se faire soit : 

 Via le CLI : https://www.jfrog.com/confluence/display/CLI/CLI+for+JFrog+Artifactory
 Via les services REST : https://www.jfrog.com/confluence/display/JFROG/Artifactory+REST+API

Il faut faire des tests sur artifactory de recette : https://artifactoryrec.efluid.uem.lan/artifactory/webapp/

Pour cela on peut utiliser un compte de test : jenkinsdev (mot de passe à demander à l'UL)<!!!>Category:notepad++
 Plugin pour la gestion des fichiers xml 
Il existe des plug in pour la gestion des fichiers xml dans notepad++.
Ils permettent en particulier d'utiliser les fichiers xsd.
Une procédure d'installation ainsi qu'un tutoriel sont diponibles sur internet ou sur eRoom.

 Configuration de l'encodage avec Notepad++ 

Il faut bien vérifier que l'encodage soit Cp1252 lorsque vous utilisez Notepad++ pour modifier des fichiers. Pour ce faire, rendez-vous dans le menu puis Encoding et choissisez "ANSI" (c'est ainsi que se nomme Cp1252 dans Notepad++...)

Dans mon exemple, j'ai écris quelques lignes en encodage Cp1252 et je les ai converties en encodage UTF-8 pour observer les différences.

 1000x700px

Avec un encodage UTF-8...

 1000x700px<!!!>Category:SoapUI

SoapUI est une application open source permettant le test de web service dans une architecture orientée services (SOA). Ses fonctionnalités incluent l'inspection des web service, l'invocation, le développement,  la simulation, le mocking, les tests fonctionnels, les tests de charge et de conformité. Une version commerciale, SoapUI Pro, qui se concentre principalement sur des fonctionnalités conçues pour améliorer la productivité, a également été mis au point par eviware software. En 2011, eviware a été racheté par SmartBear Software distribué en France par la société KYRIEL (http://www.kyrielsoft.fr).

SoapUI a été publié pour la première fois en septembre 2005 sous Licence publique générale limitée GNU. Depuis sa publication, SoapUI a été téléchargé plus de 2 millions de fois. Il est entièrement basé sur la plate-forme Java et utilise Swing pour l'interface utilisateur. Ce qui signifie que SoapUI est multiplateforme. SoapUI supporte aujourd'hui IDEA, Eclipse et NetBeans.

 Remarques 

 Ne plus utilisez la version 4.6.1 de l'outil car une erreur d'encodage UTF-8 survient lors de la création d'un nouveau service à partir d'un WSDL dans le domaine service web sge

 La version 5.3.0 est recommandé si vous avez besoin de récupérer un jeton OAuth2 avec le profil Resource Owner Password Credentials Grant

 Articles connexes 
 Guide d'utilisation de SOAP UI

 Liens externes 
 Le scripting dans Soap UI avec Groovy<!!!>Utilisé pour récupérer les fichiers de log sur linux via SFTP.

Category:outil
Category:FileZilla<!!!>Category:outil
Category:internet explorer

 Machines pour tester internet explorer x.x 
 WRPC764B4 + IE8 32/64bits + Win7 + Adobe Reader 11 + Favori Portail UEM
 WRPC764B5 + IE9 32/64bits + Win7 + Adobe Reader 11 + Favori Portail UEM
 WRPC764B6 + IE10 32/64bits + Win7 + Adobe Reader 11 + Favori Portail UEM
 WRPC764B8 + IE11 32/64bits + Win7 + Adobe Reader 11 + Favori Portail UEM 
 WRPC864B2 + IE11 32/64bits + Win8.1 + Adobe Reader 11 + Favori Portail UEM
 WRPC1064B1 + IE11 - Edge + Win 10

Accès depuis CGI StDenis:
 WRPC764B4 : 172.23.2.96
 WRPC764B5 : 172.23.2.97
 WRPC764B6 : 172.23.2.98
 WRPC764B8 : 172.23.2.142
 WRPC864B2 : ???.???.???.???

Liste des personnes ayant des accès : JTA, FDE, CTH, ADM, EFI, RLE, VBO et BDA

 Machines pour tester firefox et chrome 
 WRPC1064B7 ou wrpc1064b7.uem.lan ou 192.168.116.8
Sur cette VM, on dispose de tout les droits pour installer / désinstaller la bonne version de firefox et/ou chrome. Cette opération étant à la charge de l'utilisateur.

Les accès simultanés sont limité à 2. (A signaler si ça pose problème).

Liste des personnes ayant des accès :
 FDE, CTH, ADM, EFI, RLE, VBO et BDA
 pour ajouter un accès temporaire (pour un besoin ponctuel) ou long terme : faire une demande au 6000

 FAQ 

 Comment gérer la double session efluid avec IE 8 ? 

 Installation d'IE8 sur Windows 7 

Il suffit de supprimer la mise à jour d'IE8 vers IE9 qui vient avec Windows 7 SP1. Par contre, il n'est pas possible d'avoir les deux navigateurs côte-à-côte. 

 Aller dans "Panneau de configuration"
 Puis "Programme > Désinstaller un programme"
 À gauche choisir "Afficher les mises à jour installées"
 Dans la liste double-cliquer "Microsoft Internet Explorer 9"
 Valider puis redémarrer

 Liens utiles 

 IECollection<!!!>Utilitaire permettant d'installer différentes version d'Internet Explorer dans Windows en même temps.

 Utilité 
Internet Explorer Collection regroupe plusieurs versions du navigateur Internet Explorer pouvant être utilisées simultanément. Grâce à cette application, les développeurs ont la possibilité de tester leurs sites Web sous différentes versions d'IE.

 Version IE supportée 
Internet Explorer Collection installe les versions suivantes : 1.0, 1.5, 2.01, 3.0, 3.01, 3.03, 4.01, 5.01, 5.5, 6.0, 7.0 et 8.0.

 Problème rencontré  <!!!>Utilisé pour analyse ce qui transite sur le réseau?

La version  d'ethereal est dispo sur eRoom : http://wperoom1/eRoom/Production/QualiteDeveloppementEfluid/0_75dd7

Au prochain besoin, ne pas hésiter à monter en version et créer le document d'installation.

Nouveau nom de l'application : http://www.wireshark.org/

Category:outil
Category:ethereal<!!!> Lancement 
 sqlplus hermes_user/h4d3v@ldbddfld4:2483/dnewfld
 sqlplus hermes_user/h4d3v@DBUGFLD @U_VIDEOCOMMUNICATION_83049.sql

 Utilisation de sql Plus pour une extraction CSV 
 touche windows + R puis taper sqlplus login/mdp@SID pour lancer sqlplus. 
Le SID est celui qui est référencé par votre tnsname.ora
Par exemple : sqlplus HERMES_USER/h4d3v@DNEWFLD_10G
 vous arrivez sur le prompt SQL> --> il suffit de lancer la requête SQL comme suit : @CHEMIN_REQUETE_SQL\NOM_FICHIER_SQL. 
Exemple : @E:\Users\lagarde\Documents\Scripts\SMEG_EXTRACTION_BRANCHEMENTS_ELEC_GAZ_97988.sql
Vous pouvez également positionner directement les paramètres dans le prompt (ici 8 paramètres attendus):
Exemple : @e:\Users\weberp\Desktop\scriptsSeolis\scriptArticles.sql null null null null e:\Users\weberp\Desktop\scriptsSeolis\ 01/03/2015 31/03/2015 extractionArticleMars2015

 Problèmes dus aux accents 
Si vous avez des problèmes avec les accents, il faut définir l'encoding :
SET NLS_LANG=.WE8MSWIN1252 
 http://www.dba-ora.fr/article-set-nls_lang-sqlplus-accent-79582454.html

 liens externe 
  http://www.dba-ora.fr/article-connexion-mode-console-sqlplus-104687208.html
  http://combot.univ-tln.fr/loris/admin/sqlplus/formatage.html
  http://combot.univ-tln.fr/loris/admin/sqlplus/<!!!>Category:wikefluid

 Liens internes 
 Configuration de MediaWiki : ici
 Deprecated Modifier le fichier common.css : ici
 Modification du menu de gauche : ici
 Création / Modification des InfoBox V2 : ici

 Requêtes 

 Requêtes permettant de récupérer les statistiques d'utilisation
select COUNT(r.rev_id) as value, COUNT(DISTINCT r.rev_page) as page_value, u.user_name as title
from wikefluid.user u, wikefluid.revision r, wikefluid.page p
where u.user_id = r.rev_user and p.page_id = r.rev_page and p.page_namespace = 0
GROUP BY u.user_name

 Release notes et planning des versions 

https://www.mediawiki.org/wiki/Release_notes<!!!><!!!>Aide-mémoire pour utiliser vi<!!!>Category:outil
Category:plugin

Plugin de Firefox pour faire du debug HTML, CSS et Javascript.
 liens externes 
 http://getfirebug.com<!!!>Category:outil

 Jobs pour les équipes DEV/REC 

Job permettant de compiler / packager une version custom sur un change Gerrit

 Schéma global 

Fichier:Usine logicielle - architecture - centrale.png

Visio : http://wperoom3.uem.lan/eRoom/Production/QualiteDeveloppementEfluid/0_107a60  => Usine logicielle - architecture - centrale.vsd

 Proxy devant les applications 
 Gerrit 
 gerrit.uem.lan => lppxyifr1 => lpsrvgit2:8080
 gerritrec.efluid.uem.lan (ou gerritrec) => lpsrvperf4 => ldsrvgit3:8081

 Artifactory 
inventaire

 Jenkins 
 usinelogicielle.uem.lan => XXX => lpsrvedt1:X
 usinevalidation.uem.lan => XXX => lpsrvedt1:X
 usinerec.efluid.uem.lan => lpsrvperf4 => lpsrvedt1.uem.lan:8380   (ne gère pas CJOC avec HTTPS donc pas utilisable mais on ne fait pas plus car CJP est fin de vie)
 cje.efluid.uem.lan => XXX => XXX

 Sonar 

 especteur => XXX => lpsnredt1:9000
 sonarrec.efluid.uem.lan => lpsrvperf4 => lrdocedt1:9000

 Wikefluid 

 Eforum 

 Serveurs de l'UL et Outillage IT 
 ldsrvint1 

 ldsrvint3 
 SEF 371834
 RHEL 7.9
 Activation NFS manuellement pour besoins playbooks IT provisionning :
   # yum install nfs-utils rpcbind
   # systemctl enable nfs-server
   # systemctl enable rpcbind
   # systemctl enable nfs-lock
   # systemctl enable nfs-idmap
   # systemctl start rpcbind
   # systemctl start nfs-server
   # systemctl start nfs-lock
   # systemctl start nfs-idmap
    
 ldsrvint4 
 SEF 371834
 RHEL 8.5

 Particularités sur serveurs INT/DEV 

 Serveurs en DMZ 
Certains serveurs sont en DMZ, ce qui contraint l'accès et les montages Samba, car déconnectés de l'AD.
Dans ce cas, création d'un user technique sur le serveur : fldadmin_ro
 useradd fldadmin_ro
 usermod -a -G fldgrp fldadmin_ro

Ensuite, recopier la clé publique des utiliisateurs souhaitant accéder aux logs des serveurs dans /home/fldadmin_ro/.ssh/authorized_keys

Les serveurs pour le moment concernés sont :
 lds2medt1
 lpdocedt2

 Serveurs UL intégrés au parc de tests S&R sécurité 

* ldsanbld7.uem.lan: serveur NB V12 Weblogic
 lpdocedt2 : serveur NB docker V13
 ldsrvgit3: serveur gerrit de recette
* lddocedt1 RHEL 7.2/docker 1.10: serveur docker de test
 lpsrvulo3.uem.lan: swarm/docker RH 7.2
* lrsrvart2.uem.lan: artifactory de recette
 lrdocedt1 : Serveur applications de recette sous Docker
 lpdoculo3 : Slave UL
 lrmysedt1 : serveur mysql de recette

 Jobs de non régression de l'usine logicielle 
Les jobs suivants permettent de tester la non regression des fonctions de l'usine logicielle. Ces jobs doivent être complétés au fur et à mesure des anomalies et des nouvelles fonctions mis dans Jenkins. Cela permet de valider les différents upgrade fait sur les serveurs de tests.
 http://lpsrvedt1.uem.lan:8380/job/FtestsNonRegression

 Mise à jour de patchs 
 Q1 T10 du 16/08/2016 
Aucun serveurs UL n'est présent dans le cadre de ces patchs.

 Travaux temporaires en recette pour site sur 
Sur Jenkins, onglet SiteSur (à rebasculer sur un serveur de production quand répartition des jobs revue et stable)
- 1 job (bindé au master car il monte le NAS) pour cloner efluid sur le NAS qui sera monté
- 1 job qui lance sur lpsrvart1 le script main_sitesur.sh => récupération de l'archive sur NAS

Sur serveur Artifactory lpsrvart1, user artifactory, ajout dans .ssh/authorized_keys de la clé publique id_rsa.pub de jenkins@lrsrvulo2 =>
 permet au job de lrsrvulo2, lancé en tant que jenkins, de pouvoir lancer une commande ssh sur lpsrvart1 pour faire la procédure d'export.

 Plan de Sauvegarde  

 Serveurs physiques 

LPSRVEDT1 : (/etc /opt /home /data/EDT/jenkinsUA /data/EDT/jenkinsUD /data/EDT/jenkinsUL /data/EDT/jenkinsUR /data/EDT/jenkinsUV /data/EDT/migration /data/EDT/interneUL) incremental tous les jours avec rétention 4 semaines + full 1 fois par 2 semaine avec rétention 4 semaines

LPBDDEDT3 : full le samedi rétention 2 semaines

LDBDDEDT1 : full le samedi rétention 2 semaines

LPSRVEDT2 (provisionnement en cours sur le modèle LPSRVEDT1)

 Serveurs virtuels 

Sauf indication contraire tous les serveur virtuels sont sauvegardée 1 fois par semaine avec rétention 4 semaines,
si le jour n'est pas précisé c'est un dimanche

LDSANBLD[1-7] + LDSAQTP1 : full 1 fois par semaine avec rétention 4 semaines

LDSRVULO1 : full 1 fois par semaine avec rétention 4 semaines

LPDOCEDT1 : incremental tous les jours ouvrés avec rétention 2 semaines + full 1 fois par semaine avec rétention 4 semaines

LPDOCEDT2 : incremental tous les jours ouvrés avec rétention 2 semaines + full 1 fois par semaine avec rétention 4 semaines

LDSRVULO1 : full 1 fois par semaine avec rétention 4 semaines

LDSRVINT1 : full 1 fois par semaine avec rétention 4 semaines

LPSRVULO12 : incremental tous les jours avec rétention 4 semaines + full 1 fois par 2 semaine avec rétention 4 semaines

LPSRVULO4, LPSRVULO6, LPSRVULO7 : incremental tous les jours avec rétention 4 semaines + full 1 fois par 2 semaine avec rétention 4 semaines

LPSRVULO9 : incremental tous les jours avec rétention 4 semaines + full 1 fois par 2 semaine avec rétention 4 semaines

LPSRVEDT2 (provisionnement en cours)

LPMYSEDT1 : incrémental tous les jours ouvrés dont full le vendredi. Interruption de service de 4h45 à 5h30

LPSRVGIT2 : incrémental tous les jours avec rétention 4 semaines + full 1 fois par 2 semaine avec rétention 4 semaines

LPSRVGIT1 : incrémental tous les jours avec rétention 4 semaines + full 1 fois par 2 semaine avec rétention 4 semaines

LDSRVGIT3 : full 1 fois par semaine avec rétention 4 semaines

LDSTRSNB1 : full 1 fois par semaine avec rétention 4 semaines

LRSRVART1 : full le samedi rétention 2 semaines

LRSRVART2 : RH7 avec disque CIFS pour le /data

LPSRVART1 : incremental tous les jours ouvrés avec rétention 2 semaines + full 1 fois par semaine avec rétention 4 semaines

LPSRVART2 : incremental tous les jours ouvrés avec rétention 2 semaines + full 1 fois par semaine avec rétention 4 semaines

 Outils utilisés 

OutilVersionEditeurLicenseMaven Apache Software FoundationApache 2.0 LicenseGit (nécessite curl 7.19.4, expat 2.0.1, zlib 1.2.5) et 1.8.4.4 GitGNU General Public License v2Ant  Apache Software FoundationApache 2.0 LicenseJonas  et 5.2.3Bull and OW2LGPLHudson  Eclipse Foundation project and partly as a java.net project.actuellement MIT License - prochaine version Eclipse EPLJenkins  Jenkins CIMIT LicenseArtifactory  JFrogLicense commerciale PRO artifactory accordée à UEMWeblogic (pour la compilation des JSP) OracleLicense commerciale OraclePlay (c’est pour edoc) guillaume bort & zenexityApache 2 licenceJdk1.5.0.15 et 1.6.0.27 et 1.7.0.09OracleOracle Binary Code License Agreement for Java SE and JavaFX TechnologiesSonarCodehausLGPLCheckStyleSourceForgeLGPLPMDSourceForgeBSD-styleLGPLFindBugsSourceForgeLGPLCoberturaSourceForgeApache Software License, Version 1.1. + GNU General Public License, Version 2.0SeleniumOpenQAApache 2.0 LicenseSQLPlus et 11.2.0OracleLicense commerciale oracle [HP QC] (pour tests QTP)  10.0 + QC add-in 9.0 HP License commerciale HPStreamServeOpentextLicense commerciale Opentext

 Projet en cours 
 Évolution Usine Logicielle vers CJE 
Evènement Suivefluid: 194645
Page wiki : CJE

 Nouveaux serveurs physiques 
 Caractéristiques techniques : B420 M4, quadri-processeurs E5-4650v3 (12 cœurs hyperthreadés, 48 cœurs en tout) avec 192 Go de RAM.
 DI : http://wperoom3.uem.lan/eRoom/Production/DemandesProductionInformatique/0_1a7762
 LPDROGONEDT1
 LPVISERIEDT1
 Espace NAS : 2 To disponible en NFS sur tous les serveurs UL

 Plus le petit dernier : http://wperoom3.uem.lan/eRoom/Production/DemandesProductionInformatique/0_1ba15e 
 LPRHAEGAEDT1

 Nouvelles VM 
 DI: http://wperoom3.uem.lan/eRoom/Production/DemandesProductionInformatique/0_1b96e6

 Nouvelles VM pour env Recette CJE 
http://wperoom3.uem.lan/eRoom/Production/DemandesProductionInformatique/0_1bf392

 Cartographique de la nouvelle plateforme 

 Stratégie de migration 
 Cf http://wperoom3.uem.lan/eRoom/Prod6/ProcessFabricationEfluid/0_9671

 Licensing 
 A l'utilisateur, en comptabilisant le profil LDAP "utilisateur" de l'application "usinelogicielle"<!!!>Category:outil
Category:HMailServer
Cette page décrit comment envoyer des mails en dev avec les classes de mail de l'archi (com.imrglobal.framework.mail.Mail et autres). Ceci n'est pas un document de configuration, mais bien d'utilisation rapide. Il y a de la documentation plus complète sur eroom :

efluid - Qualité Développement > Guides et procédures > guide environnement de développement > outils et serveurs > Installation et configuration serveur mail et FTP.doc

 Généralités 

Il est possible d'utiliser le serveur de mail mailfluid, qui pointe vers Lausanne. Sur ce serveur il y a un serveur de mail de test (hMailServer) qu’on utilise et qui a plusieurs adresses mail de test, par exemple test1@efluid.net, test2@efluid.net, etc. Il est aussi possible d'utiliser un serveur de mail (par exemple hMailServer) installé en local.

En utilisant mailfluid, il faut absolument envoyer vers des mails qui se terminent en @efluid.net.

Le serveur de mail "mailfluid" est situé à l'IP suivant depuis Paris: 162.70.248.73.

 Configuration projet 

 Envoi 

Dans le framework2.properties, renseigner le nom du serveur mail.

SERVEUR_MAIL=mailfluid

 Configuration utilisateur 

Afin de s'envoyer des messages en dev, il est préférable de se créer un utilisateur afin de mettre le bon mail, et cela permet de tester aussi les nom, compléments noms, etc. Il faut d'abord trouver le LDAP sur lequel la connexion est effectuée, disponible dans le framework2.properties :

LDAP_HOST=hermesldap

Ensuite, il faut créer / modifier un utilisateur.

 Se connecter à ELDAP
 Aller à "http://envtsefluid/gestionEnvtsClients/"
 Choisir "LDP", n'importe la version
 Entrer "habilLDP" / "mdp" comme login
 Changer le LDAP avec le menu "changer connexion LDAP"
 Choisir le bon LDAP en fonction de ce qui est paramétré dans l'application de dev, voir ici-haut
 Créer / Modifier une personne
 Aller dans le menu "personne"
 Se créer / chercher
 Modifier l'entrée "mail"
 Ajouter à cette personne des postes
 Aller dans le menu "poste"
 Chercher le bon poste en s'entrant dans la recherche
 Modifier le poste, puis affecter les profils nécessaires pour les applications en dev

L'utilisateur est maintenant correctement configuré, normalement vous devriez pouvoir vous connecter directement à l'application en dev et utiliser votre nouveau mail !

 Spécificités briques (archi, ecore, edk) 

Les briques n'ont pas de profil, ils utilisent les profils des applications cibles. Il faut renseigner dans le fichier framework2 de quelle application les profils viennent.

LDAP_ATT_APPLICATION_HERMES=efluid

Par contre, en entrant vos identifiants de connection, c'est la requête suivante qui fera la recherche en base pour tester les habilitations de l'application :

select ...
from TPROFILHABILITATION A 
where (A.IDENTIFIANTLDAP= 'cn=efluid administrateur,ou=efluid,ou=Profil,o=uem,dc=uem-metz,dc=fr') and mod(A.ETATOBJET,2)=0

Il faut donc possiblement rajouter des habilitations pour l'application cible si elles n'existent pas. Par exemple :

insert into TPROFILHABILITATION ... 
VALUES ('P2','efluid administration',...,'cn=efluid administration super utilisateur,ou=efluid,ou=Profil,o=uem,dc=uem-metz,dc=fr',...);

 Spécificités Paris 

À Paris, il y a un LDAP différent, il ne sert à rien de faire l'étape ici-haut pour s'ajouter au LDAP de développement, on ne s'y connecte pas normalement. Le plus simple c'est tout simple de se connecter à celui de Metz en ajoutant au fichier framework2 ceci :

#LDAP_HOST=hermesldap
LDAP_HOST=172.16.1.27

 Configuration poste 

 Virus Scan 

Il faut désactiver l’antivirus qui bloque parfois l’accès au port 25 pour limiter la diffusion massive d’email. Il faut ouvrir la console VirusScan.

Fichier:Console-virusscan.JPG

Puis désactiver les analyseurs à l'accès, comme suit. Attention, les éléments se réactivent après 15 minutes.

Fichier:Console-virusscan-2.JPG

 Client mail 

Il est plus facile d'utiliser Thunderbird afin de ne pas pourrir son Outlook, sinon les configurations sont les mêmes.

 Vos nom et prénom : test4 (n'importe)
 Adresse électronique : test4@efluid.net
 Mot de passe : test4
 Serveur entrant
 Protocole : IMAP
 Nom d'hôte du serveur : mailfluid
 Port : 143
 Serveur sortant
 Protocole : SMTP
 Nom d'hôte du serveur : mailfluid
 Port : 25
 Identifiant : test4@efluid.net

Fichier:Configuration-thunderbird-mailfluid-01.JPG

 Comptes disponibles 

 test1
 UN : test1@efluid.net
 PW : test
 test2
 UN : test2@efluid.net
 PW : test
 test3
 UN : test3@efluid.net
 PW : test3
 test4
 UN : test4@efluid.net
 PW : test4

 Erreurs courantes 

 452 

Se produit de manière sporadique lors des premières exécutions, rééexecuter (changer "MAIL_FROM_ADDRESS" pour un truc en "@efluid.net" semble aider).

 com.sun.mail.smtp.SMTPSendFailedException: 452 Insufficient system storage

 500 

Vous utilisez une adresse où mailfluid ne peut envoyer (l'adresse ne peut pas terminer en autre chose que "@efluid.net").

 com.sun.mail.smtp.SMTPAddressFailedException: 550 Delivery is not allowed to this address.

 Connection refused 

Votre client McAffee n'est pas désactivé (il faut tout désactiver sinon le port 25 est bloqué).

 javax.mail.MessagingException: Could not connect to SMTP host: mailfluid, port: 25;
  nested exception is:
   java.net.ConnectException: Connection refused: connect<!!!>Category:outil
Category:oracle

 Presentation 
 Documentation 
PTI installation moteur et client oracle database 12.1.0.2.0 : http://WPEROOM2.uem.lan/eRoom/Production/GestionProjetEfluid/0_6219a

Documentation officielle : http://docs.oracle.com/en/database/database.html

 Patches 
 Procédure installation Patch 
La documentation se trouve sur eroom : 

 Oracle Database 12.1.0.2.0 
Patch ID 17365043 : 171716. Permet de résoudre un problème de dimensionnement de la zone mémoire STREAMS_POOL_SIZE.

 Procédures divers 
 Création trigger pour suivi connexions si paramètre oracle audit_trail=NONE 
1. Se connecter sur la BDD en tant que SYS/SYSTEM

-> Attention à se placer sur le container souhaité en oracle12c !
   alter session set container=<container> ;

2. Créer le trigger suivant :
CREATE OR REPLACE TRIGGER logon_denied_write_alertlog AFTER SERVERERROR ON DATABASE
DECLARE
 l_message varchar2(2000);
BEGIN
 -- ORA-1017: invalid username/password; logon denied
 IF (IS_SERVERERROR(1017)) THEN
 select 'ORA-1017 - Erreur de connexion instance "'
	|| sys_context('USERENV' ,'SERVICE_NAME') 
	|| '" using "'
	|| sys_context('USERENV' ,'AUTHENTICATED_IDENTITY') 
	||'" adresse_ip "'
	|| sys_context('USERENV' ,'IP_ADDRESS') 
	|| '" host "'
	|| sys_context('USERENV' ,'HOST') 
	||'" osuser "'
	|| sys_context('USERENV' ,'OS_USER')
	|| '"'
 into l_message
 from sys .v_$session
 where sid = to_number(substr(dbms_session.unique_session_id,1 ,4), 'xxxx')
 and serial# = to_number(substr(dbms_session.unique_session_id,5 ,4), 'xxxx');
 -- write to alert log
 sys.dbms_system.ksdwrt( 2,l_message );
 END IF;
END;
/

3. Consulter le fichier alerte_<nom_instance>.log dans le répertoire d'alerte de l'instance avec l'erreur ORA-1017

Exemple erreur générée : 
Wed May 10 17:12:41 2017
ORA-1017 - Erreur de connexion instance "ruemedt1" using "TOTO" adresse_ip "192.168.141.158" host "PC2126" osuser "wozniak"

 Application d'un PSU 
 Récupérer les binaires d'installation du patch

Rajouter l'entrée suivante dans le fichier /etc/fstab :

lpnfssan1-edt:/NFS_LRBDDTECX_SVG_A04/installation_oracle/patch/

 Vérifier espace disque disponible sur $ORACLE_HOME

Utiliser la commande suivante pour vérifier :
[oracle ]$ df -h $ORACLE_HOME

L'espace disque disponible doit être de 1 Go

 Vérifier la compatibilité avec l'installation
Se placer dans le répertoire du patch, puis lancer l'outil opatch :
[oracle ]$ $ORACLE_HOME/OPatch/opatch prereq CheckConflictAgainstOHWithDetail -ph ./

Le résultat doit indiqué qu'il n'y a aucun conflit. Exemple :
Oracle Home       : /opt/oracle/product/11.2.0.4/db
Central Inventory : /opt/oracle/oraInventory
   from           : /opt/oracle/product/11.2.0.4/db/oraInst.loc
OPatch version    : 11.2.0.3.11
OUI version       : 11.2.0.4.0
Log file location : /opt/oracle/product/11.2.0.4/db/cfgtoollogs/opatch/opatch2017-05-31_17-45-37PM_1.log

Invoking prereq "checkconflictagainstohwithdetail"

Prereq "checkConflictAgainstOHWithDetail" passed.

OPatch succeeded.

 Vérifier que les binaires oracle ne sont plus utilisés
 Les bases de données doivent être éteintes
 Le listener doit être éteint
 Les processus GoldenGate doivent être stoppés

Pour vérifier : 
[root ]# lsof $ORACLE_HOME

 Appliquer le patch
Toujours dans le répertoire du patch
[oracle ]$ $ORACLE_HOME/OPatch/opatch apply ;

Remarque : si blocage, se référer au lien ci-dessous Blocage lancement opatch

 Redémarrer la BDD et lancer le script
Les actions sont différentes suivante la version oracle
- Pour oracle 11g
[oracle ]$ cd $ORACLE_HOME/rdbms/admin
[oracle ]$ sqlplus / as sysdba
SQL> startup open
SQL> @catbundle.sql psu apply

 Pour oracle 12c
Action supplémentaire à réaliser pour oracle 12c
Important : Vérifier que toutes les PDB sont en mode OPEN (la PDB PDB$SEED reste en read-only)
[oracle ]$ cd $ORACLE_HOME/OPatch/opatch
[oracle ]$ ./datapatch -verbose

 Vérifier le niveau de patch PSU
 Avec l'utilitaire OPatch
Permet de savoir si les binaires sont patchés, mais pas la BDD oracle.
[oracle ]$ cd $ORACLE_HOME/OPatch/opatch lsinventory
 En base de données
set linesize 300
COLUMN action_time FORMAT A20
COLUMN action FORMAT A10
COLUMN bundle_series FORMAT A10
COLUMN comments FORMAT A30
COLUMN description FORMAT A40
COLUMN namespace FORMAT A20
COLUMN status FORMAT A10
COLUMN version FORMAT A10
SELECT TO_CHAR(action_time, 'DD-MON-YYYY HH24:MI:SS') AS action_time,
       action,
       namespace,
       version,
       id,
       comments,
       bundle_series
FROM   sys.registry$history
ORDER by action_time;

 Mettre à jour le drivers dans les applications efluid 

un change dans l'archi pour les applications basées dessus : https://gerrit.efluid.uem.lan/c/archi/+/286459

un change dans efluidUtilsPom pour celle n'utilisant pas l'archi : https://gerrit.efluid.uem.lan/c/efluidUtilsPom/+/286458

 Liens internes 
 SQL
 Documentation sur les triggers
 Documentation sur les variables de contexte SYS_CONTEXT 
 Blocage lancement opatch <!!!>Gerrit est une application web de revue de code pour le travail en équipe. C'est un logiciel gratuit développé chez Google par Shawn Pearce (†). Il est distribué sous la Licence Apache V2. Il permet à chaque utilisateur de lire, approuver ou rejeter les modifications du code source via un navigateur web. Celui-ci s'utilise avec le gestionnaire de version Git.

 Assistance / Formation 

Si vous éprouvez des difficultés avec GIT/GERRIT, contactez :
 Sur Metz : Vincent Poutissou
 Sur Paris : David Bodin
 A défaut : 
Si les messins ne sont pas joignables, tentez avec les parisiens. Et inversement.

 Supports de formation 
Voici différents supports pour vous former à utiliser GIT/GERRIT :

 Guides développeur 
 Prérequis d'utilisation de GERRIT 
Configuration de GIT :
 Version de Git : > 2
 Suivre le guide de configuration du poste développeur pour Gerrit
 Navigateur web différent de Internet Explorer
 Avoir les droits sur le référentiel Git comme publicateur ou relecteur

 Liens Utiles 
 Liens vers l'application Gerrit Revue de code : http://gerrit.efluid.uem.lan
 Guide des cas d'utilisations de Gerrit : cas d'utilisations
 Les IP depuis CPL sont :
 gerritssh et gerritssh.efluid.uem.lan : 192.168.106.93
 gerrit et gerrit.efluid.uem.lan : 192.168.106.10
 Clone referentiel Gerrit
 Suivre les actualités Gerrit via un flux RSS

 Personnalisation des menus 
 Il est possible de personnaliser les menus en utilisant des requêtes.

 Pour ce faire, aller dans VotreUser (1) > Settings(2) > Préférences(3)

default

 Ensuite aller au chapitre My Menu et entrer le nom de votre nouveau menu et dans l'url écrivez votre requête.

Par exemple une requête qui permet de voir mes changes qui ont été mergés dans Efluid branche develop : 

 Name : Changes merged efluid develop
 URL : #/q/owner:self status:merged branch:develop project:efluid

Une fois terminé appuyez sur le bouton + (1) puis Save changes (2) 

default

 Une fois les manipulations précédentes terminées votre menu personnalisé apparaît.

default 

 Exemple de requête  : Requêtes de recherche.

 Requêtes de recherche gerrit  
Documentation synthaxique : Documentation gerrit

 Requête  Commentaire owner:self is:open label:Code-Review=2 label:Verified=1 NOT label:Verified-1 NOT label:Code-Review-2  Permet de connaitre vos changes qui sont prêts à être submit owner:self is:open (label:Verified-1 OR label:Code-Review-2)  Permet de connaitre vos changes qui sont bloqués par un -1 ou -2 reviewerin:"Validateurs SQL Suite efluid et Enercom" status:open  Permet de connaitre les changes qui ont besoin d'une revue SQL

 Conventions de commit 
 Les messages de commit doivent respecter la norme suivante
 <REFERENCE_EVT_SUIVEFLUID> : <message du commit>

(<message du commit> : décrire la/les modification(s) effectuée(s) ; cette information servant à alimente le bon de livraison)
 Si l'on veut indiquer d'autres évènements (en plus de l'évènement principal) dans le commit il faut le faire de la manière suivante : 
 <REFERENCE_EVT_SUIVEFLUID> : #<REFERENCE_AUTRE_EVT_SUIVEFLUID> <message du commit>

Exemple de variante possible :
  120572 : [dependencies] ecore-4.XX.100
    -  #123456 : correction XXX

 "Tags" de commit 

il est recommandé de précéder le commentaire de message d'un tag de la liste suivante :
 [fix] évolution du code suite à une ano
 [feat] déploiement de nouvelle fonction
 [tests] mise à jour des TU/TI sans toucher au code métier

 Editer un message de commit via gerrit 

Sur votre changeset, cliquez sur le bouton "Edit" (Entre les informations générales du changeset et la liste des fichiers modifiés/créés/supprimés). La liste des fichiers devient modifiable.
Cliquer sur "Commit message" ; attention, vous ne devriez pas toucher aux autres fichiers du commit. En effet, gerrit ne conserve pas forcément l'encodage ni le formatage du projet.
Modifiez le message du commit
Cliquez sur le bouton "Done editing" (bouton qui a remplacé le bonton "Edit")
Republiez le message de commit : bouton "publish" en haut de l'interface gerrit.
Fichier:Edit_message_gerrit_1.png
Fichier:Edit_message_gerrit_2.png

 Pour aller plus loin 

Normes des messages gerrit du groupe FAC: Format FAC
Normes des messages gerrit du groupe Portail: Règles Portail
Un article intéressant à lire : https://chris.beams.io/posts/git-commit/

 Vocabulaire 
 Change : correspond à un commit en attente de revue de code. Commit poussé sur la branche refs/for/*
 PatchSet : correspond à une des versions d'un change. Chaque fois qu'un 'commit --amend' est effectué sur un change, Gerrit crée un nouveau pathchSet.
 Dépôt principal : contient l'ensemble des commits validés. Seuls les commits présents sur ce dépôt seront récupérés lors d’un pull. Le dépôt principal correspond à la référence refs/heads/*
 Dépôt gerrit : contient l'ensemble des changes. Ce dépôt permet de stocker les commits en attente d’une revue de code. Ceux-ci ne pourront pas être récupérés lors d’un pull, tant qu’ils ne seront pas mergés sur le dépôt principal. Le dépôt gerrit correspond à la référence refs/for/*
 Submit : Action qui autorise Gerrit à merger un change sur le dépôt principal.
 Abandon : Action pour archiver un change sans qu'il ne soit mergé sur le dépôt principal. Un change abandonné peut être restauré ultérieurement.
 Project : correspond à un dépôt Git. Il regroupe le dépôt principal et le dépôt Gerrit.
 Change-Id : Id commun à tous les patchSet d'un change.

 Signification des labels 
Les labels constituent les "notes" affectés à chaque ChangeSet Gerrit. Pour qu'un changeSet puisse être mergé il faut que tous les labels obligatoires soient valides, selon les règles suivantes : 
 CR : Code Review
 Note obligatoire, attribuée manuellement
 Attribution par des relecteurs / validateurs
 va de -2 à +2
 au moins un +2 est nécessaire pour qu'un changeSet soit "mergeable"
 aucun -2 ne doit être présent pour qu'un changeSet soit "mergeable"

 V : Verified
 Note obligatoire, attribuée automatiquement
 Attribution par l'usine logicielle
 va de -1 à +1
 un +1 est nécessaire pour qu'un changeSet soit "mergeable"
 un -1 rend le changeSet non "mergeable"

 MR : Maven Review
 Note optionelle, attribuée manuellement, uniquement si le changeSet contient des fichiers Maven (pom.xml, *.pom)
 Attribution par les membres du process de fabrication
 va de -1 à +1
 un +1 est nécessaire pour qu'un changeSet soit "mergeable"
 un -1 rend le changeSet non "mergeable"

 SR : SQL Review
 Note, attribuée manuellement, uniquement si le changeSet contient des modification SQL (fichiers présents dans /sql/database/*)
 Attribution par les validateurs SQL
 va de -1 à +1
 un +1 est nécessaire pour qu'un changeSet soit "mergeable"
 un -1 rend le changeSet non "mergeable"

 Signification des notes des revues de codes (CR) 
 Note Signification Catégorie disposant de la note -2Ce changeSet ne doit pas être soumis car comporte des erreurs trop importantes. Le changeSet est bloqué par le relecteur/validateur.Relecteur ou Validateur -1Ce changeSet comporte des erreurs qu'il serait intéressant de corriger tout de suite. Note non bloquante.Relecteur ou Validateur 0Note utilisée pour des remarques d'ordre générales.Relecteur ou Validateur +1Ce changeSet est OK du point de vue du relecteur mais nécessite une validation d'un validateur.Relecteur ou Validateur +2Ce changeSet est OK du point de vue du validateur.Validateur

Guide de Notation

 Processus 
 Demande de création d'un nouveau projet dans Gerrit 
Créer un évènement dans suivefluid en suivant le modèle de l'événement 388461 et le transmettre par mail à usinelogicielle.

 Workflow de publication dans gerrit 

Processus à suivre pour tous développements qui passent par une revue de code :

 Le développeur soumet son code par les mécanismes habituels (git pub) à gerrit
 Le développeur indique un ou plusieurs relecteur de son changeSet Gerrit
 Le job de compilation auto ajoute +1 si la compilation est ok
 Le relecteur fait la revue de code et ajoute +1
 En cas de modification SQL (scripts DML, modifications DDL...), un relecteur SQL de l'équipe études et performances fait la revue et ajoute +1
 Un validateur du dépôt ajoute +2
 Le développeur submit son code

Les validateurs sont indiqués dans les descriptions des projets Gerrit.

 Listes de diffusion 
Validateurs xxx (autocomplété dans gerrit).

 Workflow de soumission de code en cas de code-freeze 
Le processus de soumission de code en cas de branche gelée est disponible ici : http://WPEROOM2.uem.lan/eRoom/Production/QualiteDeveloppementEfluid/0_18703f (http://wperoom2.uem.lan/eRoom/Production/QualiteDeveloppementEfluid/0_43ed8)

Le diagramme principal est repris ici, mais c'est le document précédent qui fait foi : 
Fichier:Processus soumissionDeCodeSurBrancheGelee - Macro.png

 Configuration des labels 

 Documentation gerrit 
Config label

 Désactiver un label pour un projet 
Aller dans le project.config du projet puis ajouter function = NoOp

exemple pour mvn review : 

 [label "MVN-Review"]
   function = NoOp

 CODE-REVIEW 

[label "Code-Review"]
  function = MaxWithBlock
    copyMinScore = true
    value = -2 Do not submit
    value = -1 I would prefer that you didn't submit this
    value =  0 No score
    value = +1 Looks good to me, but someone else must approve
    value = +2 Looks good to me, approved
    defaultValue = 0
    copyAllScoresOnTrivialRebase = true
    copyAllScoresIfNoCodeChange = false

 Verified 
    [label "Verified"]
      branch = refs/heads/develop
      branch = ^refs/heads/maintenance_.*
      function = MaxWithBlock
      value = -1 Fails
      value =  0 No score
      value = +1 Verified
      defaultValue = 0
      copyAllScoresOnTrivialRebase = false
      copyAllScoresIfNoCodeChange = true 

 SQL-REVIEW 
  [label "SQL-Review"]
    function = MaxWithBlock
        value = -1 Not valid
        value =  0 No score
        value = +1 Verified
        defaultValue = 0
        copyAllScoresOnTrivialRebase = true
        copyAllScoresIfNoCodeChange = true

 MVN-REVIEW 
 [label "MVN-Review"]
    function = MaxWithBlock
        value = -1 Not valid
        value =  0 No score
        value = +1 Verified
        defaultValue = 0
        copyAllScoresOnTrivialRebase = true
        copyAllScoresIfNoCodeChange = true

Le MVN-REVIEW est désactivé pour les projets : OracleTools et Performance

 plugins 
X-docs
gerrit-suivefluid

 Archives 
 Guide sur la migration des projets : La migration sous gerrit pour les nuls

Category:Outil
Category:Gerrit

 Problème lié à Gerrit 
Il est formellement interdit de modifier les fichiers dans Gerrit car cela modifie l'encodage de la classe<!!!>Category:outil
Category:JXplorer
Outil permettant de consulter facilement les données présente dans un annuaire LDAP.

Lancement de JXplorer

 Lancer JXplorer depuis l’icône.
image:IconeJXplorer.jpg
600px

Connexion à l'annuaire
 Fenêtre  Action JXplorer  Se connecter à un annuaire LDAP « Fichier » -> « Se connecter » Ouverture d’une connexion LDAP/DSML  Choisir un template déjà défini.
Ou remplir la fenêtre avec les paramètres de connexion de votre base ldap

image:ConnectionLdap-JXplorer.jpg

Utilisation

600px
 Partie gauche de la fenêtre : Navigation dans les branches de l’annuaire
 Partie droite de la fenêtre : Visualisation des données de l’entrée sélectionnée 

Ajouter une entrée

image:AjouterEntree-JXplorer.jpg
 Saisir la valeur cn de la nouvelle entrée et valider
image:CreationEntree-JXplorer.jpg
 Remplir les champs obligatoires (en gras) et optionnels, puis soumettre
600px
 La nouvelle entrée doit apparaitre dans la fenêtre de gauche.
image:ControleNouvelleEntree-JXplorer.jpg<!!!>Utilisé pour analyser les performances d'une application Java.

Category:outil

 Prérequis 

Il est nécessaire de paramétrer, avant le lancement de la JVM à monitorer, les différents éléments du connecteur JMX, tel que décrit ici : JVM

 Connexion 

 Serveur d'applications 
 Récupérer la chaine de connexion JMX du serveur, dans le fichier de log Jonas par exemple : service:jmx:rmi://lrbddtec4/jndi/rmi://lrbddtec4:9198/jrmpconnector_jonas-1
 Ouvrir VisualVM
 Dans l'arbre de gauche, cliquer avec le bouton de droite sur "Remote". Selectionner "Add Remote Host"
Fichier:VisualVM_001.png

 Renseigner le nom ou l'IP du serveur sur lequel se trouve le serveur d'application.
 Cliquer avec le bouton de droite sur le serveur nouvellement créé, selectionner "Add JMX Connection"
Fichier:VisualVM_002.png

Le serveur d'application doit etre lancé avant de dérouler les étapes ci-après :
 Renseigner le service avec l'URL JMX récupéré dans l'étape 1.
 Double cliquer sur le connecteur JMX nouvellement créé.
Fichier:VisualVM_003.png

 Batchs 

 Récupérer le numéro de port JMX configuré dans la chaine de lancement du batch (voir JVM) 
 Ouvrir VisualVM
 Dans l'arbre de gauche, cliquer avec le bouton de droite sur "Remote". Selectionner "Add Remote Host"
Fichier:VisualVM_001.png

 Renseigner le nom ou l'IP du serveur sur lequel s'execute le batch.

Le batch doit etre lancé avant de dérouler les étapes ci-après :

 Cliquer avec le bouton de droite sur le serveur nouvellement créé, selectionner "Add JMX Connection"
Fichier:VisualVM_004.png

 Renseigner le service en concaténant IP/NOM et PORT récupéré dans l'étape 1.
 Double cliquer sur le connecteur JMX nouvellement créé.
Fichier:VisualVM_005.png

 Utilisation 
 Paramétrage JVM 
l'onglet "Overview" donne tous les paramétres JAVA chargés au lancement de la JVM, ainsi que les variables systèmes.

 Memoire, CPU et GC 
L'onglet "Monitor" affiche les graphiques temps réel :
 de charge CPU du serveur
 de charge GC de la JVM
 d'utilisation de la Heap et la PermGen de la JVM
 du nombre de classes chargées
 du nombre de threads actifs

Fichier:VisualVM_006.png

 Threads 
L'onglet "Threads" affiche en temps réels l'état de tous les threads tournant dans la JVM.

Cet onglet permet aussi de générer des threads-dump sous forme de fichier texte.

Le sous-onglet "Table" affiche les statistiques d'executions par Thread.

Fichier:VisualVM_007.png<!!!>Category:plugin
Category:Construction Zip D'Eclipse
Category:maven

Plugin eclipse pour maven; aussi appelé m2eclipse.
 Installation 
Dans eclipse Help > Install New Software ... ajouter les plugins
 m2e - Maven Integration for Eclipse
 m2e - sl4j over logback logging (Optional)
via l'url http://download.eclipse.org/technology/m2e/releases

 Bug déclarés 
 Lors d'un time out sur la récupération d'un artifact la console écrit "Downloaded" alors qu'on devrait plutôt avoir "Downloading"  => Statut : ouvert

 Liens utiles 
 Site officiel : http://eclipse.org/m2e/<!!!>Category:plugin
Category:eclipse
Category:webby
Category:maven

Plugin eclipse pour maven.

Webby (web application runner) remplace WTP et a été développé par Jason Van Zyl le fondateur de Maven.

Une des grosse plus-value de webby est qu'il réutilise les classes compilées par éclipse et qu'il pointe directement sur les bibliothèques du repository maven, publish limité donc aux ressources.

Intégration complète de Maven (war overlay, scope, etc.)
Publish presque immédiat
Le serveur d'application semble plus rapide
Les chargements des classes à chaud se réalise avec succès hormis quelque fois pour les cas complexes (normal sinon il faut utiliser JRebel)
Quelques fois le rechargement à chaud des properties ne se fait pas (voir la raison) 

 Installation 

Via eclipse Help > Install New Software ... ajouter le plugin Web Application Runner via l'url : http://m2eclipse.sonatype.org/sites/m2e-webby 

(ou http://repository.tesla.io:8081/nexus/content/sites/m2e.extras/m2eclipse-webby/0.2.2/N/ pour les versions NB)
par exemple http://repository.tesla.io:8081/nexus/content/sites/m2e.extras/m2eclipse-webby/0.2.2/N/0.2.2.201211031759/

 Comment déployer une application avec webby ? 
Pour exécuter une application sur un serveur Tomcat nous utilisons le lanceur Webby. 
Faire clique-droit sur le projet, puis debug-as, puis debug configurations 
500px

 Puis dans l'onglet Webby faire clique-droit, puis New. Et renseignez les champs comme suit 
500px

 Puis faire apply, et ensuite Debug.

Si vous souhaitez déployer en même temps plusieurs applications sur tomcat, attribuer au minimum un écart de 3 entre chaque numéro de port. (8080, 8083, 8086, ... le port n+1 est utilisé pour le port AJP et le n+2 pour le port RMI)

Le Tableau des applications utilisées par le développement détails les ports / utilisateur / url qu'il est conseillé d'utilisé sur les envirronements de développement

 Comment voir les applications déployées avec webby ? 

Une vue permet de voir les applications déployées avec Webby. 
Pour l'ouvrir faire : 
Window / Show View / Others / Webby / WebApps

none

 Paramétrage 
 Si la gestion du cache des objets métier est activé, le paramétrage suivant doit être rajouté :
-Xms3000m -Xmx3000m -verbosegc -server -XX:+DisableExplicitGC -XX:+UseAdaptiveSizePolicy
-XX:+UseParallelGC -XX:+UseParallelOldGC -XX:ParallelGCThreads=8
 En général -Xms1500m -Xmx1500m c'est suffisant

 FAQ 
 Erreur au lancement de webby après avoir lancer une commande maven pour créer une JAR ? 
 On lance un goal "clean install" pour efluid avec le profils "efluid-jar"
 On lance webby et ça plante :-(!
 Il faut lancer une "clean" + "rebuild" afin de reconstruire le projet
 Je n'arrive pas à lancer deux applications différents sur tomcat, j'ai l'erreur java.net.BindException: Address already in use: JVM_Bind ? 
Il faut installer la nouvelle version du Zip d'éclipse intégrant une nouvelle version de webby contenant un patch permettant d'utiliser 3 ports : Celui précisé sur l'interface (par exemple le 8080) ainsi que les deux suivants : 8081 (pour le port AJP) et 8082 (Pour le port RMI).

 Timeout lors du lancement du webby efluidPub ou efluid avec cache activé ? 

 On lance le webby efluidPub ou efluid
 le webby renvoie un message d'erreur : "Failed to start container"

Fichier:Erreur_webby.jpg‎

 Cette erreur arrive lorsque les objets mis dans le cache sont sérialisés, il suffit d'ignorer le message
 Tant que le repertoire du cache n'est pas vidé, ce message ne devrait pas réapparaitre.
 Pour obtenir une correction complète de ce problème il faut prendre le zip eclipse intégrant la gestion du timetout de webby sur eroom

Fichier:WebbyTimeout.png

 Java Server Facets 

Après avoir configuré la JRE avec la version 1.5 j'ai l'erreur : 

image:Projet-facets-version.JPG

Cette erreur n'empêche pas la compilation mais est ennuyante. Il faut modifier à la main le fichier .settings/org.eclipse.wst.common.project.facet.core.xml en changeant la ligne :

  <installed facet="java" version="1.6"/>

pour

  <installed facet="java" version="1.5"/>

 liens externes 
  https://github.com/sonatype/m2eclipse-webby
  https://docs.sonatype.org/display/M2ECLIPSE/Integration+with+Maven+WAR+Plugin<!!!>Category:outil
Category:JMeter
Category:Test

JMeter est un logiciel permettant de faire des tests de charge orientés performances ou orientés métiers sur différentes protocoles ou technologies. Il est développé par la Fondation Apache, en tant que sous projet Jakarta.
C’est une application entièrement Java avec une interface graphique utilisant l’API Swing, pouvant donc fonctionner sur tout environnement / poste de travail acceptant une machine virtuelle Java, par exemple : Windows, Linux, etc.

 liens  
 Guide d'utilisation de JMeter
 JMeter utilisation du template d'enregistrement
 archives
 JMeter pour échanges efluid_énercom.doc
 Configuration Files JMS JMeter.docx<!!!>Utilisé pour reconstruire un fichier Java à partir d'un fichier d'extension .class

Category:outil

 Prérequis 

 Connexion <!!!>Category:outil

Cet outil est utilisé pour visualiser des fichiers de très grande taille (>1Go, par exemple). C'est l'outil idéal pour analyser des log trop gros pour être ouverts avec Notepad++ (ou des fichiers PUB générés avec une forte volumétrie d'échanges). Il offre les fonctionnalités suivantes :
Faire des recherches de chaine de caractère
Parcourir les occurences par ordre ascendant ou descendant
Possibilité de sauter à un numéro de ligne.

A utiliser avec Notepad++ pour voir un bloc copié-collé depuis LTFViewr.<!!!>GrepConsole est un plugin d'Eclipse permettant de modifier le style d'affichage de la console d'Eclipse en fonction de son contenu, de définir des actions spécifiques (hyperlien, lancement de script), et d'extraire du contenu de la console vers une autre vue d'Eclipse.
Les actions sont définies selon le contenu de chaque ligne, par le biais d'expressions régulières.
Ce plugin nécessite le jdk 1.6 pour fonctionner correctement.

 Configuration 
Le plugin est contenu dans le fichier Zip d'éclipse.
Le paramétrage par défaut est présent dans le master Workspace, et sur eRoom.

 Paramètres du plugin 
Dans Eclipse, ouvrir Window/Preferences/GrepConsole/Settings :
 Style match length : nombre maximal de caractères à utiliser par ligne pour faire la modification de style dans la console d'Eclipse. Permet des gains de performance si les expressions régulières sont longues à évaluer sur de longues lignes. Mettre 0 pour prendre toute la ligne.
 Filter match length : nombre maximal de caractères à utiliser par ligne pour faire extraire des données vers la GrepView. Permet des gains de performance si les expressions régulières sont longues à évaluer sur de longues lignes. Mettre 0 pour prendre toute la ligne.
 Foreground / Background : couleurs par défaut à appliquer sur la vue GrepView.
 Link modifier key : touche à utiliser pour pouvoir cliquer sur les liens hypertextes.

Fichier:GrepConsole_Settings.PNG

 Paremètres de filtrage 
Cette partie présente rapidement la façon de modifier le paramétrage des filtre de GrepConsole.
La fenêtre de paramétrage se situe dans Window/Preferences/GrepConsole ou sur l'icone (?) dans la console d'Eclipse ou la Grep View.

Fichier:GrepConsole_manage_expression.PNG

Les expressions sont présentés dans une arborescense groupes / expressions pour permettre d'activer / désactiver tout un groupe.
Depuis cet écran il est possible d'importer / exporter des expressions (boutons Load / Save selected / Save all).
La case à cocher Fichier:GrepConsole_checkbox.PNG indique si l'expression est activée dans la console d'Eclipse.
L'entonnoir Fichier:GrepConsole_filter.PNG indique si les expressions trouvées sont recopiée dans la vue GrepView.

Voici l'écran d'édition d'une expression :

Fichier:GrepConsole_edit_expression.PNG

Sur cet écran, on configure les champs suivants :
 Name : nom de l'expression
 Expression : expression régulière permettant de définir quelles lignes doivent être concernée par le filtre.
 Unless : expression régulière indiquant les lignes devant être ignorées, même si elles correspondent à l'expression régulière définie plus haut.
 Active by defaut : identique à la case à cocher.
 Filter by defaut : identique à l'entonnoir.
 Rewrite : expression à réécrire dans la GrepView (si Filter by default est activé). On peut réutiliser des groupes capturés par des parenthèses du champ expression : il suffit de mettre entre accolade le numéro du groupe.

 Group : numéro du groupe dont le stype est à modifier (cela peut être la ligne entière, un groupe du champ Expression, la ligne réécrite en entier ou un groupe de la ligne Rewrite
 Style : style à définir pour ce groupe.
 Link : indique si on permet une action particulière si on clique sur le groupe : lien hypertexte / ouverture d'un fichier / ouverture d'une classe java / commande / lancement d'un script.

 Utilisation 
Ouvrir la GrepView dans Window/Show View/Other/Grep Console/GrepView
Il est possible de lier la GrepView à la console de son choix (Java Stack Trace, Maven, Webby...), si plusieurs consoles sont actives en même temps.

Lorsque du texte est ajouté dans la console, les modifications sont faites soit directement dans la console ou dans la vue GrepView

 Spécialiser les filtres par lanceur (Batch) 
Dans les lanceurs d'application, un onglet GrepConsole apparaît. Il possible définir les filtres à activer et désactiver par lanceur.
Cette fonction est intéressante pour désactiver les filtres inutiles pour les batchs.

Fichier:GrepConsole_Lanceur.PNG

Category:outil
Category:plugin
Category:Construction Zip D'Eclipse<!!!>Category:maven
Category:efluid
Category:installeur
Category:script installeur

L'installeur efluid (également appelé script installeur efluid) permet de mettre en place la suite efluid sur un environnement donné. Cela va de la récupération des livrables dans une version donnée, puis la montée de version du schéma de la base de données, puis le déploiement des applications (TP, batchs, Streamserve etc...) et enfin la vérification de la plateforme ainsi installée.

 Architecture 
 Vue globale 
Fichier:ScriptInstalleur architecture.png

 Principes de déploiement efluid 

600px|

 Phases de l'installeur 
 Partie package 
Voici les phases importantes de la partie package (donc sans profil Maven appelés) du script installeur
 process-test-classes => copie de la documentation de l'installeur  (attention cette phase est normalement reservée aux scripts pour la partie RUN avec profil, c'est un cas d'exception de mettre la copie de la documentation ici)
 test => copie des scripts de l'installeur, processing particulier de documentation dans les scripts installeurs des applications (Exemple dans efluid generate datamasking asciidoctor documentation)
 prepare-package => génération du site Maven, filtering des shell de l'installeur
 package => assembly final (création du zip de l'installeur applicatif)

 Documentation 
La documentation complète du script installeur se trouve ici.

Les opérations effectuées par les scripts de l'installeur sont décrites  ici

 Maintenance 
La documentation complète du script installeur branche maintenance_12 d'efluid se trouve ici.

 Convention de rédaction 
 Voici la convention de rédaction de la documentation de l'installeur :

 Constitution des fichiers 
 Les fichiers sont au format asciidoc, avec l'extension .ad
 Voici un guide rapide de la syntaxte asciidoctor : http://asciidoctor.org/docs/asciidoc-writers-guide/
 Les fichiers doivent encodés en UTF-8 (sans BOM)
 Les caractères de fin de ligne doivent être au format Windows avant archivage sous Git (CR LF)
 Chaque fichiers doit posséder l’entête suivant : 
:source-highlighter: coderay
:coderay-css: style

 Titres / Section 
 Le titre de la page doit être du format suivant, cela constitue une section de niveau 1, il peut y avoir plusieurs sections de niveau 1 dans une page
 == Section Niveau 1

 Chaque sous section est du format suivant (et ainsi de suite)
 === Section Niveau 2
 ==== Section Niveau 3

 Utilisation du gras et italique 
 Le gars et l'italique s'utilisent avec modération dans les pages quand l'utilité s'en fait sentir
 *texte en gras*
 _texte en italique_

 Listes 
 Une liste non ordonnée se fait de la manière suivante : 
 * un élément
 * un autre élément
 ** un sous élément

 Une liste ordonnée se faite de la manière suivante : 
 . premier élément
 . second élément

 Liens 
 Un lien vers une page interne (avec une ancre) se faite de la manière suivante :
 link:./includes-files.html#initOpenLdap.sh["texte du lien"]

 L'ancre en question se fait de la manière suivante (ce texte n'est pas affiché) : 
[[initOpenLdap.sh]]

 Une autre manière de procéder est d'ajouter une ancre sur du texte qui va être affiché : 
[desactiverRecuperationScriptDeployerStateless]

 Tableau 
 Voici un exemple de tableau avec 2 colonne, dont chaque colonne à la même taille
[cols="1,1"]
|===
|Groupe/Entreprise| Valeur de CODE_CENTRE

|Bonneville|BON
|RSEIPC|RSEIPC

|===

 Si l'on veut spécifier la taille des colonnes (en pourcentage l'une par rapport aux autres), alors on change la valeur de cols : 
[cols="1,2"]
|===
|Groupe/Entreprise| Valeur de CODE_CENTRE

|Bonneville|BON
|RSEIPC|RSEIPC

|===

 Si l'on veut pouvoir mettre du code asciidoc dans une colonne alors il faut le spécifier dans cols avec un "a" :
[cols="1,2a"]
|===
|Groupe/Entreprise| Valeur de CODE_CENTRE

|Bonneville|*BON*
|RSEIPC|*RSEIPC*

|===

 Attention il ne faut pas mettre de monospace dans les tableaux
 Code source 
 Pour afficher du code source en différents langages, il faut procéder de la manière suivante : 
[source,xml]
----
 <localRepository>/data/files/mavenRepository</localRepository>
----

 Pour afficher du texte avec un encart, mais sans coloration particulière il faut procéder de la manière suivante : 
----
# Utilisateur "deploy" : compte intermédiaire restreint, permettant de déployer les batchs sur le serveur
deploy@batchServeur:$ groups
batchuser deploy
----

 Si l'on veut pouvoir mettre une syntaxte particulière dans l'encart alors il faut utiliser des macros, comme par exemple ici pour mettre du texte en gras : 
[subs="verbatim,macros"]
----
pass:quotes[*Exemple pour le serveur cible batchServeur (cas n°2):*]
# Utilisateur "deploy" : compte intermédiaire restreint, permettant de déployer les batchs sur le serveur
deploy@batchServeur:$ groups
pass:quotes[*batchuser*] deploy
----

 Affichage de paramètres technique / fichiers 
 L'affichage d'un paramètre technique efluid doit se faire toujours de la meme façon c'est à dire avec un lien vers la page du paramètre et en italique :
 link:./suite_efluid.html#batch.min.connections.pool[_batch.min.connections.pool_]
 Attention si ce paramètre se trouve dans le tableau global des paramètres (fichier description.txt), alors la syntaxe est différente car c'est du html :
 <a href="#batch.min.connections.pool"><i>batch.min.connections.pool</i></a>
 L'affichage d'un nom de fichiers, ou de dossier se fait en monospace : 
 `/data/FLD_UR/testbatch/properties2`
 L'affichage de portion de configuration, ou de paramètres hors efluid se fait également en monospace : 
 `MEM_ARGS=-Xms4g –Xmx4g`

 Répertoires de l'installeur 
Les répertoires listés ci-dessous sont à renseigner dans l'installeur. Les contraintes/pré-requis existant pour ces répertoires sont listés ci-dessous (à reporter dans la documentation de l'installeur une fois stable):
 UPLOAD 
upload.temp.dir: Répertoire temporaire du serveur d'application pour uploader ses fichiers
 Création par installeur: oui
 Brique fonctionnelle owner du répertoire: TP
 Partage: Non
 Droits attendus: 750.

 REPERTOIRES 
 repertoire.reception: répertoire de dépot des fichiers dans le cadre de la fonctionnalité "demande multiple" 
 Création par installeur:  oui
 Brique fonctionnelle: TP
 Droits par défaut : 750
 Partage: non, seulement utilisé par le TP

 repertoire.partage.batch: référence le répertoire files des batchs qui doit exister au préalable ou le sera une fois le deployerBatch.sh exécuté. 
 Création par installeur: non
 Brique fonctionnelle owner du répertoire: batchs (par unzip des batchs)
 Droits attendus: accès lecture/écriture (770) pour permettre la lecture et la dépose de fichiers pour les batchs par TP.

 repertoire.resultat.exec.diff: référence le répertoire de stockage des résultats des exécutions différées 
 Création par installeur: non
 Brique fonctionnelle owner du répertoire: batchs
 Partage: oui
 Droits attendus: accès lecture (750) pour TP.

 cache.serialize.dir: répertoire dans lequel sont stockés les fichiers résultats de la sérialisation d’un objet maitre. 
 Création par installeur: oui
 Brique fonctionnelle owner du répertoire: TP
 Partage: Non
 Droits attendus: 750.

log.exception.dir: 
 Création par installeur: oui
 Brique fonctionnelle owner du répertoire: TP
 Partage: Non
 Droits attendus: 750.

 EDITIQUE 
repertoire.streamserve.in:
 Création par installeur: oui
 Brique fonctionnelle owner du répertoire: TP
 Partage: Oui, streamserve en lecture
 Droits attendus: 750

repertoire.racine.archivage.streamserve: non créé par l'installeur
    - brique fonctionnelle qui crée ?
    - droits requis (lecture/écriture), propriétaire et groupe ?
    - partagé? avec qui ?

repertoire.prefixe.archivage.streamserve: non créé par l'installeur
    - brique fonctionnelle qui crée ?
    - droits requis (lecture/écriture), propriétaire et groupe ?
    - partagé? avec qui ?

edition.sauvegarde.flux.repertoire: répertoire (chemin absolu) de sauvegarde des flux d'édition. Les traces des flux ne sont sauvegardées que si le répertoire existe (mode debug). Pas de valeur par défaut (paramètre vide) afin d'éviter de générer des traces..
 Création par installeur: oui si valorisé
 Brique fonctionnelle owner du répertoire: TP
 Partage: Non
 Droits attendus: 750

streamserve.application.dir: non créé par l'installeur, répertoire interne à Streamserve où sont les datas StreamServe (les services streamserves)
streamserve.home: non créé par l'installeur, répertoire interne à Streamserve où se trouve le moteur.

 BATCH 

batchs.log.home: 
 Création par installeur: oui (extract zip batchs)
 Brique fonctionnelle owner du répertoire: batchs
 Partage: oui, avec TP
 Droits attendus: 750.

batch.home:
    - brique fonctionnelle qui crée ? installeur batch
    - droits requis (lecture/écriture), propriétaire et groupe ? 750
    - partagé? avec qui ? TP, pour accéder aux répertoires de la sous-arborescence

batch.repertoire.resultat.exec.diff:
 Création par installeur: oui si renseigné
 Brique fonctionnelle owner du répertoire: batchs
 Partage: oui, lecture pour TP
 Droits attendus: 750.

batch.edition.output: 
 Création par installeur: oui si renseigné
 Brique fonctionnelle owner du répertoire: batchs
 Partage: oui, lecture pour StreamServe
 Droits attendus: 750.

batch.edition.output.tmp:
 Création par installeur: oui si renseigné
 Brique fonctionnelle owner du répertoire: batchs
 Partage: Non
 Droits attendus: 750.

batch.ordo.repertoire.traitement.tmp: N'existe plus. A nettoyer dans les properties.

 BASE PARAM 

ear.statefull.server.container.root.directory: non créé par l'installeur, requis pour le deploiement du TP
    - brique fonctionnelle qui crée ? 
    - droits requis (lecture/écriture), propriétaire et groupe ?
    - partagé? avec qui ?

ear.stateless.server.container.root.directory: non créé par l'installeur, requis pour le deploiement du TP
    - brique fonctionnelle qui crée ?
    - droits requis (lecture/écriture), propriétaire et groupe ?
    - partagé? avec qui ?

 Conception technique 
 Phases des scripts de l'installeur 
Le xls de travail se trouve sous http://wperoom2.uem.lan/eRoom/Prod6/ProcessFabricationEfluid/0_1d75 (phaseInstalleurEflud.xlsx)

 Lancement des scripts WLST 
Fichier:ScriptInstalleur LancementWLST.png
 Visio : 

 Vérification des paramètres obligatoires 
Fichier:ScriptInstalleur verificationParametresObligatoires.png
 Visio : 

 Embedded Docker via le script installEmbedded.sh 
Fichier:Conception shell installEmbedded mode docker.png
 Visio :  : ScriptInstalleur_shell_installEmbedded_mode_docker.vsd

 Installation de plugin 
 Mode automatique : partie TP 
Fichier:ScriptInstalleur shell upgradePlugin mode automatic.png
 Visio :  : ScriptInstalleur_shell_upgradePlugin_mode_automatic.vsd

 Mode automatique : partie batchs 
Fichier:ScriptInstalleur shell upgradePlugin partieBatch mode automatic.png
 Visio :  : ScriptInstalleur_shell_upgradePlugin_partieBatch_mode_automatic.vsd

 Mode à la demande 
Fichier:ScriptInstalleur shell upgradePlugin mode a la demande.png
 Visio :  : ScriptInstalleur_shell_upgradePlugin_mode_a_la_demande.vsd

 Spécificités dans les projets 
 efluid-migration 
 Zip des batchs
Fichier:Schema batchs efluid migration.png
Visio : 

 Release notes 
 Génération automatiques 
 Les releases notes se générent de manière automatique grâce aux messages de commits. Ceux-ci doivent respecter la forme suivante : 
 <N° Evènement> : [<APPLICATIONS>][info|verif|action][catégorie] description
 Exemple
 135000 : [SUITEEFLUID][info][Embedded] l'exemple special XXX

 info|verif|action : détermine si le message sera rangé dans une catégorie "Pour information", "Pour action", "Pour vérification"

 Catégorie, message libre qui détermine la catégorie ou l'on rangera le message de commit. Par exemple "Embedded", "batchs"...

 Application détermine la liste des applications pour lesquels cette release note sera appliquée (sachant que coté installeur il y a toujours une release note globale en plus) : 
 appli1 : release note sera appliquée uniquement à appli1
 appli1,appli2 : release note sera appliquée à appli1 et à appli2
 La liste complète des applications disponible est : efluid, efluidpub, efluidnet, ael, portailpartenaire, portailrecrutement, efluidproxy, edoc, enercom, ethaque, suivefluid, suiveclient, eldap, efluidmigration
 all : sera appliqué à toutes les applications
 none : ne sera appliqué à aucune application
 suiteEfluid (ou se) : sera appliqué à toutes les applications de la suite efluid
 La liste des applications de la suite efluid est : efluid, efluidpub, efluidnet, ael, portailpartenaire, portailrecrutement, efluidproxy, efluidmigration
 <vide> : idem à none

 Ces informations permettront de générer une release note de la forme : 
Nouveautés

    Ajout de paramètres link:./suite_efluid.html#migration.racine.repertoire.injection[migration.racine.repertoire.injection] (132060)

 Format de la description du commit
 Le texte peut se trouver sur plusieurs lignes ce n'est pas un problème
 Faire reference à une ancre de la documentation (lien), en utilisant la syntaxe
 <<lien|texte du lien>>
 Exemple : <<link:./suite_efluid.html#cache.mode.chargement|cache.mode.chargement>>
 Pour mettre en gras/italique une partie il faut utiliser la syntaxe asciidoctor
 *texte en gras*
 _texte en italique_

 Liste des releases notes 

La release notes du script installeur efluid se trouve ici

La documentation de l'ensemble des paramètres se trouve ici

La documentation du script installeur efluid se trouve ici

La documentation du script installeur migefluid se trouve ici

La documentation du script installeur efluidnet se trouve ici

La documentation du script installeur ael se trouve ici

La documentation du script installeur aelgrd se trouve ici

La documentation du script installeur portail-partenaire se trouve ici

La documentation du script installeur eldap se trouve ici

La documentation du script installeur edoc se trouve ici

La documentation du script installeur suivefluid se trouve ici

La documentation du script installeur suiveclient se trouve ici

La documentation du script installeur ethaque se trouve ici

La documentation du script installeur enercom se trouve ici

La documentation de signefluid se trouve ici

La documentation du scriptSqlLauncher se trouve ici

 Artifactory frontal 
Configuration maven coté client

 Validation de l'installeur 
 Validation sur CJP 
 Description des tests unitaire 
 Tests unitaires sur les enforcer 

En début de job: http://usinevalidation.uem.lan/job/Fvalidation-gerrit/job/Ftools/job/Finstalleur/job/script-installeur-V2/

Les fichiers de configurations dédiés aux tests sont à mettre dans le repo git scriptInstalleur, groupés par type de serveur d'appli:
 installeur-parent/src/test
 installeur-parent/src/test/weblogic
 installeur-parent/src/test/embedded
 installeur-parent/src/test/embedded-docker

Les noms de fichiers de properties pour les tests doivent être nommés: given-configuration.properties-<nom-du-test>

Exemple pour les enforcer de l'embedded : scriptInstalleur/installeur-parent/src/test/embedded
 installeur-parent/src/test/embedded/given-configuration.properties-https-not-activate-with-prop-not-set
 installeur-parent/src/test/embedded/given-configuration.properties-https-activate-with-prop-set
 installeur-parent/src/test/embedded/given-configuration.properties-https-activate-with-prop-not-set

Conventions prises: 
 les properties mandatory sont valorisées, avec le mot clé "mandatory" si pas de valeur spécifique imposée
 les properties sous test sont biensur valorisées pour atteindre les objectifs du test.
 les properties optionnelles hors tests restent vides pour ne pas surcharger le fichier.

Les résultats attendus sont sous installeur-parent/src/test/expected: 
 1 fichier pour tous les tests en succès: 
 installeur-parent/src/test/expected/expected-result-success.dta
 1 fichier par test en erreur: 
 installeur-parent/src/test/expected/expected-result-for-<nom-du-test>

Exemple de contenu de résultat attendu:
 [INFO] --- maven-enforcer-plugin:1.2:enforce (check-properties-secured-embedded-exists) @ scriptInstalleur-parent ---
 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireProperty failed with message:
 The parameter install.embedded.https.port or .install.embedded.https.port can not be empty and is a numeric type ([0-9]+)
 [WARNING] Rule 1: org.apache.maven.plugins.enforcer.RequireProperty failed with message:
 Property "install.embedded.https.server.privkey" evaluates to "".  This does not match the regular expression "..*"
 [WARNING] Rule 2: org.apache.maven.plugins.enforcer.RequireProperty failed with message:
 Property "install.embedded.https.server.pubcert" evaluates to "".  This does not match the regular expression "..*"
 [INFO] BUILD FAILURE
 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.2:enforce (check-properties-secured-embedded-exists) on project scriptInstalleur-parent: Some Enforcer rules have failed
 . Look above for specific messages explaining why the rule failed. -> [Help 1]

Un même test <nom-du-test> peut être exécuté pour chaque type de serveur d'application: embedded, embedded-docker, weblogic

3 méthodes (à ne pas modifier) sont disponibles pour lancer les tests et vérifier le résultat:
 executeTest : s'occupe de lancer la commande mvn requise avec les différents paramètres attendus: 
 le nom du test <nom-du-test>
 le type de serveur d'application permettant de prendre les bons fichiers de conf en entrée
 la phase sur lequel l'enforcer à tester se déclenche
 l'id dans le pom.xml contenant l'enforcer à tester
 la property testée

 assertFailureTestResult: gère la vérification par rapport à un résultat d'erreur attendu contenu dans un fichier du répertoire expected
 assertSuccessTestResult: gère la vérification par rapport à un résultat SUCCESS attendu (contenu dans le fichier expected-result-success.dta du répertoire expected)

Les tests sont faits sous la forme suivante:

 # TU: check enforcer for embedded en fonction des tests existants (fichiers given-XXX)
 if [ -f src/test/embedded/given-configuration.properties-session-tracking-nimp-set ]; then
   # Given global 
   testedEnforcerId=<nom de l'id d'exec de l'enforcer dans le pom.xml>
   testedProperty=<nom de la property>
   phaseRequired=<phase maven de déclenchement>

   ########## EMBEDDED | EMBEDDED-DOCKER | WEBLOGIC
   # Test <nom de la property> in (embedded | embedded-docker | weblogic)
   targetApplicationServer=<serveur d'application>
   
   
   ## Test XXX value set => KO
   ### Given
   testName=<nom-du-test-KO>
   ### When
   executeTest ${testName} ${targetApplicationServer} ${phaseRequired} ${testedEnforcerId} ${testedProperty}
   ### Then
   assertFailureTestResult ${testName} ${targetApplicationServer} ${testedEnforcerId} ${testedProperty}
    

   ## Test XXX value set => OK
   ### Given
   testName=<nom-du-test-OK>
   ### When
   executeTest ${testName} ${targetApplicationServer} ${phaseRequired} ${testedEnforcerId} ${testedProperty}
   ### Then
   assertSuccessTestResult ${testName} ${targetApplicationServer} ${testedEnforcerId} ${testedProperty}

 fi

Properties testées:
 Evt 213983: nouveaux paramètres pour HTTPS
 Evt 207381: session.tracking.mode 
 Evt 212957: paramètres supplémentaires du datasource embedded

 Configuration des serveurs pour tests intégration 

Les serveurs ldsanbld5 et ldsanbld6 sont utilisés pour les tests d'intégration de sshexec_maven-plugin et copy-maven-plugin. Les points de configuration suivants ont été apportés sur les serveurs:
 Configuration des accès ssh par clés 
 Les clés publiques du compte ulouser de lpsrvulo6 et lpsrvulo3 (~ulouser/.ssh/id_rsa.pub et ~ulouser/.ssh/id_rsa_passphrase.pub) ont été recopiées dans le fichier $HOME/.ssh/authorized_keys du compte weblogic sur les serveurs ldsanbld5 et ldsanbld6.
Ceci permet de faire des tests de connexion ssh avec clé publique sans passphrase, et avec clé publique avec passphrase (azerty).

 Configuration et lancement d'un démon sshd sur le port 2222 (root)
 Dupliquer la config ssh pour disposer d'une config où sera modifiée les port d'écoute et le fichier de flag pid
      - cp /etc/ssh/sshd_config /etc/ssh/sshd_config_newport
      Edition du fichier /etc/ssh/sshd_config_newport et modification de:
      Port 2222
      PidFile /var/run/sshd_newport.pid
 Dupliquer le script de lancement dans /etc/init.d pour le customiser:
      cp /etc/init.d/sshd /etc/init.d/sshd_newport
      Editer le fichier, et remplacer la chaine "sshd" par "sshd_newport" et "sshd_config" par "sshd_config_newport" sauf pour la variable SSHD, qui vaut toujours sshd (le démon est le même, seule la configuration de démarrage diffère)
      # config: /etc/ssh/sshd_config_newport
      # pidfile: /var/run/sshd_newport.pid
      [ -f /etc/sysconfig/sshd_newport ] && . /etc/sysconfig/sshd_newport
      prog="sshd_newport"
      PID_FILE=/var/run/sshd_newport.pid
 Dupliquer le fichier système /etc/sysconfig/sshd pour le customiser avec le nom du fichier de configuration sshd à utilisaer au lancement:
      - cp /etc/sysconfig/sshd /etc/sysconfig/sshd_newport
      Editer le fichier et configurer:
      - OPTIONS="-f /etc/ssh/sshd_config_newport"
 Préparer la configuration PAM pour l'authentification :
      - ln -s /etc/pam.d/sshd /etc/pam.d/sshd_newport
 Démarrer le service :
      - service sshd_newport start
      - service sshd_newport status
 Mettre à jour la configuration pour l'utilitaire de gestion des services chkconfig :
      - chkconfig --add sshd_newport
      - chkconfig sshd_newport on
 Vérification d'écoute sur le port 2222:
      - lsof -i TCP:2222
      - ps -ef | grep <pidDuProcEnEcouteSur2222>

 sshexec-maven-plugin 
Test de lancement de commandes sur une liste de serveurs définie dans le paramètre <testServerScpLocation>
Le format du paramètre est: serveurLocation[,serveurLocation]
 avec <serveurLocation> de la forme scp://<user>:<passwd>@<hostname>${sshTestPort}
Le test est fait pour 2 serveurs: scp://installeurtst:<motDePasse>@lpdocedt2${sshTestPort},scp://installeurtst:<motDePasse>@lpdocedt3${sshTestPort}
 1 test est fait avec le port par défaut vide => port 22 utilisé.

Note: un compte installeurtst spécifique a été créé sur les serveurs lpdocedt2 et lpdocedt3 pour permettre ces tests. Le fichier $HOME/.ssh/authorized_keys de installeurtst a été configuré pour contenir la clé publique sans passphrase et la clé publique avec passphrase du compte ulouser des usines (même clés/publiques/privées déployées sur les différents slaves) => Si ajout d'un nouveau slave, penser à configurer le répertoire .ssh pour disposer des clés à présenter.

Test de création/suppression de répertoire (sur ldsanbld5 uniquement) selon les 3 modes d'authentification possibles: 
 par mot de passe
 par clé publique sans passphrase
 par clé publique avec passphrase

En cas de besoin de test d'une nouvelle version de plugin sshexec-maven-plugin: -DsshExecMavenPluginVersion=<nouvelleVersion>.
    - exemple: mvn test -f scriptInstalleur-parent.pom -Ptest-unitaire-script-installeur -DsshTestPort=:2222 -DDsshExecMavenPluginVersion=0.3-beta-1-efluid1

Jobs de release du plugin: http://usinelogicielle/view/release/job/sshexec-maven-plugin/

 Création répertoire codeCentre pour streamserve (220202) 
Test de création d'un répertoire puis suppression si la property streamserve.application.codecentre.enterprise.parameter est non vide et non nulle
Test de skip de la création du répertoire si la property vide

 maven-common 
Librairie jar utilitaire pour les plugin sshexec-maven-plugin et copy-maven-plugin. Cette librairie a été fixée afin de permettre la surcharge du port d'écoute ssh pour les plugin sshexec et copy.

Job de release: http://usinelogicielle/view/release/job/maven-common-plugin-release/

 copy-maven-plugin 
Test de recopie d'un fichier (pom.xml) vers ldsanbld5 selon les 3 modes d'authentification possibles: 
 par mot de passe
 par clé publique sans passphrase
 par clé publique avec passphrase

 groovy-maven-plugin 
 Avec le fichier settings.xml par défaut (aucun client défini), on vérifie que le client renvoyé est : efluid
 On ajoute dans le fichier settings.xml la section suivante :
<servers>
   <server>
      <id>efluidMavenRepositoryFrontal</id>
      <username>BIDULE</username>
      <password>password</password>
   </server>
</servers>
 On vérifie que le client renvoyé est : BIDULE

 build-helper-maven-plugin 
 On vérifie que la variable parsedVersion.majorVersion est égale à 1

 On vérifie la bonne création de la variable test.current.time via le goal timestamp-property, pour créer une variable contenant un timestamp pour la création d'un répertoire temporaire unique. Correspond au test unitaire du profil deploy-gen appelé par les deployeurs.

 test des options de récupération de script 
 On veut ici tester toutes les options de récupérations des scripts, et vérifier que pour chacune on obtient bien le script adéquat
 Par défaut sans aucune option : récupère initOffline.sh, installeur.sh, deployerJonas.sh, deployerWeblogic.sh, README, application-configuration.properties
 desactiverRecuperationScriptCryptageProperties : gère la récupération des scripts crypterProperties.sh et generateCryptoKey.sh
 desactiverRecuperationScriptLancerLdif : gère la récupération du script lancerScriptsLdif.sh
 desactiverRecuperationScriptLancerSql : gère la récupération du script lancerScriptsSql.sh
 desactiverRecuperationDeployerStreamserve : gère la récupération du script deployerStreamserve.sh
 desactiverRecuperationDeployerBatch : gère la récupération du script deployerBatchs.sh
 desactiverRecuperationScriptDeployerStateless : gère la récupération des scripts deployerJonas-stateless.sh et deployerWeblogic-stateless.sh

 test des droits de coopération au déploiement des batchs entre 2 users 

Le but du test est de valider qu'on peut faire des modifications de droits sur le répertoire des batchs en se basant sur le contenu de l'archive zip des batchs (unzip -Zl), sans impact sur les fichiers/répertoires présents dans la sous-arborescence, qui pourrait appartenir à un autre user (user tp).

Objectif: valider la commande présente dans deployerBatchs.sh :

 sh -c  "unzip -Z1 ${applicationName}-batchs-${project.version}.zip | awk -v  installDir=${batchsInstallFolder} '{ print installDir \"/\"\$1}' | xargs chmod ${dossierDroitParDefaut} "

Note: un compte installeurtstbatch spécifique a été créé sur le serveur lpdocedt3 pour permettre ce test => Si ajout d'un nouveau slave, penser à configurer le répertoire .ssh pour disposer des clés à présenter.

 Description des tests d'intégration Docker V13 
On récupère tout d'abord un script installeur dans la version du paramètre du job (qui vient donc d'être packagé à partir du projet efluid).

Les tests sont lancés avec une version de Maven 3.3.9
 test installeur.sh 
 On récupère un properties com.efluid.properties:efluid-configuration.dockerBatchNonReg:HEAD-SNAPSHOT:properties, puis on lance : 
 ./installeur.sh

 test generatePropertiesJar.sh 
 On part d'un properties vierge, et on tente de lancer le script en désactivant globalement les enforcer afin de vérifier que la commande ne plante pas (c'est ce qui est fait dans le dockerfile application-tomcat-embedded pour générer un repo maven minimal) : 
 ./generatePropertiesJar.sh -DskipEnforcerGeneralUlOnly=true
 Ensuite on récupère un properties com.efluid.properties:efluid-configuration.dockerBatchNonReg:HEAD-SNAPSHOT:properties, puis on lance : 
 ./generatePropertiesJar.sh
 On vérifie que le jar de properties client est bien généré

 test de déploiement de batch 
 A la suite de la commande précédente on lance un déploiement de batch sur un conteneur Docker batch
 ./deployerBatchs.sh -Dbatchs.server.sshport=:22001

 test de déploiement embedded en mode classique avec plugins actifs (à faire quand on aura le remove instance) 
 A la suite de la commande précédente on lance un déploiement d'efluid embedded en mode classique en utilisant le properties suivant com.efluid.properties:efluid-configuration.embeddedClassiqueAvecPluginsNonReg:HEAD-SNAPSHOT:properties
 ./installEmbedded.sh

 On vérifie à ce moment la que les plugins sont bien déployés, via verifInstall
 On supprime l'instance embedded via remove instance

 test de déploiement embedded en mode docker avec plugins actifs 
 A la suite de la commande précédente on lance un déploiement d'efluid embedded sur un conteneur Docker en utilisant le properties suivant com.efluid.properties:efluid-configuration.dockerEmbeddedAvecPluginsNonReg:HEAD-SNAPSHOT:properties
 ./installEmbedded.sh

 On vérifie à ce moment la que les plugins sont bien déployés, via verifInstall
 Vérifier que la BDD est bien montée pour le plugin travaux dans la version cible

 test de déploiement batchs en mode docker avec plugins actifs 
 A la suite de la commande précédente on lance un déploiement des batchs sur noobaleine en utilisant le properties suivant com.efluid.properties:efluid-configuration.dockerBatchsAvecPluginsNonReg:HEAD-SNAPSHOT:properties
 ./deployerBatchs.sh

 On vérifie à ce moment la que les plugins sont bien déployés, via un cat du fichier /tmp/batchs/efluid/files/plugins/travaux/classes/META-INF/MANIFEST.MF

 test de déploiement embedded en mode docker 
 A la suite de la commande précédente on lance un déploiement d'efluid embedded sur un conteneur Docker en utilisant le properties suivant com.efluid.properties:efluid-configuration.dockerEmbeddedNonReg:HEAD-SNAPSHOT:properties
 ./installEmbedded.sh

 On vérifie à ce moment la que les plugins ne sont pas déployés.

 test de déploiement d'un plugin en standalone (= mode manuel) 
 On récupère la version courante de l'application et on la stock dans currentVersion
 On lance la commande suivante en conservant le même properties com.efluid.properties:efluid-configuration.dockerEmbeddedAvecPluginsNonReg:HEAD-SNAPSHOT:properties
 ./upgradePlugin.sh --name=travaux --version=$currentVersion

 On vérifie à ce moment la que le plugin travaux est bien actif désormais et en version $currentVersion
 Vérifier que la BDD est bien montée pour le plugin travaux dans la version $currentVersion   (pas forcément nécessaire comme test car la BDD sera deja dans la bonne version par les tests précédents)

 test de la relocalisation installation et livrables en mode embedded et récupération de l'installeur joramMQ 

 Objectifs du test 
Les objectifs de ce test sont de tester une installation:
 avec les livrables de l'installeur relocalisés (non récupérés dans livrables mais dans livraison.outputDirectory surchargé en ligne de commande via -D)
 avec un install.embedded.server.dir surchargé (et non /opt par défaut) pour relocaliser l'installation
 avec contrôle après installeur.sh que l'installeur de pivot JMS joramMQ est bien récupéré et dézippé sous ./utils : test -f utils/efluid-jorammqpivot/installeur.sh
Pas de démarrage du serveur d'application en fin d'installation car ne fait pas partie des objectifs de test

Ce test fait l'exécution du script installeur, type embedded (et non weblogic) en mode non docker (mode classique) avec relocalisation des livrables
L'installation est faite dans un conteneur Docker démarré au début du job. Le déploiement se fera en utilisant le serveur ssh démarré dans le conteneur.

 Problèmes contournés dans ce test 
Avec install.docker.enable=false => installEmbedded aura le comportement suivant:
 utilisation de yum et non rpm
 démarrage du serveur d'appli en fin d'install

> Pour changer ces 2 comportements (utilisation de rpm préférée car yum non dispo dans Docker et pas de démarrage), on force dans le script la variable isInsideContainer0 
    sed -i "s/isInsideContainer=.*/isInsideContainer=0/" ./installEmbedded.sh

 test de déploiement weblogic 
 A la suite de la commande précédente on lance un déploiement d'efluid statefull sur un conteneur Docker weblogic (Admin + un noeud managé efluid statefull)
 ./deployerWeblogic.sh -DdesactiverRecuperationScriptDeployerStateless=true -DdesactiverCreationDatasourceStatefull=false -DdesactiverCreationJms=false

 test de déploiement en mode offline 
 On récupère le properties com.efluid.properties:efluid-configuration.dockerBatchNonReg:HEAD-SNAPSHOT:properties, puis on lance un déploiement batch en mode offline : 
 ./initOffline.sh -Dmaven.repo.local=${WORKSPACE}/tempMavenRepository
 ./installeur.sh -o -Dmaven.repo.local=${WORKSPACE}/tempMavenRepository
 ./deployerBatchs.sh -o -Dmaven.repo.local=${WORKSPACE}/tempMavenRepository -Dbatchs.server.sshport=:22001

 test de déploiement de soapui 
 On récupère le properties com.efluid.properties:efluid-configuration.dockerBatchNonReg:HEAD-SNAPSHOT:properties puis on lance le déploiement de soapui
  ./utils/soapui/deployerSoapui.sh -Dsoapui.server.sshport=:22001

 On récupère ensuite le properties com.efluid.properties:efluid-configuration.dockerBatchSoapUiNonReg:HEAD-SNAPSHOT:properties puis on lance le déploiement de soapui
 ./utils/soapui/deployerSoapui.sh -Dsoapui.server.sshport=:22001

 test d'initialisation socle technique openldap 
 On récupère le properties com.efluid.properties:efluid-configuration.dockerOpenLdapNonReg:HEAD-SNAPSHOT:properties, puis on lance l'initialisation openldap
 ./socleTechnique/initOpenLdap.sh
 On récupère ensuite un fichier ldif de test com.efluid.utils:prod-uem-light-ldif:1.0.0:ldif, que l'on va injecter sur notre nouvel annuaire openldap via le script adéquat
 ./lancerScriptsLdif.sh
 On refait ensuite une initialisation sur le properties com.efluid.properties:efluid-configuration.dockerOpenLdapNonReg2:HEAD-SNAPSHOT:properties
 ./socleTechnique/initOpenLdap.sh
 Puis on injecte le fichier com.efluid.utils:prod-uem-light-ldif:1.0.0:ldif  en passant des paramètre dans la ligne de commande
 ./lancerScriptsLdif.sh -Dldap.load.ldif.filename=prod_uem_light.ldif -Dldap.load.user=cn=admin,dc=efluid,dc=fr -Dldap.load.password=passwd

 test LDIF 
 On vérifie que dans le dossier livrables il y ait bien un sous dossier "LDIF" contenant les scripts à jouer
 On vérifie que les scripts ont bien été valorisés
 On exécute les scripts via lancerScriptsLdif.sh en sachant qu'ici les objets n'existent pas encore donc pas besoin de l'option -DforceExecLdapPlugin=true
 Puis on re-exécuter le même script pour vérifier l'idempotence de celui-ci si on utilise l'option -DforceExecLdapPlugin=true qui permet de ne pas planter si le DN existe deja (ce qui est le cas ici vu qu'on a lancé le script juste avant)

 test du script lancerSqlMigrator  
 On fait une copie de base EFLUID_TI_INSTALLEUR sur un schéma temportaire EFLUID_BUILD_{numéro_build}
 On configure le fichier efluid-configuration.properties concernant les properties jdbc
jdbc.connect.string=jdbc:oracle:thin:@LPBDDEDT3:2483:PTECEDT1
jdbc.password=devdev
jdbc.user=EFLUID_BUILD_${BUILD_NUMBER}
bdd.tablespace.data=EFLUID_BUILD_DATA
bdd.tablespace.index=EFLUID_BUILD_INDEX
bdd.tablespace.param=EFLUID_BUILD_PARAM
bdd.tablespace.bloc=EFLUID_BUILD_BLOB
 On teste la livraison d'un livrable correctif, on utilise un zip sql allegé (contenant un script efluid de création d'une table de non reg) qui va jouer l'upgrade-correctif puis l'upgrade standard : com/efluid/efluid-sql-database-correctif/13.2.100-NonReg-SNAPSHOT/efluid-sql-database-correctif-13.2.100-NonReg-SNAPSHOT.zip
 On teste l'insertion d'une ligne dans la table de non régression ajouté.

 On teste l'upgrade standard, on utilise un zip sql allegé com/efluid/efluid-sql-database/13.2.100-NonReg-SNAPSHOT/efluid-sql-database-13.2.100-NonReg-SNAPSHOT.zip

 On teste l'upgrade avec l'option de remplacement, on utilise un zip sql allegé com/efluid/efluid-sql-database/13.2.100-NonReg-1-SNAPSHOT/efluid-sql-database-13.2.100-NonReg-1-SNAPSHOT.zip

 test du script genererDocumentationSqlMigrator  
 On lance la generation de la documentation sqlMigrator en skippant les modules edk, ecore, reversement et parametrage car ils ne sont pas présent dans le zip SQL que l'on utilise (com.efluid:efluid-sql-database:13.2.100_NonReg-SNAPSHOT:zip)
 On déploie cette documentation à l'emplacement suivant : http://wikefluid/docInstalleur/documentationSqlMigrator/index.html

 test du script executeScriptSqlLauncher 
 On lance le script executeScriptSqlLauncher.sh sur un evènement particulier 176671

 Tests installeur par application 

Cf http://cje.efluid.uem.lan/usinevalidationapplications/Fvalidation-gerrit/FsuiteEfluid/Fefluid/efluid.validation-installeur-gerrit

 Description des tests d'intégration Docker v12 
Cette section comprend ce qui a été supprimé à partir de la v13.
Les tests sont lancés avec une version de Maven 3.0.4

 test du script lancerScriptIndex.sh (remplacé par lancerSqlMigrator.sh) 
 On utilise un zip d'index efluid allégé (2 index) : com/efluid/efluid-sql-index/12.11.100-nonReg-SNAPSHOT/efluid-sql-index-12.11.100-nonReg-SNAPSHOT.zip
 On lance le script des index sur une petite base dont voici les coordonnées :
jdbc.connect.string=jdbc:oracle:thin:@LDBDDEDT1:2483:DFLDEDT
jdbc.password=devdev
jdbc.user=DEV_MAQ_FLD
bdd.tablespace.index=DEV_MAQ_FLD_INDEX
 On vérifie dans la logs que le script d'index a bien été joué, et qu'il a bien tracé les index qui nous interesse
 On relance ensuite le même script avec la ligne suivante en plus dans le ficheir de configuration et vérifie que le script ne renvoi aucune erreur
deployeur.ear.stateless.admin.server.password=toto

 gestion des droits sur les répertoires créés par le script installeur (à partir de la version 1.14.0) 
Afin de pouvoir surcharger facilement les droits d'accès aux répertoires, créés par le script installeur, deux variables ont été mises en place, dans le pom scriptInstalleur-parent:
 dossierDroitParDefaut : droit par défaut, affecté à l'ensemble des répertoires créés
 dossierDroitEtendu : droit moins strict, affecté aux répertoires nécessitant d'être modifié par d’autre utilisateur que le owner. Droit mis en place pour erdf, sur les répertoires suivants :
 batch.log.filename
 batch.repertoire.archivage
 batch.repertoire.certificats
 batch.repertoire.reception
 batch.repertoire.travail
 batch.repertoire.sauvegarde.echanges

 Validation de l'installeur sur CJE 

 Diagramme des properties de surcharge 

800px|sans_cadre|gauche|efluid
700px|sans_cadre|gauche|efluidpub
700px|sans_cadre|gauche|efluid.net
600px|sans_cadre|gauche|ael

600px|sans_cadre|gauche|eldap

600px|sans_cadre|gauche|enercom

600px|sans_cadre|gauche|ethaque

600px|sans_cadre|gauche|suivefluid<!!!>Category:YourKit

efluid SAS dispose de 5 licences flottantes pour utiliser l'outil YourKit qui permet d'analyser les problèmes de performances de l'application. (http://www.yourkit.com/features/index.jsp)

 Yourkit 2021 
 Installation de la licence YourKit 2021 sur le serveur 

disponible ici 

 Se connecter en ssh à 
 créer un fichier /opt/yourkit-license-server/licenses/key2.txt
 Coller la clef de licence serveur dans le fichier key2.txt (la clef est dans le keepass performance, chercher "yourkit")

Le serveur de licence yourkit s'administre via systemd.
Le service s'appelle yourkit-server. Il est activé pour redémarrage au boot.

Pilotage du service depuis le compte root. 
 arrêt  : systemctl stop yourkit-server
 Démarrage : systemctl start yourkit-server
 status depuis root : systemctl status yourkit-server

Pour visualiser les licences consommées : http://:10112/status

 Installation de la licence YourKit 2019 / 2021 sur le client 
 Récupérer la dernière version du logiciel sur eRoom : F_INSTALL\YourKit
 Ouvrir le client YourKit 2019 / 2021
 Un prompt demandant la clef apparaît
 Renseigner *:
 Accepter

Les fois suivantes le prompt ne doit plus apparaître

 Serveur de licence 
 Host :  (IP : 192.168.106.27)
 Port : 10112 (Par défaut)

 liens 
  http://code.google.com/p/maven-yourkit-plugin/wiki/GettingStarted<!!!>Ordonnanceur utilisé par les équipes d'exploitation des clients : uem, smeg, seolis, vialis, gedia, rseipc pour ordonnancer l’exécution des batchs.

Caractères interdits :
 " , [ |<!!!>Category:jenkins
Category:outil

Jenkins est un outil d'intégration continue, il permet :
 de compiler de manière régulière le code des différents applications
 de déployer automatique les envirronements nightly-build 
 de lancer de manière régulière les tests unitaires

 Surveillance activité de lancement des Jobs 

Des problèmes ont été détectés sur le lancement des Jobs planifiés dans Jenkins: règulièrement, après le week-end, les Jobs planifiés ne sont plus démarrés par Jenkins.

Pour permettre le monitoring de l'activité Jenkins et détecter au plus tôt quand les jobs ne sont plus déclenchés, le mécanisme suivant a été mis en place, en attente de la qualification/correction du problème:

 Création d'un job dans l'onglet interne: flagCronTrigger. Ce job, planifié toutes les heures, est simplement chargé de poser un fichier indicateur (chedulerIsAlive)d'activité dans /data/EDT/jenkins/tmp. Ce job est lancé sur le master (lpsrvulo10).
 Planification dans la crontab de Jenkins (toutes les heures, 45 mn après chaque heure) d'un script chargé de vérifier l'existence du fichier indicateur, et le supprimer s'il existe. S'il n'existe pas, un mail d'alerte est transmis à UsineLogicielle.

 Mise en œuvre Cloudbees Jenkins Enterprise & Jenkins Operations Center 
 Liste des nœuds Jenkins 
Le Jenkins Operations Center (JOC): 
 http://lpsrvulo12:8888/

+ Versions du core Jenkins Operations Center   lpsrvulo12 (CJOC principal)
  ??? (CJOC secondaire)
   Jenkins ver. 1.609 (CloudBees Jenkins Operations Center 1.6)
   
   
   Jenkins ver. 1.625.3.1 (CloudBees Jenkins Operations Center 1.8)
  X
  N/A

Les Jenkins Enterprise Client Master:
 UsineLogicielle: http://usinelogicielle.uem.lan (http://lpsrvedt1:8080)
 UsineValidation: http://usinevalidation.uem.lan (http://lpsrvedt1:8180)
 UsineDeploiement: http://usinedeploiement.uem.lan (http://lpsrvedt1:8280)
 UsineRecette: http://lpsrvedt1:8380
 UsineAdministration: http://lpsrvedt1:8480

+ Versions du core Jenkins client Master   usineLogicielle
  usineValidation
  usineDeploiement
  usineAdministration
  usineRecette
   Jenkins ver 1.609.1.1 (CloudBees Jenkins Enterprise: 15.05)
   
   
   
  
  
   Jenkins ver 1.625.3.1 (CloudBees Jenkins Enterprise: 15.11)
  X
  X
  X
  X
  X

 Redémarrage des Master de lpsrvedt1  

Un job est disponible sur l'UsineAdministration: http://lpsrvedt1.uem.lan:8480/view/jenkins/job/jenkins.restart-master/

L'UsineAdministration ne peut pas être redémarrée via son propre job !

=> Un script de redémarrage est disponible sur lpsrvedt1, à lancer en tant que root: 
   $ cd /data/EDT/interneUL/scripts/
   $ restart-master.sh <action> <master>
     * action = start | stop | restart
     * master = jenkinsUR | jenkinsUV | jenkinsUD | jenkinsUA | jenkinsUL

 Procédure de migration des Client Master  

Pour redémarrer après migration 1.625.x -> 2.19.4.2 : https://stackoverflow.com/questions/38157686/migrating-to-jenkins-2-1-ajp-support-is-removed-in-winstone-3-0-due-to-jetty-9

 UL/UV/UD/UR  

Afin de migrer le core des Jenkins Enterprise Client Master (UL, UV, UD, UR), la procédure suivante est à suivre:
 Télécharger le WAR du CJE (CloudBees Jenkins Enterprise) à installer depuis http://download.infradna.com/cje/ et le déposer dans /data/install/jenkins/${JenkinsVersion}
 Lancer le job de migration: http://lpsrvedt1.uem.lan:8480/view/jenkins/job/jenkins.upgrade-version
 Redémarrer l'usine migrée en utilisant le job: http://lpsrvedt1.uem.lan:8480/view/jenkins/job/jenkins.restart-master/
 Suivre la précédure de mise à jour des plugins qui suit.

NOTE: en cas de problème rencontré, il est possible de restaurer le master migré pour le remettre dans son état pré-migration: http://lpsrvedt1.uem.lan:8480/view/jenkins/job/jenkins.restore-version/

 UA  

 Le WAR est forcément déjà présent sur le serveur (migration UR en amont)
 Se connecter ulouser sur lpsrvedt1
 Se placer dans le répertoire /data/EDT/interneUL/scripts
 Exécuter le script jenkins-upgrade-version.sh
 Redémarrer l'usine jenkinsUA: sudo /etc/init.d/jenkinsUA start
 Vérifier via 'ps -ef | grep jenkinsUA' que le master est bien démarré en "daemonized". 
 Si ce n'est pas la cas, refaire "sudo /etc/init.d/jenkinsUA restart" jusqu'à obtenir "-Dcom.sun.akuma.Daemon=daemonized" sur la ligne du 'ps -ef | grep jenkinsUA'

Suivre la précédure de mise à jour des plugins qui suit.

NOTE: en cas de problème rencontré, il est possible de restaurer le master migré pour le remettre dans son état pré-migration: se connecter ulouser sur lpsrvedt1, se placer dans le répertoire /data/EDT/interneUL/scripts et lancer jenkins-restore-version.sh (voir les paramètres attendus en lancant l'outil sans paramètre).

 Mise à jour des plugins via UpdateCenter 
 Se connecter sur le client Master à mettre à jour et aller dans la page de gestion des plugins: http://<usineXXX>.uem.lan/pluginManager/
 Se rendre dans l'onglet "Disponibles" (http://usinevalidation.uem.lan/pluginManager/available) et sélectionner tous les plugins que l'on souhaite activer sur l'usine. ATTENTION: ne jamais sélectionner un plugin "Operations Center Server ....", dédié au JOC.
 Cliquer sur "Installer sans redémarrer", et cliquer sur redémarrer une fois les installations attendues OK.
 Une fois l'usine le client Master redémarré, se rendre dans l'onglet "Mises à jour" (http://usinevalidation.uem.lan/pluginManager/) et sélectionner tous les plugins que l'on souhaite mettre à jour (tous, normalement).
 Redémarrer le client Master une fois les mises à jour OK.

 Redémarrage des Master 

Les anomalies suivantes ont été observées sur Usine logicielle et usine validation, en fin d'execution, après le SUCCESS de la compilation:

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 7:08.997s (Wall Clock)
[INFO] Finished at: Fri Jan 08 10:57:21 CET 2016
[INFO] Final Memory: 60M/1026M
[INFO] ------------------------------------------------------------------------
[JENKINS] Archiving disabled
En attente que Jenkins finisse de récupérer les données
Triggering projects: Fgestion-interne » bdd.free-resources-token
channel stopped
ERROR: Echec à la lecture des POMs
java.io.IOException: Can't read POM: /data/EDT/jenkinsSlave_4all_1/4edcd6d0/workspace/Fvalidation-gerrit/ecore.JDK8/pom.xml
	at jenkins.plugins.maveninfo.extractor.properties.PomPropertiesFinder.findProperties(PomPropertiesFinder.java:54)
	at jenkins.plugins.maveninfo.extractor.MavenInfoExtractor.extract(MavenInfoExtractor.java:58)
	at jenkins.plugins.maveninfo.extractor.MavenInfoEnvironment.tearDown(MavenInfoEnvironment.java:42)
	at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:882)
	at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:536)
	at hudson.model.Run.execute(Run.java:1741)
	at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:531)
	at hudson.model.ResourceController.execute(ResourceController.java:98)
	at hudson.model.Executor.run(Executor.java:374)
Notifying upstream projects of job completion
Finished: FAILURE
Un redémarrage des usines a permis de résoudre le problème. 
Pour éviter ce type d'erreur, les usines (sauf UA) sont donc redémarrées tous les dimanches à 11h00 via le job http://lpsrvedt1.uem.lan:8480/view/jenkins/job/jenkins.orchestrate-restart-all-masters/
 Cela ne règle pas le vrai problème de fond qui est un problème d'encodage Cp1252 non supporté dans le pom.xml. Pour corriger cela il faut utiliser le job suivant : http://lpsrvedt1.uem.lan:8480/job/pom.change-cp1252-with-windows1252/

 Configuration spécifique des MASTER Jenkins 

Le  fichier /etc/sysconfig/jenkinsUx est modifié sur les master (variable JENKINS_JAVA_OPTIONS):

 Pour permettre les échanges de fichiers sans problèmes de droits entre ulouser (slaves) et jenkins (master) sur lpsrvedt1, un fichier a été ajouté: /var/lib/jenkins/.bashrc contenant la commande "umask 002". Comme le compte jenkins ne dispose pas de login (/bin/false dans /etc/passwd), la ligne suivante a été ajoutée à la fin de chaque fichier /etc/sysconfig/jenkinsU*: "source /var/lib/jenkins/.bashrc".
Pour l'utilisateur ulouser de lpsrvedt1, la commande "umask 002" a aussi été ajoutée en fin de fichier $HOME/.bashrc

 Pour changer la stratégie d'allocation/réservation des slaves lors des demandes faites par les différents job: 
   -Dcom.cloudbees.opscenter.client.cloud.CloudImpl.retentionStrategyShotCount=-1

 Pour augmenter la taille du contenu d'un job. Notamment pour les scripts groovy: -Dorg.eclipse.jetty.server.Request.maxFormContentSize=500000

 Etat CJP 

CJOC : lpsrvulo12
Masters : lpsrvedt1
 jenkinsUV : stoppé
 jenkinsUA
 jenkinsUL
 jenkinsUD: désactivé

Pour arrêter ou redémarrer un master, aller sur lpsrvedt1. Se connecter jenkins user.
 cd /data/EDT/interneUL/scripts
 ./restart-master.sh stop jenkinsUV

Usage: $(basename $0) <stop | start | restart> <jenkinsUA | jenkinsUR | jenkinsUL | jenkinsUV | jenkinsUD>

 RAF migration CJP vers CBC 

 https://usinelogicielle.uem.lan/job/FefluidTools/job/FdbTools/ => à conserver jusqu'à extinction V13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FefluidTools/job/Fetools/dockerfile.release => à conserver jusqu'à extinction V13 prévue pour février 2022 (mais ne sert plus car les fichiers ont été remis dans l'installeur directement)
 https://usinelogicielle.uem.lan/job/FefluidTools/job/Finstalleur => à conserver jusqu'à extinction V13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FefluidTools/job/FjmsTools/  => à conserver jusqu'à extinction V13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FefluidTools/job/FmavenPlugins/job/sql-maven-plugin.release/ => à conserver jusqu'à extinction V13 prévue pour février 2022 
 https://usinelogicielle.uem.lan/job/FefluidTools/job/FscriptSqlLauncher/job/scriptsqllauncher.release/ --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/Feldap/job/eldap.release-socle-13/  --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FjobsCommuns/job/application.deploy-v13/   --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FjobsCommuns/job/efluidDemat.deploy-v13/   --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FjobsCommuns/job/release.call-suivefluid-gestion-version-pour-creer-branche-maintenance/  --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FjobsCommuns/job/release.call-suivefluid-gestion-version-pour-construire-version/  --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FjobsCommuns/job/weblogic.create-realm-for-erdf-environnement-v13/ ->  --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FsuiteEfluid/job/suite-efluid.orchestrate-new-release-COPIE/ --> --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FsuiteEfluid/job/suite-efluid.orchestrate-new-release-socle-14-afterMigration/ --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinelogicielle.uem.lan/job/FsuiteEfluid/job/suite-efluid.orchestrate-nightly-build-workflow-maintenance_13-UEM-NB-CLASSIQUE/  --> à conserver jusqu'à extinction v13 prévue pour février 2022, on voulait garder une NB pour EDFSEI en v13

 https://usinelogicielle.uem.lan/job/FjobsUtilitaires/job/etools.deploy-properties-to-artifactory/ -> à migrer 
 https://usinevalidation.uem.lan/job/Fvalidation-gerrit/job/performances/--> migrer et ajouter le shellCheck
 https://usinevalidation.uem.lan/job/Fvalidation-gerrit/job/Ftools/job/Fetools/job/etools.validation-shell/ --> à migrer
UD
   - NA
UV
 
 https://usinevalidation.uem.lan/job/Ftests/job/scriptInstalleur.test-installation-docker/ --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinevalidation.uem.lan/job/Fusinelogicielle/job/FjobsCommuns/ --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinevalidation.uem.lan/job/Fvalidation-gerrit/job/Ftools/job/FdbTools/job/efluidDbTools-script-sql-launcher/ --> à conserver jusqu'à extinction v13 prévue pour février 2022 
 https://usinevalidation.uem.lan/job/Fvalidation-gerrit/job/Ftools/job/Finstalleur/job/script-installeur-V2/ --> à conserver jusqu'à extinction v13 prévue pour février 2022
 https://usinevalidation.uem.lan/job/Fvalidation-gerrit/job/Ftools/job/FjenkinsConfigurations/ => on garde maintenance_legacy_v13

UA

 http://lpsrvedt1.uem.lan:8480/job/batchs.start-docker-environnement/ --> on garde NB ELDAP v12 (weblo jusque fin edfsei)
 http://lpsrvedt1.uem.lan:8480/job/gerrit.freeze-unfreeze-branche/ --> on garde pour le job de release
 http://lpsrvedt1.uem.lan:8480/job/weblogic.start-docker-environnement-pre-deploiement-application/ --> on garde NB ELDAP V12 (weblo jusque fin edfsei)

 Interaction avec Jenkins 

Pour interagir avec Jenkins il faut utiliser les services REST mis à disposition.

La documentation suivante explique comment s'en servir : https://www.jenkins.io/doc/book/using/remote-access-api/

Pour faire des tests, il faut utiliser l'utilisateur : jenkinsdev  (mot de passe à demander à l'UL)

Et il faut utiliser le master suivant : https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinerecette/

 liens 
 externes 
  http://jenkins-ci.org
 internes 
 http://wikefluid/docInstalleur/jenkinsConfigurations/<!!!>Category:outil
Category:hp

 Informations générales 
 Documents de conception 
Comment réagit le bouchon en fonction de mon contexte ? Comment a-t-il été pensé ? Toutes ces réponses se trouvent dans les documents de conceptions sur eRoom : 

Mes eRooms > efluid - Qualification Logicielle > 03_ICF_ARGOS > 35-Chantier Automates et Simulateurs > 10-Conception

 Tutoriels Service Virtualisation et UFT 
Tous les tutos sont disponibles sur eRoom : 

Mes eRooms > efluid - Qualification Logicielle > 03_ICF_ARGOS > 35-Chantier Automates et Simulateurs > 00-Stratégie et guides

 Problèmes rencontrés avec l'outillage HP 
Pour recenser les différents bugs/problèmes rencontrés avec l'outillage HP, ou si vous avez simplement des questions, il y a un fichier sur eRoom servant à communiquer avec l'équipe HP. Vous pouvez l'alimenter sans problèmes : 

Mes eRooms > efluid - Qualification Logicielle > 03_ICF_ARGOS > 20-Outillage > 30-Fichier échanges HP > IF argos - FichierEchange OutilsHP.xlsx

 ALM 
Vous pouvez découvrir l'ensemble des ressources déployées sur HP ALM Explorer.
Adresse ALM : http://wrqcedt1:8080/qcbin/
Fichier:AuthALM.png

 Bouchons déployés 
 Toutes versions 12.x 
L'ensemble des bouchons HP disponibles (avec leurs URL de déploiement) est disponible sur eRoom.

Mes eRooms > efluid - Projets ERDF > ERDF > 140 - Mutualisation > Suivi > Mutualisation_suivi_simulateurs.xlsx

 12.5 
 XSD et WSDL de références 
Les XSD et WSDL de référence utilisés pour le développement des simulateurs est disponible sur eRoom : 

Mes eRooms > efluid - Qualification Logicielle > 03_ICF_ARGOS > 35-Chantier Automates et Simulateurs > 20-Composants > argos - Simulateurs V12.5 - Référentiel des WSDL et XSD - v1.docx

 Liens externes 
  http://www8.hp.com/fr/fr/software-solutions/software.html?compURI=1174233#.Uyhcj_l5O0c<!!!>Category:outil
Category:éditique

 Généralités 

Exstream Opentext est un outil de composition de documents tels que les factures, les courriers, les états comptables, etc... dans différents formats (PDF, RTF, CSV, PCL).

 Fonctionnement  
Exstream peut être considéré comme une interface notamment dans le projet efluid. Les applications telles que efluid générèrent des fichiers XML. Ces fichiers seront interprétés par le code applicatif Exstream afin de composer un document en sortie.

cadre|centré|Fonctionnement TP cadre|centré|Fonctionnement Batch

 Architecture générale  

cadre|centré|Architecture générale

 FAQ  

En cas de saturation des Tablespaces, déterminer la table concernée

 SELECT
    owner,
    table_name,
    TRUNC(sum(bytes)/1024/1024) Meg,
    ROUND(ratio_to_report( sum(bytes) ) over () * 100) Percent
 FROM
 (
    SELECT segment_name table_name, owner, bytes
    FROM dba_segments
    WHERE segment_type IN ('TABLE', 'TABLE PARTITION', 'TABLE SUBPARTITION')
    
    UNION ALL
 
    SELECT i.table_name, i.owner, s.bytes
    FROM dba_indexes i, dba_segments s
    WHERE s.segment_name = i.index_name
    AND s.owner = i.owner
    AND s.segment_type IN ('INDEX', 'INDEX PARTITION', 'INDEX SUBPARTITION')
    
    UNION ALL
     
    SELECT l.table_name, l.owner, s.bytes
    FROM dba_lobs l, dba_segments s
    WHERE s.segment_name = l.segment_name
    AND s.owner = l.owner
    AND s.segment_type IN ('LOBSEGMENT', 'LOB PARTITION')
 
    UNION ALL
 
    SELECT l.table_name, l.owner, s.bytes
    FROM dba_lobs l, dba_segments s
    WHERE s.segment_name = l.index_name
    AND s.owner = l.owner
    AND s.segment_type = 'LOBINDEX'
 )
 
 WHERE owner in UPPER('&owner') -- nécessaire d'indiquer le OWNER à regarder
 GROUP BY table_name, owner
 HAVING SUM(bytes)/1024/1024 > 10 /* Ignore really small tables */
 ORDER BY SUM(bytes) desc ;

Si la table est DATA_CXIN_00000035, la supprimé serveur arrêter et la recréer par exemple :

      CREATE TABLE "TENANT_LDSRVEDT2"."DATA_CXIN_00000035" 
       (	"DOCUMENTABSTRACTIONID" RAW(16) NOT NULL ENABLE, 
        "CHUNKINDEX" NUMBER(*,0) NOT NULL ENABLE, 
        "DATA" BLOB, 
         CONSTRAINT "PK_DATA_CXIN_00000035" PRIMARY KEY ("DOCUMENTABSTRACTIONID", "CHUNKINDEX")
      USING INDEX PCTFREE 10 INITRANS 2 MAXTRANS 255 COMPUTE STATISTICS 
      STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645
      PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1
      BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)
      TABLESPACE "USERS"  ENABLE, 
         CONSTRAINT "FK_DATA_CXIN_00000035_01" FOREIGN KEY ("DOCUMENTABSTRACTIONID")
          REFERENCES "TENANT_LDSRVEDT2"."CXIN_00000035" ("ID") ON DELETE CASCADE ENABLE
       ) SEGMENT CREATION IMMEDIATE 
      PCTFREE 10 PCTUSED 40 INITRANS 1 MAXTRANS 255 
     NOCOMPRESS LOGGING
      STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645
      PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1
      BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)
      TABLESPACE "USERS" 
     LOB ("DATA") STORE AS SECUREFILE (
      TABLESPACE "USERS" ENABLE STORAGE IN ROW CHUNK 8192
      NOCACHE LOGGING  NOCOMPRESS  KEEP_DUPLICATES 
      STORAGE(INITIAL 106496 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645
      PCTINCREASE 0
      BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)) ;

la même opération est possible sur DATA_CXOUT_00000036

Il est possible que le contenu de la table stockant les logs d’ExStream ne soit jamais supprimé.
Dans ce cas il est nécessaire de purger manuellement cette table.

La procédure est la suivante :

1.	Arrêt de ExStream

2.	Création d’un index sur le logtime : 
 CREATE INDEX IDX_LOG_TIME ON CXLOG_00000039 (logtime);
3.	Enregistrement de toutes les lignes qu’on souhaite conserver dans une nouvelle table (ici tout ce qui a moins de 10 jours) :
 create table TMP_CXLOG_00000039 as
 select /*+ FULL */ *
 from CXLOG_00000039
 where logtime >= to_timestamp(sysdate-10)
 ;
4.	Vidage de la table des logs
 truncate table CXLOG_00000039;
5.	Insertion des lignes sauvegardées
 insert into CXLOG_00000039
     select /*+ FULL */ *
     from TMP_CXLOG_00000039
 ;
6.	Suppression de la table temporaire créé
 drop table TMP_CXLOG_00000039;
7.	Redémarrage de ExStream<!!!>Category:outil

 Présentation 

Screen est un multiplexeur de terminaux, capable d'ouvrir un ou plusieurs terminaux dans une même console, de passer de l'un à l'autre, de s'en déconnecter pour les laisser tourner en tâche de fond et de les récupérer plus tard.

Il offre d'autres services utiles comme :
 La possibilité d'attacher et de détacher une session, pratique par exemple pour lancer un traitement batch, de déconnecter et récupérer le résultat plus tard,
 La possibilité de partager un terminal avec un autre utilisateur, idéal pour aider un utilisateur distant,
 La possibilité d'ouvrir plusieurs ligne de commande sur un serveur sans avoir à se reconnecter à chaque fois.

Cette page se veut être un tutoriel très simple de l'outil. La documentation complèt est disponible ici : http://www.gnu.org/software/screen/manual/screen.html

 Principe de fonctionnement 
Bien évidement, l'outil doit être installé sur le serveur au préalable. Voir avec le sysadmin en charge.

Il se lance très simplement, en entrant la commande screen:
  [git@lpsrvgit1 ~]$ screen -S <un nom explicite>

Selon la configuration du serveur, un écran d'accueil peut s'afficher. Ou pas. Quoi qu'il en soit, il est simple de confirmer qu'on se retrouve à l'intérieur d'une session screen, en entrant le raccourci <ctrl><a> suivi de " (touche 3 du clavier).

L'argument <un nom explicite> permet de retrouver des sessions plus tard, voir plus bas "Gestion des sessions". Dans mon cas j'ai choisi mon nom de famille.

Ce raccourci affiche le sélecteur de fenêtre screen actives, dans notre cas :
  Num Name                  Flags
    0 bash                  $
On y navigue à l'aide des flèches du clavier, on active la fenêtre sélectionnée en appuyant sur <entrer>.

 Gestion des fenêtres 
Pour s'y retrouver, on peut libeller la fenêtre active. Le raccourci est <ctrl><a> A :
  [git@lpsrvgit1 ~]$
  Set window's title to: Mon premier exemple

On vérifie en consultant la liste des fenêtres (<ctrl><a> ") :
  Num Name                  Flags
    0 Mon premier exemple   $

Pouvoir lister les fenêtres signifie qu'on peut en créer plusieurs. Le raccourci est <ctrl><a> c. Si je crée une nouvelle fenêtre (<ctrl><a> c), que je la labellise "Mon deuxième exemple" (<ctrl><a> A) en enfin que le liste les fenêtres disponibles (<ctrl><a> "):
  Num Name                  Flags
    0 Mon premier exemple   $
    1 Mon deuxième exemple  $

Grâce au sélecteur, je peux changer de fenêtre. Je peux également naviguer d'une fenêtre avec les raccourcis <ctrl><a> n (avancer) et <ctrl><a> p (reculer). Un peu comme <alt><tab> sous windows.

Une fenêtre se détruit en quittant le shell lancé. Avec la commande exit par exemple:
 [git@lpsrvgit1 ~]$ exit
On peut également tuer une fenêtre avec le raccourci <ctrl><a> K.

 Gestion des sessions 

screen permet non seulement de multiplexer des terminaux, mais également de se déconnecter en les laissant tourner en tâche de fond. Reprenons notre exemple. Voici un commande affichant la date chaque seconde: while true; do date; sleep 1; done. Je l'entre en ligne de commande :
  [git@lpsrvgit1 ~]$ while true; do date; sleep 1; done
  ven mai 16 14:59:20 CEST 2014
  ven mai 16 14:59:21 CEST 2014
  ven mai 16 14:59:22 CEST 2014
  ven mai 16 14:59:23 CEST 2014
  ven mai 16 14:59:24 CEST 2014

Je vais me détache de ma session avec le raccourci <ctrl><a> d :
  [git@lpsrvgit1 ~]$ screen -S rueff
  [detached]
  [git@lpsrvgit1 ~]$
Je peux m'y rattacher à l'aide de la commande screen -r <mon identifiant de session> (dans mon cas screen -r rueff). Je retrouve ma fenêtre, dont le contenu a entre temps progressé :
  ven mai 16 15:00:07 CEST 2014
  ven mai 16 15:00:08 CEST 2014
  ven mai 16 15:00:09 CEST 2014
  ven mai 16 15:00:10 CEST 2014
  ven mai 16 15:00:11 CEST 2014
  ven mai 16 15:00:12 CEST 2014
  ven mai 16 15:00:13 CEST 2014

Il peut avoir tellement progressé qu'on ne voit plus le début. Il est possible de "remonter" dans l'affichage en passant en mode 'scrollback'' avec le raccourci <ctrl><a> <echap>. On s'y promène à l'aide ds touches fléchés, et on en sort à nouveau via <echap>. On peut également logguer le contenu de l'écran dans un fichier : <ctrl><a> H.

 Session partagée 
Pour que plusieurs utilisateurs puisse se conencter simultanément, le prmier crée une session screen avec un nom déterminé, les autres s'y attache via screen -x <nom de la session>.

 Résumé des commandes raccourcis 
En premier lieu : <ctrl><a> ? affiche l'aide.

 screen -S toto : créer une session
 screen -r toto : s'attacher à une session en mode exclusif
 screen -x toto : s'attacher à une session en mode prtagé

 <ctrl><a> A : nommer la fenêtre
 <ctrl><a> c : créer une fenêtre
 <ctrl><a> d : se détacher de la session
 <ctrl><a> H : logguer le contenu de l'écran
 <ctrl><a> K : tuer une fenêtre
 <ctrl><a> n : aller à la fenêtre suivante
 <ctrl><a> p : aller à la fenêtre précédent
 <ctrl><a> " : lister les fenêtres
 <ctrl><a> <echap> : passer en mode scrollback, dont on sort en appuyant sur <echap><!!!>Category:outil
Category:docker

 Conteneurs disponibles 
 MongoDB 
 Un conteneur mongoDB est disponible sur LDSRVULO1.
 Instanciation du conteneur
 docker run -d --restart=always -p 27017:27017 --name mongoDBName mongo

 Wildfly 
 Un conteneur avec Wildfly 8.1.0.Final est disponible sur LDSRVULO1
 Image de base disponible ici : https://registry.hub.docker.com/u/jboss/wildfly/
 Image jdk 8 sur etools/docker/images/wildfly-admin-jdk8
 Instanciation du conteneur
 docker run -it --restart=always -p 8080:8080 -p 9990:9990 --name wildflyName jboss/wildfly-admin-jdk8
 Les login/mdp d'accès à la console sont : admin/Admin#70365

 Sur lpdocedt1, le Dockerfile se trouve sous /data/scripts/wildfly-admin-jdk8

 Oracle 11G expres 
 Conteneur disponible sur LDSRVULO1
 Image https://registry.hub.docker.com/u/wnameless/oracle-xe-11g/
 Instanciation du conteneur
 docker run -d -p 49160:22 -p 49161:1521 wnameless/oracle-xe-11g

 TODO efluid embedded 
 Instanciation du conteneur
   docker run -d -p 8090:8080 -v /root/lib:/opt/apache-tomcat-8.5.4.5-SNAPSHOT/webapps/efluid-13.3.100-PROTO-EMBEDDED-SNAPSHOT/WEB-INF/classes --name efluidEmbedded partifactorydocker.uem.lan/application-tomcat-embedded:efluid-13.3.100-PROTO-EMBEDDED-SNAPSHOT

 Wekan 
 Instanciation des conteneur
    docker run -d --name wekan-db mongo
    docker run -d --link "wekan-db:db" -e "MONGO_URL=mongodb://db" -e "ROOT_URL=http://lddocedt1:8091" -p 8091:80 mquandalle/wekan

 Images via Dockerfile 
 https://docs.docker.com/reference/builder/
 Les packages utilisés dans les Dockerfiles sont récupérés via la commande wget (--user=usinelogicielle). Les packages sont stockés dans l'artifactory sous http://eartifact/artifactory/libs-release-local/com/efluid/dockerfiles/...
 Les Dockerfile doivent être placés dans etools : etools\docker\images\dockerfile

 Création d'un Dockerfile 
 Il faut créer un fichier Dockerfile dans un répertoire donné
 Pour builder l'image il faut lancer la commande suivante
 docker build --tag=nomDuTag <RepertoireDuDockerFile>
 Pour se placer dans l'image et faire des modifications
 docker run -i -t --name essaiTartempion imageRepository:imageTag /bin/bash
 Pour commiter les modifications
 docker commit idContainer ImageRepository:ImageTag

 Utilisation du proxy UEM 
 Il faut déclarer les variables d'environnements suivantes pour pouvoir se connecter à Internet directement (via wget ou curl par exemple)
 ENV http_proxy http://USER:PASSWORD@lpsrvpxy:8080
 ENV no_proxy localhost,127.0.0.1,wikefluid,eartifact,usinelogicielle
 Pour faire du yum il est nécessaire de déclarer le proxy dans la configuration de yum
 RUN echo "proxy=http://lpsrvpxy:8080/" >> /etc/yum.conf
 RUN echo "proxy_username=D_NT_UEM\USER" >> /etc/yum.conf
 RUN echo "proxy_password=PASSWORD" >> /etc/yum.conf

 Le context du build 
 Tous les fichiers présents "à coté" du Dockerfile seront présents dans le context du build
 Exemple : on a l'arborescence suivante
 /data/file/
             - Dockerfile
             - toto.jar
             - titi.txt
 Les fichiers toto.jar et titi.txt seront accessible via les commandes du Dockerfile
 On pourra donc faire par exemple
 ADD toto.jar /opt/toto.jar

 Faire un mapping de répertoire local pour le rendre disponible dans un conteneur 
 Il faut déclarer le volume dans le Dockerfile
 VOLUME ["/data/install"]
 Puis au moment du lancement du conteneur, faire le mapping
 docker run -v [host directory]:[container directory]

 Commandes utiles 
 https://www.docker.com/sites/default/files/Docker_CheatSheet_08.09.2016_0.pdf

 Conteneurs 
 Connaitre la liste des conteneurs démarré
 docker ps
 Connaitre la liste de tous les conteneurs
 docker ps -a
 Supprimer un conteneur
 docker rm -v containeurId
 Démarrer un conteneur
 docker start containeurId
 Arreter un conteneur
 docker stop containeurId
 Redemarrer un conteneur
 docker restart containeurId
 Voir les logs du conteneur
 docker logs containeurId

 Images 
 Les backup d'images peuvent aussi être placés dans etools : etools\docker\images\backup

 Lister les images
 docker images
 Supprimer une image
 docker rmi ImageId
 Backuper une image
 docker save ImageName | gzip -c > PathToTarGzippedFile
 exemple : docker save rhel6-base | gzip -c > /data/rhel6-base.tgz
 Charger une image
 gunzip -c PathToTarGzippedFile | docker load
 exemple : gunzip -c rhel6-base.tgz | docker load

 Articles intéressants 
 Extraits de Linux Magazine 174 
Chapitre 1 - Vos applications intermodales

Chapitre 2 - L'utilisation des conteneurs sous Docker

Chapitre 3 - Outils et écosystème de Docker

 Liens internes 
 Bugs 
Bug utilisation module docker_container en 2.2 : http://wikefluid/index.php/Guide_d%27utilisation_de_Ansible#Bug_module_ansible_docker_container_18461<!!!>Category:Xming<!!!>Category:Outil
Category:Intégration Continue
Category:Jenkins

NOUVELLE DOCUMENTATION : http://wikefluid/docInstalleur/jenkinsConfigurations/
 Cas d'école d'assemblage 

 Créer une branche de maintenance sans job / Oubli de cocher la case "création d'une branche de maintenance" 

Prenons le cas de suivefluid. Il nous a été demandé d'assembler une version 4.5.100.RC1 sachant que la prochaine sera une version 5.1.100-SNAPSHOT, celà implique la création de deux branches de maintenances :
 La branche de maintenance_4.5 où l'on va assembler les prochaines 4.5.200, 4.5.300, etc...
 La branche de maintenance_4 où l'on va assembler les prochaines 4.6.100, 4.7.100, 4.8.100, etc...

Lorsque l'on lance le job d'assemblage, celui-ci ne va pouvoir créer qu'une seule branche de maintenance : la maintenance_4.5

A notre charge de créer la branche de maintenance_4.

La première chose à faire est de créer le nouveau pom parent pour la prochaine version qui sera assemblée sur la maintenance_4 à savoir la version 4.6.100-SNAPSHOT

On part toujours d'un pom de release, aussi commençons par télécharger le pom 4.5.100.RC1 : http://eartifact.uem.lan/artifactory/libs-release-local/com/efluid/suivefluid-parent/4.5.100.RC1/suivefluid-parent-4.5.100.RC1.pom

Il faut maintenant changer la version du pom suivefluid-parent :
http://usinelogicielle.uem.lan/job/FjobsUtilitaires/job/pom.update-properties-multi-versions-in-git/ 

<version>4.5.100.RC1</version> 
             en
<version>4.6.100.RC2-SNAPSHOT</version>

et sauvegarder le fichier avec un nom de la forme <application>-parent-<nouvelleVersion>-SNAPSHOT.pom

Pour le cas courant cela nous donnerait suivefluid-parent-4.6.100.RC2-SNAPSHOT.
Pour finir, il faut déployer le nouveau pom dans eartifact.

La deuxième étape consiste à créer la nouvelle branche de maintenance_4 et de mettre à jour les poms pour qu'ils portent la bonne version et que parent de cette version corresponde au nouveau que l'on vient de déployer. Lorsque l'on créer une nouvelle branche, on la créer à partir du tag de la dernière RC (la RC1 dans tout les cas)

meyerj@PC2060 MINGW64 /d/java/workspaces/developpement_dev/suivefluid (develop u=) 
$ git branch maintenance_4 4.5.100.RC1
$ git push origin maintenance_4
$ git checkout maintenance_4

Mettons à jour la version des poms de tout les modules et du parent. On lance également une compilation pour être sûr que tout fonctionne encore :
meyerj@PC2060 MINGW64 /d/java/workspaces/developpement_dev/suivefluid (develop u=) 
$ mvn org.codehaus.mojo:versions-maven-plugin:1.2.2-hermes:set versions:commit -DnewVersion=4.6.100.RC2-SNAPSHOT -DparentVersion=4.6.100.RC2-SNAPSHOT -DforceChangeAllModules=true
$ mvn versions:update-parent -N -DforceChangement=true -DparentVersion=4.6.100.RC2-SNAPSHOT
$ mvn clean install -DskipTests

On ajoute, on commit et on push les modifications :
meyerj@PC2060 MINGW64 /d/java/workspaces/developpement_dev/suivefluid (develop u=) 
$ git add . -A
$ git commit -m '120682 : [fix] Création de la branche de maintenance_4'
$ git push --set-upstream origin maintenance_4

Dans le cas où la gestion des scripts ne s'effectuerait pas encore avec SQLMigrator (enercom par exemple), il ne faut pas oublier de créer les branches de maintenances et les environnements sur ebuild. La démarche est expliquée ici : http://wikefluid/index.php/Assemblage_des_applications_/_composants#Op.C3.A9rations_.C3.A0_effectuer_sur_ebuild_.28hors_suite_efluid.29

Pour finir cette section, il faut lancer l'API suivefluid de création d'une branche de maintenance : 
http://usinelogicielle.uem.lan/job/FjobsCommuns/job/release.call-suivefluid-gestion-version-pour-creer-branche-maintenance/

 Pour le cas de suivefluid, il faut penser à effectuer les mêmes opérations Git sur le repository suivefluidStrs qui stocke le contenu Streamserve de suivefluid.

 TODO 
 Liste de check 
assemblage

 commun

 (POST) Vérifier que la version est à jour dans Suivefluid et que la prochaine est bien créée

 briques

 Archi/EDK/ecore

 commun

 (PRE) Vérifier qu'il y a des modifications, et si non vérifier que la brique parente n'a pas été releasée pour la même application cible
 (PRE) Regarder s'il existe des commits en attente dans Gerrit

 3.X

 (PRE) Vérifier les TI via chaque job dédié qui doit avoir été lancé juste avant la release
 (POST) Mettre à jour le pom parent de chaque application dont on veut faire l'intégration, via le job XXX pour efluid, et manuellement pour les autres applications

 develop

 (POST) Controler que toutes les intégrations sont faites et si ce n'est pas une erreur technique sinon contacter le responsable du composant pour qu'il corrige. Puis mettre le MVN Review et submiter.

 applications

 Tutos 
 Cas de reprise sur erreur

 Etape : mise à jour Suivefluid
 TODO

 Création des RPM dans une application 
 mvn deploy -P create-rpm-packages -pl : ${ applicationName}-server-embedded-tomcat  

 Génération d’une image docker pour l’application (ex: lddocedt1, ulouser) 
Depuis un serveur linux avec existance de /opt/conf-maven-docker :
 mkdir DOCKER8080
 cd DOCKER8080
 mvn org.apache.maven.plugins:maven-dependency-plugin:2.6:get -Dartifact=com.efluid.utils.dockerfile:application-tomcat-embedded:1.30.1:zip -Ddest=application-tomcat-embedded.zip
 unzip application-tomcat-embedded.zip
 cp -R /opt/conf-maven-docker .
 - 
 docker build --build-arg APPLICATION_NAME=${applicationName} --build-arg APPLICATION_VERSION=${version} -t partifactorydocker.uem.lan/application-tomcat-embedded:${applicationName} -${version} --no-cache=true --pull=true .

Pour tagguer l’image:
 docker tag partifactorydocker.uem.lan/application-tomcat-embedded:${applicationName} -${version} partifactorydocker.uem.lan/application-tomcat-embedded:${applicationName}-${version}

Pour publier dans Artifactory : 
 docker push partifactorydocker.uem.lan/application-tomcat-embedded:${applicationName}-${version}<!!!>Category:outil

L'usine de déploiement permet le pilotage des opérations sur les environnement de recette éditeur des applications :

 Enercom
 Ethaque
 Suivefluid

la procèdure compléte est disponible ici<!!!>Category:sublime text
C'est un éditeur de texte avancé pour les développeurs.
 Fonctions de base 
 Ctrl+P = Recherche de fichier (très performant)
 et "@" = nom de méthode
 et "#" = recherche de caractères dans le fichier
 et ":" = numéro de ligne
 Ctrl+shift+P = Commandes (syntaxe, git)
 Ctrl+Shift+L = Sélection multiple et édition multiple
 Ctrl+D = sélectionner nouvelle occurrence + modification en masse
 Ctrl+shift+K = supprimer une ligne
 Shift+"Bouton droit de la souris" ou  "Bouton milieu de la souris" =  sélection en bloc

 Configuration de Sublime Text 
 Encodage des fichiers 

Comme pour n'importe quel autre éditeur de texte, il est très important pour nous de mettre en place le même encodage sur tout nos fichiers. L'encodage actuel est le Cp1252. Pour convertir un fichier dans l'encodage voulu, il suffit d'aller dans File > ReOpen with encoding et choisir Western (Windows 1252 = Cp1252).

Il est également possible d'écrire un fichier et de le sauvegarder directement avec l'encodage désiré en allant dans File > Save with encoding et choisir Western (Windows 1252 = Cp1252)

 1000x700px

 Plugin complémentaires (non inclus dans l'installation standard) 
 Installation 
 Il faut installer en 1er lieu "package control" pour installer les autres packages comme git

 Package control 
https://sublime.wbond.net/installation

 Click the Preferences > Browse Packages… menu 
 Browse up a folder and then into the Installed Packages/ folder 
 Download Package Control.sublime-package and copy it into the "Installed Packages/" directory 
 Restart Sublime Text 

 Package git 
https://github.com/kemayo/sublime-text-git/wiki

 Configuration options internet 
 décocher la case ci-dessous :
Image:Verification internet.jpg

 Configuration proxy 
 Modifier le fichier de configuration package control : Package Control.sublime-settings
 "http_proxy": "192.168.102.103:8080",
 "https_proxy": "192.168.102.103:8080",
 "proxy_username": "bouthino",
 "proxy_password": "********",
Image:Proxy package control 2.jpg

 Configuration git 
 Modifier le chemin vers l'exécutable GIT
 "git_command": "D:/Programs/git/bin/git.exe"
Image:Proxy package control.jpg<!!!>Ordonnanceur utilisé par les équipes d'exploitation des clients : ES, EDF-SEI pour ordonnancer l’exécution des batchs.

Caractères interdits :
 [ ( %<!!!>Category:outil
Category:openldap

 Pour configurer un serveur LDAP conforme aux préconisations éditeur 

 Méthode avec installation sur un serveur Linux 

 Pré-requis 

En pré-requis, les RPMS suivants doivent être installés sur le serveur (ex: ici pour un redhat 6.5)
 openldap-servers-2.4.23-32.el6_4.1.x86_64
 openldap-2.4.23-32.el6_4.1.x86_64
 openldap-clients-2.4.23-32.el6_4.1.x86_64

 Configuration cn=config 

Le service ldap doit être stoppé
 service slapd status

Sinon le stopper
 service slapd stop

Déplacer l'ancienne base LDAP
 mv /etc/openldap/slapd.d /etc/openldap/slapd.d.svg
 
Contenu du fichier /etc/openldap/slapd.conf (qui doit appartenir à ldap:ldap). Le mot de passe est à modifier, au choix de la personne qui installe.
 include         /etc/openldap/schema/core.schema
 include         /etc/openldap/schema/cosine.schema
 include         /etc/openldap/schema/nis.schema
 include         /etc/openldap/schema/inetorgperson.schema
 pidfile         /var/run/openldap/slapd.pid
 argsfile        /var/run/openldap/slapd.args
 idletimeout     900
 allow bind_v2
 database        config
 rootdn          "cn=config"
 rootpw          passwd

 
Contenu du fichier config.ldif pour le respect des préconisations éditeur pour LDAP  (olcAllows, olcIdleTimeout, olcConnMaxPending, olcConnMaxPendingAuth). Les préconisations éditeurs LDAP se trouvent là.
 dn: cn=config
 changetype: modify
 replace: olcAllows
 olcAllows: bind_v2
 dn: cn=config
 changetype: modify
 replace: olcIdleTimeout
 olcIdleTimeout: 900
 dn: cn=config
 changetype: modify
 replace: olcConnMaxPending
 olcConnMaxPending: 5
 dn: cn=config
 changetype: modify
 replace: olcConnMaxPendingAuth
 olcConnMaxPendingAuth: 10

Conversion de la configuration stockée dans slapd.conf au format OLC (on-line configuration cn=config) dans la base LDAP
 mkdir -p /etc/openldap/slapd.d
 slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d
 chown -R ldap:ldap /etc/openldap/slapd.d

Démarrage du service 
 service slapd start

 Configuration de la base LDAP pour efluid 

Ajout de la configuration pour les préconisations éditeurs. Si le port n'a pas été changé, le port par défaut est le port 389. Sinon l'adapter à la valeur configurée dans la commande ci-dessous. Le mot de passe (option -w) est aussi à remplacer.
 /usr/bin/ldapmodify -w passwd -D "cn=config" -H ldap://localhost:389 -f /root/config.ldif; 

Création du répertoire qui va acceuillir la base LDAP pour efluid. Ici on choisit /data/EDT/openldap. Ce chemin est libre de choix. Reporter son choix dans la suite de la procédure décrite ici.
 mkdir -p /data/EDT/openldap
 chown ldap:ldap /data/EDT/openldap

Contenu du fichier base.ldif pour la création de la BASE LDAP pour efluid (respecte aussi les préconisations éditeurs sur olcSizeLimit, olcTimeLimit, olcDbCachesize, olcDbCheckpoint)
 dn: olcDatabase=bdb,cn=config
 objectClass: olcDatabaseConfig
 objectClass: olcBdbConfig
 olcDatabase: bdb
 olcSuffix: dc=erdf,c=fr
 olcReadOnly: FALSE
 olcRootDN: cn=admin,dc=erdf,c=fr
 olcRootPW: password
 olcSizeLimit: 2000
 olcTimeLimit: 600
 olcDbCachesize: 10000
 olcDbCheckpoint: 128 5
 olcDbDirectory: /data/EDT/openldap

Création de la base LDAP pour efluid. Si le port n'a pas été changé, le port par défaut est le port 389. Sinon l'adapter à la valeur configurée dans la commande ci-dessous. Le mot de passe (option -w) est aussi à remplacer.
 /usr/bin/ldapadd -w passwd -D "cn=config" -H ldap://localhost:389 -f /root/base.ldif;

 Méthode avec Docker 

Un containeur docker intégrant un serveur openLDAP a été mis au point. La configuration du serveur openLDAP respecte les préconisations éditeurs.
Ce type de container est utilisé pour tester l'outil initOpenLdap de l'installer, qui permet d'initialiser l'annuaire LDAP efluid.

Son dockerfile et fichiers joints se trouvent dans le dépôt GIT etools sous docker/images/dockerfile/openldap-nonregression.

Pour construire l'image correspondante, il faut se connecter sous un serveur disposant déjà de docker.
Voir page wiki sur le sujet ici.

Construire l'image en lançant la commande depuis le répertoire contenant le DockerFile et les fichiers joints. Le nom du containeur est libre de choix.
 docker build -t test-openldap-nonregression:v12 $PWD

Pour lancer un containeur. L'alias du containeur est libre de choix.
 docker run --name monContaineur -d -p 2389:389 -p 2222:22 test-openldap-nonregression:v12

Avec un volume pour la persistance des données.
 docker run --name monContaineur -d -p 2389:389 -p 2222:22 -v /data/EDT/files/ENEDISLDAPV13/openldappersist:/data/EDT/openldap partifactorydocker.uem.lan/openldap-v13

et modifier la racine de la structure par défaut dc=efluid,c=fr

 docker run --name monContaineur -d -p 2389:389 -p 2222:22 -v /data/EDT/files/ENEDISLDAPV13/openldappersist:/data/EDT/openldap -e LDAP_ENV_DC=dc=erdf,c=fr partifactorydocker.uem.lan/openldap-v13

Ici on mappe le port 389 (LDAP) du containeur sur le port 2389 du serveur hôte. Depuis l'extérieur, le serveur LDAP est donc accessible sur le serveur 2389.

On mappe le port 22 (SSH) du containeur sur le port 2222 du serveur hôte. Depuis l'extérieur, le containeur est accessible en SSH sur le port 2222 du serveur hôte.<!!!>Category:outil
Category:intelliJ

 Installation IntelliJ 

Voir le Guide d'installation d'intelliJ.

 lancement IntelliJ 

Pour lancer IntelliJ, il faut aller dans D:\Programs\intellij\bin puis cliquer sur idea64
Fichier:Cheminversintellij64.PNG

La première fois, une fenêtre "complete Installation" s'ouvrira. Cocher Custom location. Config folder or installation home of the previoues version: aller chercher la configuration dans D:\Programs\intellij\.IdeaIC2017.2

Fichier:Configenvironnement.PNG

 configurer Maven 

Sur la page de démarrage cliquer sur Configure puis Settings 

Fichier:IntelliJsettings.png

aller dans Build,Execution,Deployement > BuildTools > Maven

 Maven home directory choisissez : D:/Programs/maven
Normalement le local repository sera automatiquement configuré par contre le User settings file il faut cocher Override
 User settings file  : D:\Programs\maven\conf\settings.xml
 Local repository : D:\java\mavenRepository

Fichier:SettingsMaven.PNG

 configuration générale 
 changer le formattage par défaut de l'éditeur 

 via plugin SettingsRepository 
A installer suivant les instructions de la page dédiée au plugin SettingsRepository

 via fichier xml (Eclipse) 
Par défaut, intellij (sous windows) utilise le formattage par défaut de l'IDE. 
Il faut récupérer le fichier de configuration efluid-formatter.xml qui se trouve à la racine de votre installation eclipse et le copier à la racine de votre installation IntelliJ.
Il faut ensuite importer la configuration dans IntelliJ.
 Pour cela il vous suffit de vous rendre dans File > Settings > 
Fichier:file_settings.jpg
 Dans Editor > Code Style > Java sélectionner la fonction Import Scheme > Eclipse XML Profile
Fichier:Editor.jpg

 changer le terminal de windows command line vers git bash 

Par défaut, intellij (sous windows) a comme terminal de commande : windows command line. Pour le changer par exemple par le git bash il vous suffit de vous rendre dans File > Settings > Terminal 
 Dans shell path mettre : D:\Programs\Git\bash.exe
Fichier:SettingsTerminalGitBash.PNG

Résultat : 
1300px

 configurer serveur d'application 
 Lancer l'application - Version V14+ (suite à la re-modularisation)
 Lancer l'application - Versions jusqu'à V13.x

 plugins 
 plugin mise à disposition 

 Nom  version  description  Asciidoc  0.19.2  AsciiDoc language support for IntelliJ platform. String manipulation  5.7.143.000.0  /  Java Stream Debugger  0.1.5   Bazel  2017.11.06.0.5  /  Cucumber for Groovy  172.4343  /  Cucumber for Java  172.4343  /

 plugins intéressants 
 https://plugins.jetbrains.com/plugin/10345-assertions2assertj
 https://wikefluid.efluid.uem.lan/index.php/SettingsRepository

 Optimisations 
 déplacer le répertoire de travail vers un emplacement non scanné par l'antivirus
 Créer un nouveau répertoire <tt>D:/java/intellij-home
 Déplacer le contenu de ${user.home}/.IntelliJIdea/config vers D:/java/intellij-home
 Editer le fichier D:\Programs\<RepertoireInstallation>\bin\idea.properties et modifier les 2 propriétés suivantes :
 idea.config.path=D:/java/intellij-home/config
 idea.system.path=D:/java/intellij-home/system

 Licences 
 Gestion du serveur de licence intelliJ efluidSAS

 Utilisation d'une licence efluidSAS depuis intelliJ 
 Via le serveur de licence 
 permet 48h offline tant que IJ n'a pas été redémarré depuis qu'il a récupérer un ticket en étant connecté au serveur de licence
 Si les licences sont toutes utilisées, faire une demande via Footprints.
Fichier:IJLicense01.PNG

 Via une licence nominative 
Un email doit vous avoir été envoyé, de ce format : 

 Dear Thomas, 
 A JetBrains IntelliJ IDEA Ultimate license has been connected to your t-collignon@efluid.fr JetBrains Account. 
 Please use these JetBrains Account credentials in the product to activate your license. 
 If for some reason it is impossible for you to use your JetBrains Account, please ask the owner of the license to generate license key for you.

 Si on a un compte JetBrains (cas à privilégier dans le cas d'une licence nominative car nécessite moins de gestion d'administration), on utilise l'option get license from JB account, puis on saisit les informations et activate :
Fichier:IJLicense02.PNG

 Sinon il faut demander à l'administrateur du serveur de licence (Actuellement TCO) de donner une clé de licence :
Fichier:IJLicense03.PNG

 Raccourcis 
 https://resources.jetbrains.com/storage/products/intellij-idea/docs/IntelliJIDEA_ReferenceCard.pdf
 http://blog.soat.fr/2014/04/shortcuts-mapping-eclipse-intellij-idea/
 http://javamind-fr.blogspot.fr/2013/01/intellij-et-les-raccourcis.html
 http://www.jetbrains.com/help/idea/mastering-keyboard-shortcuts.html
 Mettre en commentaire Ctrl + / (celui du numpad)
 Toutes actions CTRL + Maj + A 
 Recherche Double shift
 Précédent / Suivant = Ctrl + Alt + gauche|droite (désactiver dans icone intel, touches d’accès rapide)

 liens 
 https://www.jetbrains.com/help/idea/symbols-reference.html
 http://eforum.uem.lan/viewtopic.php?f=16&t=3309

 Plugins 

 installation bazel 
 récupérer bazel pour Windows et l'unzip dans D:\Programs\bazel : https://github.com/bazelbuild/bazel/releases/download/0.25.3/bazel-0.25.3-windows-x86_64.zip
 ajouter bazel dans les variables d'environnement utilisateur:
 BAZEL_HOME: D:\Programs\bazel
 Path: …;%BAZEL_HOME%
 récupérer le plugin bazel pour intellij : https://plugins.jetbrains.com/plugin/8609-bazel/versions
 installer le plugin : File > Settings > Plugins > options > Install Plugin from Disk … > (sélectionner le zip contenant le plugin bazel)
default<!!!>Category:outil

 Documentation 

https://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/

 Présentation globale 
La présentation initiale du projet se trouve ici : http://wperoom2.uem.lan/eRoom/Production/QualiteDeveloppementEfluid/0_15dffe

SQLMigrator comprend deux modes de fonctionnement.
 Le mode upgrade (utilisé par les devs et le scriptInstalleur). Ce mode est destiné à la pure montée de version 12.15.1200 -> 13.4.100 par exemple.
 Le mode upgrade ne joue que ce qui se trouve dans le dossier {application}/sql/database/{application}/ddl|dml/upgrade 
 Le mode upgrade ne peut être joué sur une base vide
 Le mode upgrade peut prendre ces paramètres : 
 sqlmigratorOffline : permet de simuler l'exécution de sqlMigrator. Génère les fichiers sql dans target qui auraient été joués sur la base
 context : permet de setter un context
 label : permet de setter un label
 skipIndex : permet de skipper l'exécution des indexs. Par défaut à false
 skipFinish : permet de skipper l'étape finish qui contient les scripts de perf et la maj de TAPPLICATIONINFO. Par défaut à false.
 skipParametrage : permet de skipper les scripts de paramétrage se trouvant dans le dossier paramétrage. Par défaut à true en mode dev et à false en mode scriptInstalleur.
 Le mode UL-destruction (utilisé uniquement côté dev, à implémenter dans le sriptInstalleur quand le besoin sera nécessaire)
 Ce mode permet de construire une BDD from scratch en utilisant tout ce qu'il se trouve dans les dossiers drop, create, init
 Par défaut, il faut ajouter en paramètre un fichier de properties permettant de cible la BDD voulue.
 Ce mode n'est utilisé que par l'UL

 FAQ 
 Taille maximal d'un nom de module 
 Les noms des modules ne doivent pas dépasser 12 caractères. La limitation lors de la création de la table de changelog étant de 30.

 Fonctionnement job de validation 

3 étapes dans les jobs de validation des projets : 
 Création d'une base de données 
  lien jdbc LPBDDEDT4:2483/PTECEDT2
  schemaUser : {APPLICATION}_BUILD_{BUILD_NUMBER}
  mot de passe Its4bdd_{schémUser}

 Lancement du mode upgrade
 Lancement du mode UL-destruction afin de vérifier que les dev ont livrés la même chose dans les deux dossiers.

 Conventions TBS Oracle12 

Les tablespaces sont nommés différemment en oracle12 par l'équipe perf.
DATA devient DTA
INDEX devient IDX
PARAM devient PRM
BLOB devient BLB
BATCH_INDEX devient B_IDX
BATCH_DATA devient B_DTA

Il faut faire attention de prendre en compte ces nouveaux nommages dans les fichiers de configurations

 Génération du script des index à partir du CSV 
 Lancer la commande suivante : 
 mvn validate -P generer-script-index -N
 Le script index.sql sera généré à la racine du projet.

 Génération du SQL sans passage en base 

Pré-requis : avoir un fichier framework2.properties qui pointe vers une BDD valide

 scripts/sql-upgrade.sh -DsqlmigratorOffline=true

 Conventions SQLMigrator 

Les conventions concernant les scripts sqlMigrator se trouvent ici : Conventions SQLMigrator

 Procédure de livraison d'un script de montée de version 
Procédure de livraison d'un script de montée de version : 

http://wikefluid/index.php/Livraison_de_script_de_mont%C3%A9e_de_version_dans_ebuild_pour_sqlMigrator

 Context pour le paramétrage 

Aucun script de paramétrage ne doit être inclut dans les scripts de montée de version via SqlMigrator sauf pour la dernière RC.

Pour ne pas jouer ces scripts il faut ajouter l'option au lancement -DskipParametrage=true

Cette option prend en compte le fait de jouer ou non le dossier paramétrage qui est désormais définit comme un module.

 Exécution manuelle des scripts de paramétrage (PARAM_ERDF) 

Procédure d'exécution manuelle des scripts de guichet de paramétrage :

http://wikefluid/index.php/Guichet_de_param%C3%A9trage

 Livraison d'un livrable de remplacement 

Il peut arriver qu'un script soit en erreur dans une RC (ou après release) et qu'on s'en rende compte après coup. Il est possible de ne relivrer, pour débloquer, qu'un livrable sql dit de remplacement. 
Cette exécution jouera uniquement ce livrable de remplacement et passera outre le livrable d'origine livré avec la version et n'est actuellement disponible que pour efluid.

Ce livrable est à mettre dans artifactory avec le nom suivant : efluid-sql-database-{numeroVersion}_{numeroRemplacement}
Par exemple pour la 13.4.100.RC8, nous avons du livrer un livrable de remplacement que nous avons nommé efluid-sql-database-13.4.100.RC8_2.zip

La ligne de commande à lancée pour exécuter ce livrable est :

./lancerSqlMigrator.sh -DskipUtilisationZipSqlDatabaseRemplacement=false -DversionLivrableSqlDatabaseDeRemplacement=13.4.100.RC8_2

Un enforcer est présent afin de contrôler le nom du livrable. Ce nom doit comprendre le numéro de version en cours de déploiement. Cette validation peut être skippée (à destination uniquement de l'UL) avec l'ajout du paramètre : -DskipCheckVersionLivrableSqlDatabaseDeRemplacementWithProjectVersion=true

Attention, penser à ajouter le -DclientName si jamais on passe ce livrable sur une base ERDF.

 Comment exécuter un script de manière isolé avec SQLMigrator 

Il peut être pratique d'exécuter un script de manière isolée, sans exécuter la montée de version intégralement.
Pour cela, on peut utiliser les labels, qui permettent de filtrer les scripts jouées pour la montée de version.

Par exemple, si l'étiquette suivefluid-183542-ddl-1 n'est utilisée que pour un seul changeset :

  	 		
 	
 	 		call ModificationStructure.rebuildIndexesBeforeDrop('tevenement','OLD_TYPEERREUR')
 	
 	 		
 	
 

On peut n'exécuter que ce changeset de la manière suivante :

 ./sql-upgrade-light.sh -D label=suivefluid-183542-ddl-1

Attention : Cette pratique n'est pas le cas d'usage nominal, elle ne garantie pas que la BDD soit à jour avec le code Java

 Comment valoriser une valeur variabilisée dans un script ? 

On peut définir des variables dans le code SQL d'un changeset LiquiBase/ SQLMigrator avec la syntaxe suivante "${maVariable}".
Par exemple : 

 grant select, insert, delete, update on TVALEURINTERVALLEQUANTITE to ${USER_GRANT};
 grant select, insert, delete, update on TSTAT to ${USER_GRANT};
 grant select, insert, delete, update on TLIGNESTATISTIQUE to ${USER_GRANT};
 commit;

Attention : Il faut utiliser cette syntaxe que la variable utilisée est gérée dans le déploiement de l'application.

Si cette syntaxe est utilisée, pour tester un script sur le poste de développement, on peut valoriser une telle variable de la manière suivante :

 ./sql-upgrade-light.sh -D maVariable=maValeur

Pour l'exemple ci-dessus :

 ./sql-upgrade-light.sh -D USER_GRANT=SEF_CLIENT

 FAQ des cas d'erreurs 

 ddl/upgrade/4/changelog/arc/functionalunit/changeLog.xml::suivefluid-172926-ddl::lagarde is now: 7:83c897ab2167fb89628d7c1a193a432e

 Le checksum du script a été modifié. Ce qui veut dire que la signature du script (décrite par id, author, filepath et contenu du fichier) a été modifié. En général, se passe sur les bases de DEV car les développeurs ont joués le script avec sql-upgrade sur la base puis l'on mit sur gerrit, l'ont modifié puis mergé. Donc le checksum est encore l'ancien. Il faut demander au développeur de corriger.

 [ERROR] Failed to execute goal com.efluid.utils.sql:sqlmigrator-maven-plugin:2.96.1:upgrade (upgrade) on project efluidnet-parent: Failed to execute SQLMigrator:   
 liquibase.exception.MigrationFailedException: Migration failed for change set ddl/upgrade/4/changelog/arc/modeleObjetMetier/changeLog.xml::suivefluid-194687-ddl-2 ::thibaut:
 [ERROR] Reason: liquibase.exception.DatabaseException: ORA-01430: la colonne ajoutée existe déjà dans la table
 [ERROR] [Failed SQL: ALTER TABLE DEV_FLDNET.TVALEURATTRIBUTPIECEJOINTE ADD REFERENCE VARCHAR2(250)]

 "La colonne ajoutée existe déjà dans la table" ou "La table ajoutée existe déjà" ou "ce nom d'objet existe déjà" veut dire que l'objet existe déjà sur la base. Peut se produire car : 
 Le développeur a ajouté manuellement son objet sur la bdd. Dans ce cas, il faut lui demander de corriger
 Le développeur a oublié de livrer un drop de son objet, il faut donc qu'il livre dans le dossier correspondant.
 Cet objet existe déjà et est créé en amont. Soit dans une brique soit dans le même projet mais le développeur ne l'a pas vu.
 Il est possible que l'exécution des drop se soit mal passé. Il faut donc regarder s'il n'y a pas eu un no wait ou une indispo de la base lors de l'exécution du drop de cette table

 ERROR] Failed to execute goal com.efluid.utils.sql:sqlmigrator-maven-plugin:2.96.1:upgrade (upgrade) on project ecore-racine: Failed to execute SQLMigrator:  
 liquibase.exception.MigrationFailedException: Migration failed for change set ddl/upgrade/4/changelog/arc/modeleObjetMetier/changeLog.xml::suivefluid-188055-ddl::thibaut:
 [ERROR] Reason: liquibase.exception.DatabaseException: ORA-00908: mot-cl� NULL absent
 [ERROR] [Failed SQL: create table TCONTRAINTEATTRIBUT
 [ERROR] (
 [ERROR] ID VARCHAR2(25) ,
 [ERROR] ROLE VARCHAR2(80) ,
 [ERROR] ETATOBJET NUMBER(1) ,
 [ERROR] ATTRIBUT_ID VARCHAR2(25) ,
 [ERROR] ATTRIBUT_ROLE VARCHAR2(80) ,
 [ERROR] LIBELLE VARCHAR2(80) ,
 [ERROR] COMPLEMENTJSON CLOB ,
 [ERROR] DATEMODIFICATION TIMESTAMP(6) ,
 [ERROR] ACTEURMODIFICATION VARCHAR2(50) ,
 [ERROR] DATECREATION TIMESTAMP(6) ,
 [ERROR] ACTEURCREATION VARCHAR2(50) ,
 [ERROR] DATESUPPRESSION TIMESTAMP(6) ,
 [ERROR] ACTEURSUPPRESSION VARCHAR2(50) ,
 [ERROR] constraint PK_TCONTRAINTEATTRIBUT PRIMARY KEY (ID) using index tablespace DEV_ECORE_INDEX
 [ERROR] , constraint COMPLEMENTJSON_JSON_CHECK check (COMPLEMENTJSON IS JSON)
 [ERROR] ) tablespace DEV_ECORE_DATA]
 [ERROR] -> [Help 1]
 [ERROR] 

constraint COMPLEMENTJSON_JSON_CHECK check (COMPLEMENTJSON IS JSON) utilise une fonction oracle12. Si une erreur intervient sur cette table, cela signifie que nous ne sommes pas sur une base oracle12

 [ERROR] Reason: liquibase.exception.DatabaseException: ORA-02289: la s�quence n'existe pas
 [ERROR] ORA-06512: � "TST_INT_MIG_ERDF.GESTIONIDENTIFIANT", ligne 139

 Cette erreur se produit sur les tests de non régression en général. Se produit quand l'import s'est mal passé. Il faut donc remonter sur le job d'import et voir ou se trouve l'erreur.

 PostgreSQL 

 Modifications code SqlMigrator 
Des modifications de code sont nécessaire pour faire fonctionner sqlMigrator avec d'autres datasource.

Les adaptations de code côté sqlMigrator : https://gerrit.efluid.uem.lan/c/sqlMigrator/+/157998

Ajout d'un Dialect permettant de spécifier côté oracle ou postgre si des différences sont à prendre en compte.

Les scripts de preparation (CreatePackageGestionIdentifiants …) ont été splittés dans deux dossiers. Un dossier oracle et un postgre.

sans_cadre|default

 Architecture d'un projet pour sql/database (exemple Archi) 

Avant l'architecture était :

 ddl/
   drop/
   create/
         table/
         sequence/
         package/
   index/
        DescriptionIndex.csv
   upgrade/
        4/
        5/

On considère que les actions drop sont les mêmes peu importe la datasource actuellement. 
Seules les actions create et upgrade peuvent différer. Notamment à cause des différences remontées dans ce document (comme les type de colonne non supportés d'un SGBD à un autre) : http://wperoom4.uem.lan/eRoom/Production/GestionProjetEfluid/0_27ea13 

On va donc splitté le dossier create en deux sous dossiers qui sont nécessaires car le traitement est fait par simple fichier sql :
 ddl/
   drop/ 
   create/
         oracle/
               table/
               sequence/
               package/
         PostgreSQL/
               table/
               sequence/
   index/
   upgrade/ 
   
Côté upgrade, il y a moins d'obligations à splitté, différentes façons de faire peuvent être envisager. Donc on resterait sur un même dossier upgrade pour tous.

Côté projet archi, modification de la structure des ddl/dml : https://gerrit.efluid.uem.lan/c/archi/+/158912

 Comment traiter la partie upgrade pour ne pas dupliquer ? 

L'idée et de rendre la vie plus facile pour les devs et qu'ils aient à dupliquer ou à penser à la datasource le moins possibles. 
Plusieurs possibilités, mais l'idée serait par ordre de priorité de traiter les cas comme suit :

1-  Utiliser un maximum les balises liquibase : https://docs.liquibase.com/change-types/community/home.html 

Les balises liquibase pour la majorité peuvent être utilisées sur plusieurs datasource sans préciser quelque chose de particulier.

2-  Rendre les scripts communs pour les deux SGBD grâce aux properties 

Les properties sont des propriétés paramétrable dans les changeLog.xml qui peuvent être utilisées via des varibles dans les balises liquibase.
Par exemple, si je sais que je vais créer une table mais que je dois utiliser des BLOB. Pour Oracle ça fonctionne, mais pour PG, le BLOB n'existe pas, on utilisera BYTEA. 
On pourrait donc avoir :

   
   
   
                        
            
         
   
   
Les properties sont valables pour tout un fichier changeLog.xml (à vérifier si on peut également tirer ça d'un changeLog parent). On pourrait alors définir nos spécificités avec ça et continuer d'utiliser les balises liquibase.

3-  Avoir un changeLog particulier pour un SGBD grâce à l'attribut dbms de liquibase

Dans certains cas, les commandes sql sont trop complexes pour pouvoir être utiliser via des balises liquibase. Notamment les scripts de haute volumétrie.
Dans ce cas de figure, on peut tout simplement utiliser l'attribut dbms sur un changeset en spécifiant la datasource voulue :

Cas Oracle : 
                      
            
            
         
   

Cas PostgreSQL : 
                      
            
            
         
   

On peut également penser à traiter ces changes particuliers en les incluants dans un fichier changelog nommé spécifiquement changeLog_Oracle.xml par exemple.

 Temps observés 

Concernant la création de schéma from scratch pour archi :

Oracle : Ran in 0:01:32.892

PostgreSQL : Ran in 0:01:08.356

A voir par la suite sur les upgrades et sur la création from scratch d'efluid si différence

 Questions et travail restant 

 Est ce qu'il y a une limite sur la taille des colonnes comme ce qu'on a connu sur Oracle ?
--> pas de limite à 30 comme Oracle pour la v13. On est à plus de 100 donc on devrait être bon.

 Dans les tables CHANGELOG, on avait en oracle l'insertion de CLIENT_ID et IP_ADRESS 
  CLIENT_ID     VARCHAR2(64 BYTE) DEFAULT SYS_CONTEXT('USERENV', 'CLIENT_IDENTIFIER'),
  IP_ADDRESS    VARCHAR2(15) DEFAULT SYS_CONTEXT('USERENV', 'IP_ADDRESS')
Quel est l'équivalent PG ?
A--> voir côté doc et variable SYSTEM, il y a un équivalent 

 TBS : Est ce qu'il y a encore un intêret à devoir les spécifier lors des créations de table ? La majorité partent sur le TBS par défaut. Cela ne suffit pas ?
--> A rediscuter mais on garder les variables TBS. 

 Equivalent paramètre PARALLEL en PG ? Comment va être gérer la haute volumétrie ?
--> A analyser

 Il faut ré ecrire GestionIDentifiant et ModificationStructure
--> A analyser et rediscuter (354833 et 354694)

 Il faut revoir la moulinette de gestion des indexs 
--> A analyser (354694)

 Est ce que les scripts finish sont encore tous nécessaires ?
--> A analyser (360910)
 Comment sont gérées les colonnes JSON en PG ? Actuellement en oracle on doit définir un store as, en PG une simple création de colonne en spécifiant json suffit-elle ?
--> Potentiellement rien de plus à faire. Juste à définir si on garde les colonnes de type json ou jsonb pour PG.

 Voir pour ajouter plus de tests dans sqlMigrator pour la partie PG -> nécessite de travailler sur les jobs de validation Jenkins et d'avoir ce qu'il faut pour créer des schémas BDD PG<!!!>Description
SonarQube est le nouveau nom de Sonar.
C'est un outil gratuit et open-source de mesure de la qualité du code.
Lien : especteur 

Il permet, entre autres, de :
 localiser les endroits où les tests sont trop superficiels, voire inexistants;
 détecter des incompréhensions ou des étourderies, puis de nous orienter dans la bonne direction pour corriger (que ça soit de l'ordre de la gestion de la mémoire jusqu'à l'élégance du code);
 vérifier que le projet est bien délimité, en contrôlant les dépendances entre les différents packages, et que les conventions internes sont bien respectées (ex: pas d'appel à la couche "process" dans la couche "présentation");
 générer plein de chiffres et de graphes pour montrer au chef que la qualité du projet augmente.

SonarQube se base sur 7 axes pour mesurer la qualité du code :
 Bugs potentiels : anomalies remontées provoquant un bug;
 Règles de codage : anomalies remontées affectant la lisibilité/maintenabilité du code;
 Tests : la couverture des tests et leur succès (plus y'en a, mieux c'est);
 Duplications : les copier-coller (moins y'en a, mieux c'est);
 Commentaires : les API publiques doivent être documentées et les commentaires "non-javadoc" sont à bannir;
 Architecture et design : les problèmes de dépendances entre packages;
 Complexité : le nombre de méthode par classe (moins y'en a mieux c'est), le nombre de ligne par méthode (le nombre parfait est ici 1);

Tous ces axes sont mesurés au travers de "widgets" paramétrables sur le "dashboard" de SonarQube et par projet (efluid, ecore, suivefluid, etc.).

Utilisation de base
Page d'accueil
Sur la page d'accueil de SonarQube, on peut voir chaque projet avec un résumé en quelques chiffres.
Les colonnes particulièrement importantes sont celles nommées "ISSUES" (problèmes) et "UTS SUCCESS" (réussite des tests unitaires). Les colonnes "ISSUSES" permettent de suivre le nombre d'anomalies détectées et leurs tendances.

Fichier:Sonar_main.PNG

Consultation d'un projet
Lors du clic sur le nom d'un projet, par exemple "efluid", on peut consulter plus en détail les métriques.
Sur la capture suivante, on a:
<li>A: le "chemin de fer" ("breadcrumb" dans la doc) du projet en cours de consultation (par exemple: "efluid>crm-contrat");
<li>B: le détail des anomalies remontées en fonction de leur "poids";
<li>C: "Components" permet d'accéder aux différents composants (modules, packages, classes) du projet en cours de consultation.
Le reste de la page est composé de "widgets" que l'on peut paramétrer, ajouter, déplacer...

Fichier:Sonar_projet.png

Anomalies
Depuis un projet, en cliquant sur un groupe d'anomalies ou sur "Issues Drilldown" dans le menu "TOOLS" à gauche, on peut consulter les anomalies détectées dans ce projet.
Il est possible de les filtrer par sévérité, ou par delta de temps ("Time changes...").

Fichier:Sonar_issuesdrilldown.PNG

Exemples
Pour exemple, recherchons une anomalie bloquante dans le projet efluid:
<li>Depuis le "Dashboard" (c'est la page d'accueil);
<li>Cliquer sur le projet "efluid"
<li>Sur l'écran de consultation du projet, cliquer sur "Blocker" pour les événements bloquants;
Ou sur "Issues Drilldown" à gauche pour consulter toutes les anomalies du projet "efluid";
<li>Cliquer sur le ou les fichier qui nous intéresse;
<li>La belle erreur avec une belle description qui nous explique pourquoi c'est pas bien;
<li>Corriger !
Fichier:Sonar_examples_1_2.PNG
Fichier:Sonar_examples_3_4.PNG 

En cliquant sur "Issues Drilldown" depuis le projet efluid, on aurait la totalité des anomalies, triées par jour d'apparition, sévérité, anomalie, package et fichier:

Fichier:Sonar_issues_detail.png

Anomalies

Sévérité
Il y a 5 types d'anomalies différentes:
<li>Blocker: pas de doute, c'est un bug;
<li>Critical: c'est sûrement un bug, ou alors ça ne va pas tarder;
<li>Major: c'est mal codé, inefficace;
<li>Minor: c'est inefficace, ou le code ne respecte pas les conventions;
<li>Info: le code ne respecte pas les conventions.
Fichier:Sonar_issues.PNG

Sur n'importe quelle page de SonarQube, on a toujours accès au menu avec notamment les liens "Issues", "Rules" et "Quality Profiles".

Fichier:Sonar_menu.PNG

Issues
La page "Issues" liste tous les problèmes remontés par SonarQube.
On peut, rechercher les anomalies suivant plusieurs filtres, et les consulter en dessous.
A la différence de l'approche par projet, package, etc., ici tout est en vrac. Vue la répartition en groupe de dev, cette section ne devrait pas être très utile.

Fichier:Sonar_rules.png

Rules
De la même manière, la page "Rules" liste, en vrac, toutes les anomalies actuellement détectable par SonarQube.
Elles ne sont pas toutes actives, heureusement, il y en a environ 3000 par défaut ! Seules celles dans les Quality Profiles "efluid" et "commun efluid" sont activées.
Il est possible d'en ajouter de nouvelles, personnalisées, ex: interdire l'appel de la couche "process" dans la couche "web".

Fichier:Sonar_rules2.png

Quality Profiles
Les "Quality Profiles", sont les anomalies paramétrées pour être remontées par SonarQube. Il y en a une 30aine actuellement (sur 3000 possibles donc...)
On peut paramétrer leur pertinence, leur sévérité, leur analyse dans tel ou tel projet.
Si vous voulez, par exemple, savoir quels sont les anomalies bloquantes, vous pouvez le voir ici. En cliquant sur "Blocker" vous serez redirigés vers la page "Rules" en filtrant sur la sévérité voulue.

Fichier:Sonar_qualityprofiles.PNG

Category:outil<!!!> Procédure d'installation des JDKs 

Il faut installer tous les JDK mentionnés ci-dessous pour que votre poste de développeur soit valide.

 Installation du JDK  

Récuperez la version du jdk dans https://eartifact.efluid.uem.lan/artifactory/ext-release-local/org/openjdk/jdk//jdk.zip (il s'agit d'un zip) puis dézippez l'archive dans le répertoire d'installation classique des jdks :
 D:\Programs\jdk\jdk

Fichier:DezipJDK8_01.jpg

N'oubliez pas de mettre vos variables d'environnements à jour en suivant la procédure ici]

 Installation du JDK 11.0.8 
Récuperez la version du jdk11.0.8 dans   (il s'agit d'un zip) puis dézippez l'archive dans le répertoire d'installation classique des jdks :
 D:\Programs\jdk\jdk11.0.8

 Installation du JDK   

Récuperez la version du jdk désirée dans  puis lancez l'installation windows classique, en spécifiant le répertoire d'installation du JDK :
 D:\Programs\jdk\jdk

Fichier:DezipJDK8_01.jpg

Il faut ensuite ajouter quelques fichiers dans votre installation du JDK :
 Copier le fichier , dans le sous-répertoire de la jre : D:\programs\jdk\jdk\jre\lib\security
 Dézipper l'archive  dans le sous-répertoire de la jre : D:\programs\jdk\jdk\jre\lib\security remplacez les deux jars local_policy.jar et US_export_policy.jar

 Mise à jour de la variable d'environnement JAVA_HOME 

Il faut vérifier que l'on a bien une variable d'environnement JAVA_HOME existante et que celle-ci soit présente dans le PATH. 
 Panneau de configuration > Système et sécurité > Système > Paramètres système avancés > Variables d'environnement

La variable JAVA_HOME doit être valorisée à JAVA_HOME = "D:\programs\jdk\jdk"

Fichier:VEnv_01.jpg

Dans la variable PATH, il vaut vérifier qu'on retrouve bien la chaîne %JAVA_HOME%\bin en tête de PATH

Fichier:VEnv_02.jpg

En mode console : 
Utilisez la commande : echo %JAVA_HOME%

Fichier:JAVA HOME CMD.jpeg

 Mise à jour de la version du JDK dans eclipse 

 Mettre à jour le JDK dans eclipse

 Constitution du zip de JDK 

Pour constituer les zip JDK Windows et Linux il faut utiliser le job jenkins dédié : https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellerelease/job/Ftools/job/Fjdk/job/jdk.create-new-version-zip/

Une fois un zip Linux constitué il faut mettre à jour l'image Docker socle jenkins java et mettre à jour le RPM java embedded (tout est dans etools)

Le changement de version nécessite deux changes par exemple : https://gerrit.efluid.uem.lan/q/OpenJDK%252B17

Les étapes de constitutions sont listées ci-dessous

 11 
 Windows 
 Télécharger le JDK sur le site Adoptium (https://adoptium.net/temurin/releases?version=11) pour la bonne target (Linux, Windows etc...)
 Le dézipper dans un répertoire temporaire, qu'on zippera toutes les opérations complémentaires terminées
 Ajouter Eclipse Mission Control (tous les dossiers dans bin) dans le dossier bin (bien prendre la version pour la bonne target) https://adoptium.net/jmc
 Vérifier que la propriété crypto.policy vaut bien unlimited dans le fichier conf/security/java.security

 Linux 
 Vérifier que la propriété securerandom.source vaut file:/dev/./urandom dans le fichier conf/security/java.security
 Ajouter Eclipse Mission Control (tous les dossiers dans bin) dans le dossier bin (bien prendre la version pour la bonne target)=> https://adoptium.net/jmc
 Faire attention à ce que les fichiers dans bin soient bien executables

 1.8 

 Tests réalisés sur le jdk 1.8.0_172 
A partir de la version 1.8.0_172 : il existe à partir de cette version un dossier policy > unlimited et limited avec les deux jar.

Par défaut, le jdk pointe sur la policy unlimited pour le changer il faut aller dans jdk/jre/lib/security/java.security la properties : crypto.policy.
# Cryptographic Jurisdiction Policy defaults
#
# Import and export control rules on cryptographic software vary from
# country to country.  By default, the JDK provides two different sets of
# cryptographic policy files:
#
#     unlimited:  These policy files contain no restrictions on cryptographic
#                 strengths or algorithms.
#
#     limited:    These policy files contain more restricted cryptographic
#                 strengths, and are still available if your country or
#                 usage requires the traditional restrictive policy.
#
# The JDK JCE framework uses the unlimited policy files by default.
# However the user may explicitly choose a set either by defining the
# "crypto.policy" Security property or by installing valid JCE policy
# jar files into the traditional JDK installation location.  To better
# support older JDK Update releases, the "crypto.policy" property is not
# defined by default.  See below for more information.
#
# The following logic determines which policy files are used:
#
#         <java-home> refers to the directory where the JRE was
#         installed and may be determined using the "java.home"
#         System property.
#
# 1.  If the Security property "crypto.policy" has been defined,
#     then the following mechanism is used:
#
#     The policy files are stored as jar files in subdirectories of
# <java-home>/lib/security/policy.  Each directory contains a complete
# set of policy files.
#
#     The "crypto.policy" Security property controls the directory
#     selection, and thus the effective cryptographic policy.
#
# The default set of directories is:
#
#     limited | unlimited
#
# 2.  If the "crypto.policy" property is not set and the traditional
#     US_export_policy.jar and local_policy.jar files
#     (e.g. limited/unlimited) are found in the legacy
#     <java-home>/lib/security directory, then the rules embedded within
#     those jar files will be used. This helps preserve compatibility
# for users upgrading from an older installation.
#
# 3.  If the jar files are not present in the legacy location
#     and the "crypto.policy" Security property is not defined,
#     then the JDK will use the unlimited settings (equivalent to
#     crypto.policy=unlimited)
#
# Please see the JCA documentation for additional information on these
# files and formats.
#
# YOU ARE ADVISED TO CONSULT YOUR EXPORT/IMPORT CONTROL COUNSEL OR ATTORNEY
# TO DETERMINE THE EXACT REQUIREMENTS.
#
# Please note that the JCE for Java SE, including the JCE framework,
# cryptographic policy files, and standard JCE providers provided with
# the Java SE, have been reviewed and approved for export as mass market
# encryption item by the US Bureau of Industry and Security.
#
# Note: This property is currently used by the JDK Reference implementation.
# It is not guaranteed to be examined and used by other implementations.
#
#crypto.policy=unlimited

 test sous eclipse  
Le cas de test est le suivant : Nous voulons tester qu'on pointe bien sur crypto.policy=unlimited

public static void main(String[] args) {
   try {
     int maxKeyLen = Cipher.getMaxAllowedKeyLength("AES");
     System.out.println(maxKeyLen);
   } catch (Exception e) {
     System.out.println("Sad world :(");
   }
 }

résultat console : en unlimited : 2147483647 , limited : 128

 Intégrer une nouvelle version du Jdk dans l'usine logicielle 
Guide de migration du JDK

 générer le tar.gz dans artifactory à l'aide du job https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellerelease/job/Ftools/job/Fjdk/job/jdk.create-new-version-zip
 mettre à jour les images : https://gerrit.efluid.uem.lan/c/etools/+/247489
 mettre à jour le pom parent : https://gerrit.efluid.uem.lan/c/efluidUtilsPom/+/247490
 générer le nouveau rpm : https://gerrit.efluid.uem.lan/c/etools/+/274670 + https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellerelease/job/Ftools/job/Fetools/job/etools-rpm-java.release/
 mettre à jour les images embedded et batch : https://gerrit.efluid.uem.lan/c/scriptInstalleur/+/260020
 mettre à jour le wiki : https://wikefluid.efluid.uem.lan/index.php/Mod%C3%A8le:Outil.jdk.version

 Bugs répertorié 
 Problème de latence dans les jobs 

 Problème de lenteur jobs Jenkins : Dans chaque répertoire /opt/jdk1.*/jre/lib/security/java.security
 Modifier la ligne : securerandom.source=file:/dev/urandom --> Par : securerandom.source=file:/dev/./urandom<!!!>Category:outil
Category:tomcat

 Prérequis 

Tomcat nécessite l’installation d’un JDK.

 Installation de Tomcat via l'archive classique 
 Récupération de l'archive 

Le fichier Zip permettant l’installation de Tomcat se trouve à l'adresse suivante : https://archive.apache.org/dist/tomcat/tomcat-10/v/bin/apache-tomcat-.zip

 Installation de Tomcat 
 Dézippage de l'archive 

 Dé-zippez le fichier d’archive zip dans le répertoire « D:\Programs\tomcat ». Le sous-répertoire « apache-tomcat-X.X.X » est créé dans le dossier « D:\Programs\tomcat »

 Paramétrage de l'installation 

 Suivez cette documentation pour paramétrer le comportement des JSP.<!!!>Category:outil

 Documentation 
https://wikefluid.efluid.uem.lan/docInstalleur/artifactory-rules/<!!!><!!!>Category:outil

 Présentation SqlMigrator 

Vous trouverez la dernière présentation sqlmigrator ainsi que le ppt associé ici : https://wikefluid.efluid.uem.lan/partage/sqlmigrator/ 
 Liste relecteurs SQL 

 Gabriel Barthelemy (Relance/contentieux/cac)
 Brice Castagna (Contrat)
 Anthony Bainville (Référentiel)
 Victor Cholley-Barroyer (Courbe/enercom/échange)
 Marie Coste (UL)
 Carole Favier  (Intervention)
 Roderick Pierre (Consommation)
 Stéphane Poirot (Consommation)
 Vincent Poutissou (Composants)
 Mickael Storck (eot / conso / relève)
 Didier GRZEJSZCZAK (eot / conso / relève / Intervention)
 Heloise Turquais (offre)

 Documentation sqlMigrator 

Pour rappel la plupart des cas sont décrits dans : Documentation sqlMigrator

 Structure du changeSet 
Conditions à vérifier : 
 Présence des attributs obligatoires avec la bonne convention d'écriture décrite ci-dessous :
 [obligatoire] author : auteur du changeSet : il faut mettre son login défini dans l’annuaire LDAP (c’est normalement la valeur par défaut renseignée par map4J).
 [obligatoire] id : suivefluid-<Numéro d'événement suivefluid>-dml|ddl
 [obligatoire] labels : <id ChangeSet>, v12|v13, dml|ddl (les labels n'influe pas sur le checksum)
 [optionnel] context : Chaque contexte doit être séparé d'une virgule et écrit en minuscule. Un contexte peut être écrit edfsei_* pour définir tous les sites edfsei Liste des contextes existants

Attention : les attributs context et labels sont sensible à l'orthographe. Il s'agit donc bien de context et labels

 Un ChangeSet déjà livré ne peut en aucun cas être modifié. En effet, chaque ChangeSet sql migrator se voit affecté un hash calculé à partir de 3 éléments :
 L’ID du ChangeSet 
 Le chemin absolu du ChangeSet 
 Le contenu du ChangeSet (y compris le script sql lié si présent)

A chaque exécution de sql migrator, pour chaque ID de ChangeSet donné, le hash est recalculé pour vérification : si l’on modifie le chemin et/ou le contenu du script sans en modifier l’ID, un hash différent va être calculé et cela va faire planter sql migrator.

Dans certains cas, on peut modifier un ChangeSet déjà livré (par exemple une coquille dans le fichier sql) mais pour cela il faut donc s'assurer de changer le change ID tout en s'assurant aussi que cela ne posera pas de problème à l'exécution : en effet, si le change n'est pas rejouable (create...), il y aura des erreurs (doublons de tables, etc...). C'est pour cela que cette tolérance ne doit être appliquée qu'aux données (dml) ou aux changes liés aux vues requêteur (car chaque script commence par la clause CREATE OR REPLACE : c'est donc rejouable sans problème sql).

 La mise à jour des ddl (CREATE/DROP) doit être livré systématiquement à chaque fois qu'on ajoute un upgrade ddl
 Une modification ddl ne doit jamais comporter de contexte, c'est systématiquement du produit
 Il faut privilégier la livraison d'un fichier dans le cas de trop gros blocs SQL.
 Dans le cas de plusieurs modifications de structure dans un même change GERRIT, il faut livrer un changeSet sqlMigrator par modification. 

 Utilisation du PARALLEL dans Oracle 

Pour rappel, une convention s'applique sur l'utilisation de la variable PARALLEL. Cette dernière ne doit être utilisée que pour Oracle.
En fonction de l'utilisation dans un .sql ou un .xml, sa définition diffère : https://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/sqlmigrator/conventions_vocabulaire.html

Evt en cours : 401934

Cet evt a été créé afin d'automatiser une règle de relecture. En effet suite à des anomalies ouvertes par Enedis, nous nous sommes rendus compte qu'ils surchargeait la valeur de ce paramètre. Lors de nos réinits, les montées de versions sont donc KO.
Afin d'éviter ce problème, lors de l'utilisation de la variable PARALLEL dans un script, il est demandé d'ajouter la balise ANY qui permet de ne pas rencontrer de soucis.

 Cas de régularisation 
Dans le cas d'une régularisation de colonne ou d'une table, le changeset doit obligatoirement comporter des pré conditions. cf Documentation Pre conditions

 Correspondance entre les types liquibase, Oracle et PostgreSQL 

 Liquibase  PostgreSQL  Oracle boolean  BOOLEAN  NUMBER(1) tinyint  SMALLINT  NUMBER(3) int  INT  INTEGER mediumint  MEDIUMINT  MEDIUMINT bigint  BIGINT  NUMBER(38, 0) float  FLOAT  FLOAT double  DOUBLE PRECISION  FLOAT(24) decimal  DECIMAL  DECIMAL number  numeric  NUMBER blob  BYTEA  BLOB function  FUNCTION  FUNCTION UNKNOWN  UNKNOWN  UNKNOWN datetime  TIMESTAMP WITHOUT TIME ZONE  TIMESTAMP time  TIME WITHOUT TIME ZONE  DATE timestamp  TIMESTAMP WITHOUT TIME ZONE  TIMESTAMP date  date  date char  CHAR  CHAR varchar  VARCHAR  VARCHAR2 nchar  NCHAR  NCHAR nvarchar  VARCHAR  NVARCHAR2 clob  TEXT  CLOB currency  DECIMAL  NUMBER(15, 2) uuid  UUID  RAW(16)

 Modifications de schema 
Cette partie présente les modifications de schéma autorisées et la manière dont il faut les livrer.
Le leitmotiv justifiant l'existance de ces procédures de livraison est le suivant: La version N du code source d'efluid doit pouvoir fonctionner sur une version N+1 de la base de données migrée à l'aide de SQLMigrator. Ceci assure une rétro-compatibilité entre application et base de données et permet une montée de version au fil de l'eau (d'abord la base, puis les différents serveurs d'application).

Peu importe le cas, s'il y une partie "initialisation" dans un fichier .ddl et une partie "upgrade" dans un fichier changelog.xml, il faut veiller à ce que l'orthographe des objets traités soit identique entre les 2 fichier.

 Cas de suppression 

 Cas 1: Il n'est fait aucune mention de ma colonne/table dans le code source de mon application en version N.  
 Solution: 
 Je peux livrer mon script de suppression de colonne/table en version N

 Cas 2: Des références à ma colonne/table existent encore dans le code source de l'application en version N
 Solution: 
 Je supprime les références à cette colonne/table dans le code source de l'application. 
 Je livre ces modifications de code en version N. 
 Je livre le script de suppression de colonne/table en version N+1, pour assurer une compatibilité entre la version N de mon code source et la version N+1 de ma base de données.

Exception : si la colonne/table est supprimée dans la même version où elle a été créée, cette procédure n'est pas nécessaire : on peut effectuer la suppression dans la version N.

 Création d'une table 

Conditions à vérifier : 
 Vérifier l'utilisation de tablespaces
 Vérifier qu'un drop et un create de ddl associé à bien été livré avec l'upgrade
 Dans le cas d'une création de table temporaire, le tablespace ne doit pas être renseigné.
 Dans le cas d'une création de table batchs (BATCH_*), le tablespace doit être renseigné et pointer le tablespace dédié aux tables batchs.
 Dans le cas d'une création de table batchs (BATCH_*), elle doit être en mode NOLOGGING.
create table MA_TABLE_BATCH(
COL1 VARCHAR2(10),
COL2 DATE,
...
COLN VARCHAR2(10)
) NOLOGGING tablespace &&TBS_BATCH_DATA.;

 Dans le cas d'une création de table batchs (BATCH_*), créer un index composite sur PROGRAM_ID,TECH_LOT_ID, rajouter CODE_BATCH à l'index si plusieurs batchs utilisent la même table.

La création d'une table peut être faite comme suit : 
 Fichier sql comportant une création de table standard.

Pas besoin d'alimenter le fichier DescriptionIndex.csv : les créations d'index sont présentes dans le DDL ou dans le script d'upgrade.

 Renommage d'une table 

Conditions à vérifier : 

Le renommage d'une table peut être fait de deux manières : 
 Fichier sql comportant un ddl de renommage de la table standard (ALTER TABLE <OLDTABLE> RENAME TO <NEWTABLE>;).
 Manière liquibase avec des balises <renameTable> Documentation
Dans les 2 cas il faudra livrer aussi un script de renommage de la contrainte et de l'index de clé primaire à exécuter après le renommage de la table:
 ALTER TABLE <NEWTABLE> RENAME CONSTRAINT PK_<OLDTABLE> TO PK_<NEWTABLE>;
 ALTER INDEX PK_<OLDTABLE> rename to PK_<NEWTABLE>;
La nouvelle PK doit aussi être mise à jour dans le ddl de création de la table.

Et mettre à jour le nom de la table pour les index (hors PK) dans le fichier de référentiel des index DescriptionIndex.csv.

Vérifiez l'existence de dml d'init (de test ou non).

 Suppression d'une table 

Conditions à vérifier :  
 Vérifier la présence de pré conditions
 Vérifier que la suppression se fait dans le bon dossier (cf Cas de suppression au dessus)

La suppression d'une table peut être fait de deux manières : 
 Fichier sql comportant un drop de table standard avec purge (DROP TABLE XXXX PURGE ;).
 Manière liquibase avec des balises <dropTable> Documentation

Supprimer Les lignes contenant le table droppée dans le fichier de référentiel des index (DescriptionIndex.csv).

Vérifier l'existence de dml d'init (de test ou non).

 Ajout d'une colonne 

Conditions à vérifier : 

Si la colonne créée est une colonne XXX_ID (ou SOURCE/DEST dans le cas des tables de lien), vérifier qu'un index associé a été ajouté dans le fichier DescriptionIndex.csv, sauf si l'ajout se fait sur une table batch.

La création d'une colonne peut être fait de deux manières : 
 Fichier sql comportant un alter table standard.
 Manière liquibase avec des balises <addColumn> Documentation
 Lors d'un ajout de colonne, si cette dernière possède une defaultValue, précisez la defaultValueNumeric (ou default prévue pour votre champ) mais ne pas préciser en plus l'attribut value. Ce dernier va ralentir la montée de version et n'apporte pas quelque chose de supplémentaire
 A noter que si l'attribut "type" de la balise "column" vaut "varchar" et non "varchar2" (la documentation Oracle préconisant de ne pas utiliser varchar et de lui préférer varchar2), liquibase réalise la conversion automatiquement. Cette dernière n'est cependant mentionnée nulle part dans la documentation, à surveiller donc.

Vérifier l'existence de dml d'init (de test ou non).

 Modification de structure d'une colonne 
 Augmentation de la taille d'une colonne 

Conditions à vérifier : 
 Vérifier qu'il n'existe pas d'index fonction ou bitmap de jointure sur la colonne dans le référentiel des index, le cas échéant :
 - Passer par un script SQL
 1) Supprimer l'index fonction/bitmap de jointure : DROP INDEX NOM_INDEX
 2) Modifier la colonne: ALTER TABLE MODIFY...
 3) Recréer l'index fonction/bitmap de jointure

Dans les autres cas, l'augmentation de la taille d'une colonne peut être faite de deux manières : 
 Fichier sql comportant un alter table modify standard. 
 Manière liquibase avec des balises modifyDataType/newDataType Documentation

 Renommage d'une colonne 

Conditions à vérifier : 
 Vérifier qu'il n'existe pas d'index dans le fichier DescriptionIndex.csv référençant la colonne qui va être renommée, auquel cas ajouter une version de fin à l'ancien index et recréer un nouvel index.

Le renommage d'une colonne doit se faire de la façon suivante : 
 Ajout d'une colonne avec le nouveau nom dans la version majeur N.
 Suppression de le la colonne avec l'ancien nom dans la version majeur N+1.

 Il est conseillé de migrer les données de l'ancienne colonne vers la nouvelle juste après avoir créé la colonne dans le fichier d'upgrade ddl (exception où l'on peut mettre du dml dans du ddl) : en effet, les ddl étant tous joués séquentiellement (dans leur ordre d'inclusion dans le changeLog parent), puis la même chose avec tous les dml, si on code la migration dans la partie dml, un possible autre script dml affectant la nouvelle colonne pourrait livré par la suite mais pourrait être inclus avant notre script de migration, corrompant alors les données (puisque notre script de migration, s'il passe après, va écraser les valeurs).

Le renommage de colonne dans une même version majeur pose des problèmes au niveau des bases de référence ainsi que des modifications de vue.

Vérifier l'existence de dml d'init (de test ou non).

 Modification du type d'une colonne 
La modification de type d'une colonne doit faire l'objet d'une demande au pôle études et performance car nécessiterait une gestion de conversion des données déjà présentes dans la colonne

Conditions à vérifier : 

Dans l'état actuel des choses le processus suivant ne permet pas d'assurer une rétrocompatibilité code/BDD

La modification de type se fait par livraison d'un seul script SQL à livrer dans le répertoire ddl/upgrade/<versionN>/... (pour ce cas spécifique le script SQL contient à la fois des DDL et DML)

Ce script contiendra les étapes suivantes :
 Création d'une colonne <nomColonneAMigrer>_2 du nouveau type
 Recopie des données de la colonne d'origine <nomColonneAMigrer> vers la nouvelle colonne <nomColonneAMigrer>_2 (utiliser les mécanismes PARALLEL/NOLOGGING pour accélérer la mise à jour des données)
 Suppression de la colonne d'origine <nomColonneAMigrer>
 Renommage de la colonne <nomColonneAMigrer>_2 vers <nomColonneAMigrer>

Exemple :
alter table tmodeleobjetmetier  add TYPEESPACE_NEW NUMBER(2);
update tmodeleobjetmetier set TYPEESPACE_NEW = TYPEESPACE;
commit;
alter table tmodeleobjetmetier drop column TYPEESPACE;
alter table tmodeleobjetmetier rename column TYPEESPACE_NEW to TYPEESPACE;

Ou via SQLMigrator si faible volumétrie et petite modification structurelle comme augmenter la taille :

Exemple :
 <changeSet author="bouthino" id="suivefluid-350700-ddl" labels="suivefluid-360645-ddl, v15, ddl" objectQuotingStrategy="LEGACY">
  <modifyDataType columnName="TYPEESPACE" newDataType="NUMBER(2)" tableName="TMODELEOBJETMETIER"/>
 </changeSet>

Les index éventuels supprimés seront automatiquement reconstruits par le process de mise à jour des index.

Vérifier l'existence de dml d'init (de test ou non).

 Suppression d'une colonne

Conditions à vérifier : 
 Vérifier la présence de pré conditions
 Vérifier que la suppression se fait dans le bon dossier (cf Cas de suppression au dessus)
 En v14 uniquement, vérifier que l'appel à la reconstruction des indexs avant le drop est bien faite : ModificationStructure.rebuildIndexesBeforeDrop('TABLENAME','COLUMNNAME')
 Attention ici : comme cette fonction est spécifique oracle, il faut ajouter un attribut "dbms=oracle" sur le tag <sql> qui inclut cette clause.
 A partir de la v15, ne plus utiliser cette fonction
 Plus d'infos sur la migration vers postgresql : http://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/migration_postgre.html#compatiblite

La suppression d'une colonne peut être fait de deux manières : 
 Fichier sql comportant un drop de colonne standard.
 Manière liquibase avec des balises <dropColumn>

Vérifier l'existence de dml d'init (de test ou non).

 Séquence et Vue 
 Création 

Conditions à vérifier : 
 Pour une vue, vérifier la présence d'un "create or replace"

La création d'une séquence ou d'une vue peut être fait de deux manières : 
 Fichier sql comportant un create standard.
 Manière liquibase avec des balises <addSequence> ou <addView> Documentation séquence
Documentation vue
 Le cache des séquences doit être positionné à 20

Comme pour toute création/modification de structure : pas de context. Quand bien même les séquences véhiculent des informations concernant des clients en particuliers (typiquement les séquences de numérotation de factures), ne pas avoir les séquences de créées chez tout le monde de la même façon peut poser des soucis lorsqu'on veut diagnostiquer des anomalies de montée de version sur des BDD issues d'un client différent de celui à l'origine de l'anomalie (c'est déjà arrivé par le passé). De plus, aucune directive officielle concernant le caractère sensible de ces séquences n'a été donnée, en conséquence, il vaut mieux observer un principe de précaution et créer les séquences de la même manière chez tous les clients.

 Suppression 

Conditions à vérifier : 
 Vérifier la présence de pré conditions
 Vérifier que la suppression se fait dans le bon dossier (cf Cas de suppression au dessus)

La suppression d'une séquence ou d'une vue peut être fait de deux manières : 
 Fichier sql comportant un drop de séquence/vue standard.
 Manière liquibase avec des balises <dropSequence> ou <dropView> Documentation séquence
Documentation vue

 Procédure PL/SQL  

Conditions à vérifier : 
 Les caractères spéciaux &, <, > doivent être remplacés par leurs char correspondant. 
 Dans le cas d'une procédure complexe, le script doit être livré dans des blocs <createProcedure> afin que sqlMigrator se joue correctement Procédure complexe  Suivefluid/Suiveclient  Cas des synonyms et des grantLors de la livraison de synonyms ou de grant, il faut :
 Inclure un nouveau changeset qui inclut un fichier SQL uniquement !!! Les fichiers xml ne fonctionnent pas. Les balises sql non plus !
 Créer un fichier sql avec uniquement les create or replace de synonyms et de grant avec les bonnes variables.

 Index  
 Création d'un index depuis la V15 

Le fichier DescriptionIndex.csv n'existe plus. Il a été migré et remplacé par un nouveau fichier contenant directement les "changeset" de création des index. Il s'agit donc d'un fichier xml contenant les commandes liquibase de création d'index.
Le nouveau fichier trouve dans le dossier "ddl/upgrade/NUMERO_VERSION_PRODUIT/changelog/index". Dans le fichier, les changeset sont concaténés.

Ajouter le code suivant pour créer un index : 
<changeSet author="XXX" labels="NOM_INDEX" id="CREATE_NOM_INDEX">
  <preConditions onFail="MARK_RAN">
    <not>
      <indexExists indexName="NOM_INDEX"/>
    </not>
  </preConditions>
  <createIndex indexName="NOM_INDEX" tableName="Nom_TABLE" tablespace="${tablespace.index}">
    <column name="NOM_COLONNE_A_INDEXER"/>
  </createIndex>
</changeSet>

Pour plus de détails, voir la documentation officielle de liquibase : https://docs.liquibase.com/change-types/create-index.html

Il faut aussi ajouter les instructions SQL de création d'index pour chaque base de données : Oracle, et PG SQL.
Se rendre dans les fichiers :
 "sql/database/efluid/ddl/create/oracle/04_index/index.sql"
 "sql/database/efluid/ddl/create/postgresql/04_index/index.sql"

Exemple : 
CREATE INDEX NOM_INDEX ON NOM_TABLE(NOM_COLONNE) TABLESPACE &&TBS_INDEX. ; 

La section ci-dessous concerne la manière de faire avant la V15

Pour raison de performances, il est demandé aux développeurs d'ajouter un index sur une colonne d'une table servant par exemple de clef étrangère. Ces colonnes ont un nom de la forme XXXXX_ID. Le pôle performance peut aussi demander l'ajout d'index sur d'autres colonnes d'une table. Il faut donc faire bien attention à ajouter ou supprimer ces index à chaque fois qu'on fait une modification dans la structure d'une table. Les cas d'utilisations les plus rencontrés sont : l'ajout de colonne, la modification de colonne, la suppression de colonne, la suppression de table...Attention : dans certains cas il faut se poser la question si l'index est déjà présent dans un autre produit. Ce dernier est parfois une brique applicative utilisée par plusieurs autres produits. Par exemple, Il faut vérifier la présence d'un index dans Ecore avant de l'ajouter dans Efluid. Et s'il n'existe pas dans cet autre produit, alors il faut se demander s'il ne devrait pas y être. Puis en terminant le raisonnement par la création de l'index dans le produit final concerné (par exemple Efluid). Ceci est important à prendre en compte aussi dans le cas de rattrapage en masse des index à ajouter ou à migrer de produit.

La mise à jour des index se fait dans un fichier portant le nom : DescriptionIndex.csv. On ne supprime pas de ligne dans le fichier. On ne fait que en ajouter ou en modifier.
Il existe un fichier par produit (Ecore, Efluid, etc...). Le fichier se trouve dans le dossier ddl/index.  Ex : sql/database/efluid/ddl/index/DescriptionIndex.csv

Le fichier CSV comporte 7 colonnes :Evénement : le numéro d'événement Suivefluid portant la création/modification de l'index.Nom de l'index : le nom de l'index limité à 30 caractères. Par convention le nom commence toujours par IDX_XXXXXXXXX.Table : le nom de la table concernée. Même limitation du nombre de caractères.Colonne(s) : le nom de la colonne de la table concernée. Même limitation du nombre de caractères. Dans des cas plus rares, on peut être amené à créer un index sur un couple de colonnes. Dans ce cas, chaque nom de colonne doit être séparé par une virgule. On rencontre ce genre de cas parfois sur les tables servant de liens. On place donc un index sur le couple : DEST, SOURCE. On rencontre aussi ce genre de cas pour optimiser les batchs (voir exemple ligne 2).Tablespace : le tablespace concerné. Il y en a deux pour ainsi différencier le domaine d'utilisation (batch et non batch) :  ${tablespace.batch.index}, ${tablespace.index}.Version d'ajout : la version du produit dans lequel l'index est créé.Version de suppression : la version du produit dans lequel l'index est supprimé. Attention dans le cas d'une suppression de colonne, il faut bien mettre la version dans laquelle la colonne de la table est supprimée du code et non pas mettre la version N+1. Exemple : Suppression d'une colonne en 14.17. Alors, je mets 14.17.100 dans la dernière colonne du fichier CSV tout en appliquant les mêmes autres consignes concernant la suppression de la colonne de la table (script de suppression placé en V15).

 événement  Nom de l'index  Table  Colonne(s)  Tablespace  version d'ajout  Version de suppression 152611  IDX_TELTPOPRELEVE_LOT  TELEMENTDEPOPULATIONRELEVE  LOT_ID  ${tablespace.index}  12.15.100  13.1.100 234178  IDX_BTCH_IMPFICH_PROG_COD_LOT  BATCH_IMPORT_FICHIER  PROGRAM_ID,CODE_BATCH,TECH_LOT_ID  ${tablespace.batch.index}  13.13.100 

 Cas de suppression d'un changeSet 

Un développeur peut être amené à faire un revert ou une suppression d'un changeSet livré.
Dans ce cas le développeur doit suivre la procédure suivante : 
 Faire un revert du commit dans lequel se trouve le script à retirer
 Se placer sur le change gerrit afin de le modifier et d'ajouter un script (sous format changeSet) faisant les modifications inverses 
 Obtenir un +1 d'un relecteur SQL et de l'usine logicielle
 Merger le change

A la fin de cette procédure, la base doit avoir repris un état comme si aucun script n'avait été passé.

Plus d'informations : http://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/liquibase.html#cas_revert

 Scripts impactant des tables hautes volumétries 
Les tables hautes volumétries sont listées ici : http://wikefluid.uem.lan/index.php/Scripts_migration_haute_volum%C3%A9trie#Liste_des_tables.

Si un script effectue des actions sur une de ces tables, alors le développeur propriétaire du change doit mesurer le temps d'exécution des actions en question sur des bases UEM, ES et Enedis et en mesurer le temps d'exécution. Si ce dernier dépasse les "quelques secondes" (à l'appréciation du relecteur SQL), alors le développeur propriétaire du change doit obtenir la validation de l'équipe études et performances, puis en notifier le relecteur SQL une fois que c'est fait.

 Vérification des scripts d'habilitations 

 Lorsqu'un nouveau risque est ajouté dans l'archi, il est créé avec un code dans HermesCodesRisques.java
 C'est à partir de ce code que l'on construit les habilitations en BDD :
 Ajout du risque dans THERMESRISQUE en utilisant ce code pour la colonne RISKDESCRIPTOR
 Ajout du/des privilèges liés à ce risque dans THERMESPRIVILEGE
 Ce privilège est associé à un groupe d'utilisateurs, en l'affectant à un profil via la table PROFILHABILITATION_LISTOFPRI7D

 Points d'attention 
Voici une liste de pratiques qui sont habituellement réalisées dans le cadre de changes sql, bien qu'aucune règle spécifique n'existe

 Si un script d'upgrade doit être livré sur plusieurs branches (par exemple, maintenance_14 et develop), il faut toujours commencer par développer sur la version la plus ancienne puis cherry-pick vers les branches les plus récentes. En effet, le dossier dans lequel est placé le script (dans ce cas, /14/) doit être le même partout sous peine de voir sql migrator planter (le chemin absolu du change set est l'un des trois éléments qui entrent en jeu lors du calcul du checksum, pour un ID de changeset donné).

 Dans le cas d'insertions dans les fichiers dml, on peut faire précéder ces insertions du delete correspondant, cela permet ainsi rendre le script rejouable et plus facile à corriger dans le cas d'un script complexe. Cela permet aussi de moins polluer les fichiers dml dans le cas d'une correction (au lieu de créer un nouveau changeSet qui va faire un update, on va juste modifier le script existant et changer l'ID du changeSet en lien).

 Le commit à la fin d'un fichier sql de données n'est normalement plus obligatoire (sql migrator fait des commit après chaque changeSet) mais reste conseillé car, dans le cas de gros scripts PL/SQL, il devient obligatoire entre certains blocs pour une question de perf. Le mettre dans tous les cas à la fin peut donc être une bonne chose afin de s'en souvenir.

 D'une manière générale, il faut toujours renseigner les acteur et date modification lorsque l'on fait un update. Cependant, si l'update concerne une colonne purement technique (par exemple ROLE si l'on veut passer d'un rôle court à un rôle long et inversement), on peut s'en passer. En effet, cela ne correspond pas une vraie modification de l'objet et l'on pourrait perdre l'acteur/la date de la dernière vraie modification de l'objet.

 Il faut éviter au maximum de mettre des contraintes sur les colonnes des tables (par exemple, NOT NULL ou DEFAULT 0). Cela fait suite à une décision prise il y a longtemps qui a pour but d'éviter le plantages lors de l'insertion de données en masse dans les tables. Il est donc demandé de gérer toutes les contraintes niveau code.

 Dans les <changeSet> des changeLog.xml il n'est pas nécessaire d'ajouter l'instruction objectQuotingStrategy="LEGACY" car il s'agit de la valeur par défaut.

 Veiller à ne pas oublier les attributs onFail et onFailMessage lors de l'utilisation de la balise <preConditions>. En effet, sql migrator plante si la pré-condition n'est pas vérifiée et qu'aucun comportement n'a été prévu dans le onFail. Par défaut, on renseigne onFail="MARK_RAN" (= considéré comme exécuté).

 Lorsqu'il faut reporter la création d'un index dans DescriptionIndex.csv d'une branche plus ancienne vers une branche plus récente, il faut conserver l'info concernant la version d'ajout provenant de la branche de maintenance (donc la plus ancienne). On applique donc la règle simple du cherry-pick de la version la plus ancienne vers la version la plus récente, sans rien changer à la ligne d'index créée.

 Dans le cas du requêteur, on peut supprimer un changeSet existant et le remplacer par un autre portant sur la vue que l'on modifie (bien évidemment toujours en changeant le change ID), car chaque fichier sql contient un CREATE OR REPLACE, ce qui garantit qu'aucun problème de table déjà existante ne sera rencontré lors du lancement de sql migrator. Le fichier sql en lien peut ainsi être réutilisé (éventuellement renommé si son nom contient le numéro de l'événement précédent). Le but est de ne pas avoir toute une flopée de changeSet qui font des CREATE OR REPLACE sur une même vue (perte de perf et de lisibilité dans les changeLog). On peut ainsi résumer : pour une vue requêteur, un seul fichier d'upgrade lié à un seul fichier sql, à remplacer à chaque fois qu'une modification doit être faite sur cette vue.  Un exemple ici : https://gerrit.efluid.uem.lan/c/efluid/+/173111

 Cas de la migration Oracle vers PostgreSql 
 Les répertoires /oracle/ et /postgresql/ sont à présent disponibles pour archi, ecore, efluid et efluidNet en develop.
 A présent, uniquement à partir de la version develop, il ne sera plus nécessaire de livrer les drop des tables dans les dossiers ddl/drop : https://eforum.uem.lan/viewtopic.php?f=14&t=3484
 Il n'est donc nécessaire de gérer la distinction oracle/postgresql qu'en develop, cependant afin de simplifier les reports on peut déjà inclure, sur les branches v14 et dans le cas des changelog uniquement, des changeset portant à la fois des dbms="oracle" et dbms="postgresql". Cela n'est pas impactant, postgre n'étant pas géré en v14.
 Les index doivent être à présent créés respectivement dans les dossiers ddl/create/oracle/05_index/index.sql et ddl/create/postgresql/05_index/index.sql
 Le type oracle NUMBER est dans tous les cas mappé vers le type postgre numeric, que ce soit un entier : NUMBER(X) ou un double : NUMBER(X,Y). Sur ce point, une conversion automatique des types est en général réalisée par sql migrator, ce qui fait que bien souvent un seul changeSet est nécessaire (plus besoin d'en faire deux avec un attribut "dbms" différent pour chacun). Plus d'infos ici : https://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/migration_postgre.html#liquibase_spec
 En v14, lors de la création d'un changeSet de suppression de colonne, il convient de faire habituellement un appel à ModificationStructure.rebuildIndexesBeforeDrop('TABLENAME','COLUMNNAME'). Cependant, comme cette fonction est spécifique oracle, il faut ajouter un attribut "dbms=oracle" sur le tag <sql> faisant l'appel à cette fonction. L'appel à cette fonction ne doit plus être fait en develop. Une passe sur les changes de suppression v16 sera faite par l'équipe UL afin de les corriger le cas échéant, dès que des bases v16 seront disponibles.
 Deux changeSet complémentaires (faisant la même chose mais l'un étant dbms="oracle" et l'autre dbms="postgresql") peuvent partager le même change ID : en effet, aucune erreur de doublon ne peut se produire dans ce cas.
 Sous Oracle, une DATE gère les heures, ce qui n'est pas le cas de PostgreSql. Il faut pour cela remplacer DATE par TIMESTAMP, et la fonction de conversion TO_DATE par TO_TIMESTAMP. De même, lors d'une mise à jour de données, la date concernant les acteurs création, modification et suppression devrait être remplacée par current_timestamp : http://wikefluid.efluid.uem.lan/docInstalleur/archi/develop/documentation/architecture/base_de_donnee/FICHE_utilisation_des_dates.html
 Les scripts de migration de type UPDATE doivent être écrits de manière à être compatible Oracle et PostgreSql. Pour cela, il ne faut pas utiliser l'alias pour décrire les colonnes à mettre à jour dans la partie SET. L'alias peut tout de même être défini pour une utilisation dans la clause WHERE ou dans des sous requêtes.
 Dans le cas du requêteur, postgresql ne gère pas la suppression d'une colonne dans un create or replace view, ni l'ajout d'une colonne si celle-ci ne se fait pas en dernier. Pour corriger :
 En upgrade, faire un changeset liquibase contenant un <dropView> placé juste avant le changeset classique contenant notre fichier sql qui fait le create or replace. Ne pas oublier les préconditions. Un exemple ici : https://gerrit.efluid.uem.lan/c/efluid/+/217903
 De manière plus générale, un problème peut intervenir dans le cas où l'on veut modifier le type d'une colonne appartenant à une table qui est référencée par une ou plusieurs vues. Cela n'est effet pas possible avec postgresql. Pour corriger :
 Ajouter un changeSet de drop de la vue problématique avec préconditions, uniquement pour postgresql (dbms="postgresql")
 Faire suivre par le changeSet de modification du type de la colonne
 Enfin, repérer le changeSet qui crée la vue requêteur et lui ajouter l'attribut runAlways="true" afin de forcer la recréation de la vue (les vues étant jouées en dernier, on est sûr que notre colonne aura eu son type modifié avant). Attention, plusieurs changeSet peuvent créer la même vue pour le requêteur, ainsi la bonne pratique est de n'en garder qu'un seul (le dernier). Si toutefois le cas de plusieurs changeSet se présente, ajouter l'attribut runAlways="true" sur le dernier afin de s'assurer d'avoir la dernière version de la vue.
 Attention''' : certaines vues requêteur n'ont pas pu être migrées vers PG. Un fichier dans efluid/sql/database nommé TODO_PG.txt a été créé avec vues en question et les raisons pour lesquelles ces vues n'ont pas pu être migrées. Ce fichier est à traiter par les domaines et à tenir à jour.
 Création d'une BDD temporaire de tests postgresql pour l'archi : https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellecompilation/job/Farchi/job/archi.compile-and-test-PG/
 Création d'une BDD temporaire de tests postgresql  pour ecore : https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/usinelogiciellecompilation/job/Fecore/job/ecore.compile-and-test-PG/
 Plus d'infos sur la migration vers postgresql : http://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/migration_postgre.html#compatiblite
 En particulier, pour les changements v14/v15 concernant les différences d'encodage et le checksum : https://wikefluid.efluid.uem.lan/docInstalleur/sqlMigrator/migration_utf8.html
 Enchaînements réalisés au moment où un nouveau patch est publié sur gerrit :
 Création d'une base from scratch : exécution des ddl create puis dml init
 Exécution des TU/TI
 Drop du schéma et recréation par une copie de la base de référence
 Exécution de l'upgrade SQL (avec context s'il y a)
 Dans le cas PG, création d'un schéma PG par copie d'une base de référence PG
 Exécution de l'upgrade SQL PG<!!!>Category:VSCode
C'est un éditeur de code javascript pour les développeurs.

 Installation VSCode 
Récupérér le zip sur le site https://code.visualstudio.com/download.

Déziper le fichier dans "D:\Programs\VSCode"

 licence 

La licence du logiciel est disponnible ici : 
https://code.visualstudio.com/License

La licence du code source est la :
https://github.com/Microsoft/vscode

 Configuration VSCode 
A mettre dans les users settings : 

// Place your settings in this file to overwrite the default settings
{
    	"files.encoding": "windows1252",
        "editor.tabSize": 2,
        "terminal.integrated.shell.windows" : "C:\\Program Files\\Git\\bin\\bash.exe"
}

 Plugin complémentaires (non inclus dans l'installation standard) 
 Intellisense du code interne 
il faut rajouter la routine suivante en début de fichier de spec :
/// <reference path="../../tomcat/hermes/jsp/arc/commun/js/masques.js" />

 Intellisense de jasmine (au d'autre librairie) 
Prendre le fichier suivant et le glisser dans un dossier visible par VSCode et le mettre en gitignore : https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/jasmine/jasmine.d.ts<!!!>Category:nodejs
C'est un serveur d'application js et fourni également un outils de gestion de projet js : npm (équivalent de maven pour le js).
C'est surtout NPM qui nous intéresse afin de gérer les dépendances et de bénéficier d'outils de transpilation comme Babel etc...

Les maquettes de l'ergonomie phase 3 ont été développé avec ces technologies.

 Licence  
Licence assimilé MIT
https://raw.githubusercontent.com/nodejs/node/master/LICENSE<!!!>Category:ansible

 Guide d'utilisation 
 Guide d'utilisation de Ansible

 Documentation 
 Scripts : http://wikefluid/docInstalleur/documentationScriptsOracleTools/

 Inventaire : /etc/ansible/hosts 
Plus d'info : http://docs.ansible.com/ansible/intro_inventory.html

 Formation 
 Notes prises en formation : http://wikefluid/docInstalleur/formationAnsible/
 Pour contributions la source est ici : etools\ansible\formation\src\site\asciidoc (redéploiement automatique au merge du changeSet Gerrit via http://usinelogicielle/job/FjobsUtilitaires/job/etools.deploy-formation-ansible-documentation/)

 Liens internes 
Bug utilisation module docker_container en 2.2 : http://wikefluid/index.php/Guide_d%27utilisation_de_Ansible#Bug_module_ansible_docker_container_18461

 Liens externes 
  Documentation officielle<!!!>Category:ironjacamar
Category:embedded

 Modification effectuées 
 Permettre une surcharge de RADeployer en tant que deployer custom : https://github.com/ironjacamar/ironjacamar/pull/559
 Intégrée dans 1.3.5.Final
 Permettre une surcharge de DsXmlDeployer en tant que deployer custom : https://github.com/ironjacamar/ironjacamar/pull/576
 Intégrée dans 1.3.5.Final

 Récuperation d'une nouvelle version 
 Modifier la variable ironjacamar.version vers la nouvelle version dans le pom efluid-parent
 Cela récupérera les nouveaux artifacts mais il y a un livrable à générer manuellement et à uploader dans artifactory :
 org.jboss.ironjacamar:jdbc-xa est de type RAR dans maven central, hors nous avons besoin en JAR, il faut donc : 
 le récupérer sous forme de RAR
 placer le RAR dans un nouveau jar et renommer le rar à l'intérieur du jar "jdbc-xa.rar"
 l'uploader dans artifactory sous org.jboss.ironjacamar:jdbc-xa:<version>:jar dans ext-release-local avec un pom généré minimal<!!!>Une extension open source de Git pour le versioning des fichiers volumineux

Git Large File Storage (LFS) remplace des fichiers volumineux tels que des échantillons audio, des vidéos, des jeux de données et des graphiques avec les pointeurs de texte à l'intérieur de Git, tout en stockant le contenu du fichier sur un serveur distant comme Artifactory.

 Installation 
pré-requis : git version >= 1.8.2

téléchargement : https://github.com/github/git-lfs/releases

Disponible dans BMC en version 

 Configuration générale 
 Il faut ajouter des options lfs particulières dans git config, via les commandes suivantes
  git config --system filter.lfs.required true
  git config --system filter.lfs.smudge = "git-lfs smudge -- %f"
  git config --system filter.lfs.clean = "git-lfs clean -- %f"

 Configuration Artifactory 

https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories

 Configuration sur serveur Linux 

 connexion par ssh 
 chantier en cours 
suivi du chantier par l'évènement : 219011
 Actuellement on n'utilise pas le mode "connexion par ssh" mais à terme il faudrait l'implémenter (Cf https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories#GitLFSRepositories-AuthenticatingwithSSH)

 Liens utiles 
 http://blogs.collab.net/git/scm/gerrit-productivity-hack-handling-large-binary-files-with-gerrit-artifactory-and-git-lfs#.Wfrzj1vWzRZ
 https://www.jfrog.com/knowledge-base/how-to-make-git-lfs-work-and-configure-it-with-artifactory-in-5-min/
 Installation  
 côté artifactory suivre la doc : https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories#GitLFSRepositories-AuthenticatingwithSSH
 Verification 
 Pour vérifier si la communication est OK on peut envoyer la commande : 
 ssh <ServerName> -p <port> git-lfs-authenticate artifactory/<reponame> download idLFSArtifactory  
 exemple : NotifLFSArtifactory

 trouble shooting 
2017-10-31 18:26:59,661 [sshd-SshServer[15bcce34]-nio2-thread-2] [WARN ] (o.a.s.s.c.ChannelSession:102) - Error processing channel request exec
java.lang.IllegalStateException: No match found
        at java.util.regex.Matcher.group(Matcher.java:536) ~[na:1.8.0_91]
        at org.artifactory.addon.gitlfs.GitLfsAuthenticateCommand.parseCommandDetails(GitLfsAuthenticateCommand.java:44) ~[artifactory-addon-git-lfs-4.16.1.jar:na]
        at org.artifactory.security.ssh.command.AbstractAuthenticateCommand.<init>(AbstractAuthenticateCommand.java:73) ~[artifactory-core-4.16.1.jar:na]
        at org.artifactory.addon.gitlfs.GitLfsAuthenticateCommand.<init>(GitLfsAuthenticateCommand.java:37) ~[artifactory-addon-git-lfs-4.16.1.jar:na]
        at org.artifactory.addon.gitlfs.GitLfsAddonImpl.createGitLfsCommand(GitLfsAddonImpl.java:44) ~[artifactory-addon-git-lfs-4.16.1.jar:na]
        at org.artifactory.security.ssh.ArtifactoryCommandFactory.createCommand(ArtifactoryCommandFactory.java:56) ~[artifactory-core-4.16.1.jar:na]
        at org.apache.sshd.server.channel.ChannelSession.handleExec(ChannelSession.java:441) ~[sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.server.channel.ChannelSession.handleRequest(ChannelSession.java:311) ~[sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.server.channel.ChannelSession$ChannelSessionRequestHandler.process(ChannelSession.java:602) ~[sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.server.channel.ChannelSession$ChannelSessionRequestHandler.process(ChannelSession.java:600) ~[sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.channel.AbstractChannel.handleRequest(AbstractChannel.java:100) ~[sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.session.AbstractConnectionService.channelRequest(AbstractConnectionService.java:274) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.session.AbstractConnectionService.process(AbstractConnectionService.java:153) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.session.AbstractSession.doHandleMessage(AbstractSession.java:431) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.session.AbstractSession.handleMessage(AbstractSession.java:326) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.session.AbstractSession.decode(AbstractSession.java:780) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.session.AbstractSession.messageReceived(AbstractSession.java:308) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.AbstractSessionIoHandler.messageReceived(AbstractSessionIoHandler.java:54) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.io.nio2.Nio2Session$1.onCompleted(Nio2Session.java:184) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.io.nio2.Nio2Session$1.onCompleted(Nio2Session.java:170) [sshd-core-0.14.0.jar:0.14.0]
        at org.apache.sshd.common.io.nio2.Nio2CompletionHandler$1.run(Nio2CompletionHandler.java:32) [sshd-core-0.14.0.jar:0.14.0]
        at java.security.AccessController.doPrivileged(Native Method) [na:1.8.0_91]
        at org.apache.sshd.common.io.nio2.Nio2CompletionHandler.completed(Nio2CompletionHandler.java:30) [sshd-core-0.14.0.jar:0.14.0]
        at sun.nio.ch.Invoker.invokeUnchecked(Invoker.java:126) [na:1.8.0_91]
        at sun.nio.ch.Invoker$2.run(Invoker.java:218) [na:1.8.0_91]
        at sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:112) [na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91
 Problème dû a une incompatibilité entre git LFS (version < 2.0.0) et artifactory < 5.6.2 : (cf ticket sur jfrog  :https://www.jfrog.com/jira/si/jira.issueviews:issue-html/RTFACT-14672/RTFACT-14672.html )
 pour résoudre le problème il faut monté git LFS et artifactory

 connexion basic via git credentials 
 La connexion se fait donc via une authentification basic à Artifactory qui doit être déclarée à GIT via les "git credentials". Pour se faire il faut : 
 Créer un fichier .git-credentials dans le repertoire HOME de l'utilisateur (exemple : /home/ulouser) contenant l'URL avec login/mot de passe, exemple
 http://<username>:<password>@eartifact.uem.lan
 Ajouter les options suivantes dans git config afin de déclarer que l'on veut utiliser un store git par credentials, et d'indiquer les login pour chaque url : 
 git config --system credential.helper store
 git config --system credential.http://eartifact.uem.lan/.username usinelogicielle

 Configuration poste de dev  

Activer wincred sous windows pour ne pas avoir à saisir à chaque appels les certificats Artifactory. (normalement par défaut à l’installation)
 git config --global credential.helper wincred

 FAQ 

Vous avez une difficulté si quelqu'un commit un fichier binaire et le pousse. Après rebase tout le monde aura ce fichier en stage, il ne peut pas être enlevé ou réinitialisé.
=> https://shuhrat.github.io/programming/git-lfs-tips-and-tricks.html

 1. Turn off Git LFS
 2. Hard reset, git reset --hard JUNK_COMMIT_PREV_SHA
 3. Turn on Git LFS
 4. Push with force, git push origin +dev<!!!>Category:JMS
Category:Tutoriaux

Category:Outil
Category:Intégration Continue
Category:Jenkins

 Checklist à adresser 

Lors d'une demande de prise en compte d'une nouvelle interface basée sur JMS, les opérations et composants  suivants doivent être mis à jour:
 Demande de paramètrage à créer pour suivi avec IT
 jms-config.xml : dans efluid, mettre à jour le fichier jms-config.xml qui centralise la déclaration des interfaces et objets JMS pour l'embedded. 
 documentation installeur : la table définissant les files JMS à créer doit être complétée: scriptInstalleur/src/site/asciidoc/installation_prerequisites.adoc.vm. 
 script wlst de création des files JMS éditeur : la file JMS et la connectionFactory associée doivent être créées dans scriptInstalleurComposant/wlst/deployerWeblogic/deployerWeblogic_app.py
 anonymisation technique : les paramètres de transmission de la nouvelle interface doivent être prise en compte dans l'anonymisation technique: dans le repo efluid, outils/datamaskingpolicy/changeParamTechnique/src/main/resources/*. 
 referentielConfiguration : :
 valorisation de l'anonymisation technique: une fois l'anonymisation technique ajoutée livrée, mise à jour des valeurs attendues: efluid(net)-datamasking/<typeEnv>/<env>/source-.....properties
 ajout si nécessaire dans le domaine de sécurité weblogic d'un user ou groupe et de la mise en oeuvre de la policy sur l'objet en question si non déjà couvert par une politique globale sur un module: efluid/<typeEnv>/<env>/create-realm.py. 

Il faudrait, au niveau UL, se créer un evt chapeau de prise en compte de l'interface, et faire un découpage technique pour chaque étape. Si une étape n'est pas requise, le dire explicitement dans l'evènement pour tracer la raison.<!!!>Category:outil

 Guide utilisateur 
À quoi sert la virtualisation ?

Lancer  plusieurs  systèmes  d’exploitation  en  même  temps.
VirtualBox  vous  permet
d’exécuter plus d’un système d’exploitation en même temps.   De cette façon,  vous pou-
vez  lancer  des  logiciels  écrits  pour  un  système  d’exploitation  dans  un  autre  (par  ex-
emple  un  logiciel  Windows  sur  Linux  ou  Mac)  sans  devoir  redémarrer  pour  l’utiliser.
Comme vous pouvez configurer les types de matériels “virtuels” connectés à chaque sys-
tème d’exploitation, vous pouvez installer un vieux système d’exploitation tel que DOS ou
OS/2 même si le matériel de votre machine physique n’est plus supporté par ce système
d’exploitation.

L'installation d'une application cracké ou de hacking quelconque est interdit.

 Démarrer avec une image ISO 
Pour démarrer une vm avec une image vous pouvez :
Cliquez sur Nouvelle
Choisir le nom de la machine et sont type.
Exemple:
Nom: Debian
Type: Linux
Version: Debian(64bit)
https://drive.google.com/open?id=0B_rPMWF1mpNnTloyc3J2ajdtY0k

Attention: Si vous n'avez pas le 64bit dans le menu déroulant, faite une demande au 6000 pour qu'ils vous activent la "Technologie virtualisation" dans le bios.

Cliquer sur Créer

 Emplacement du fichier vous allez choisir le l'emplacement (attention par défaut l'emplacement est sûr le SSD (C:\Users\<VOUS>\VirtualBox VMs\<NOM_VM>)).
 Taille du fichier vous allez definir la taille du disque dur virtuel.
 Type de fichier de disque dur ici vous pouvez choisir .vdi qui est le format de disque qui est le mieux supporté par Vbox.
 Stockage sur disque dur physique Défini si la taille du disque dur est fix ou dynamique. (dynamique recommandé)

Sélectionnez votre nouvelle machine virtuel et lancé la.

Par défaut la première fois ou vous allez lancé une VM il vous demandera l'iso (Choisissez le disque de démarrage).

Si vous voulez réinstallé un OS sur un VM existante vous pouvez aller dans : Périphériques / lecteur optique.

 Démarrer avec un disque virtuel 
Pour démarrer une VM avec un disque existant il faut faire Nouvelle
Choisir "utiliser un fichier de disque dur virtuel existant"
 Choisir un .vdi ou .vmdk puis cliqué sur créer.

Info: on ne peux pas utiliser le même disque dur sur plusieurs machine. Si vous voulez plusieurs machine identique il faudra les cloner via Vbox

 Astuces 
 Augmenter la taille d'un disque 

 Eteindre la VM.
 Ouvrir un CMD

Vous pouvez récupéré le lien du disque dur virtuel dans: 
 Configuration (de la VM avec le disque trop petit)
 Stockage 
 Contrôleur: SATA
Dans emplacement faire clique droit/copier.

Taper les commandes suivante:
cd d:
cd D:\Programs\Vbox
VBoxManage.exe modifyhd "C:\Users\<VOUS>\VirtualBox VMs\Debian\Debian.vdi" --resize 10000

10000=10Go

 Gestion du network 
 La base 
vignette|gauche|http://wikefluid/images/6/63/Network_vbox.png

NAT: le réseau passe par l'hote pour se connecter.

Bridge: Se connecte directement sur le réseau (plus simple permet de ne pas router les ports dans vbox).

Récupérer son ip et nom de carte réseau sur Debian :
 ifconfig

Récupérer son ip et nom de carte réseau sur centOS et Fedora :
 ip addr

 Configuration sur une VM CentOS et Fedora 
Pour que la machine virtuelle récupère correctement une IP il faut faire deux modifications:

Dans le fichier /etc/sysconfig/network ajouter la ligne :
 NETWORKING=yes

Dans le fichier /etc/sysconfig/network-scripts/ifcfg-<CARD> ajouter les lignes:
DEVICE=<CARD>
BOOTPROTO=dhcp
ONBOOT=yes

 Ajouter un Proxy 
 CentOS 
Editer le fichier yum.conf
 vi /etc/yum.conf

proxy=http://192.168.102.103:8080/
proxy_username=D_NT_UEM\<USER>
proxy_password=<PASSWORD>

 Debian/Ubuntu 
Editer le fichier apt.conf et ajouter la ligne suivante :

 vi /etc/apt/apt.conf

 Acquire::http::Proxy "http://D_NT_UEM\<USER>:<PASSWORD>@192.168.102.103:8080/";

 Fedora 
Editier le fichier dnf.conf

  vi /etc/dnf/dnf.conf

Y ajouter la configuration suivante :

proxy=http://192.168.102.103:8080/
proxy_username=<USER>
proxy_password=<PASSWORD>

 Attention 
Toujours pour Fedora
 Ne pas mettre D_NT_UEM\ dans proxy_username.
 certains caractères spéciaux en fin de mot de passe peuvent empêcher l'identification de fonctionner (le système ne dira rien, mais vous ne pourrez pas utiliser dnf).

 Tricks 

 Ajouter un proxy sur git 
1. Créer et ajouter un fichier pour que git utilise un proxy.

 vi ~/.gitconfig

2. Dans ce fichier ajouter les lignes suivante.
[http]
        proxy = http://D_NT_UEM\<USER>:<PASSWORD>@192.168.102.103:8080

 Ajout d'un proxy sur wget 

Créer le fichier de config dans son home : 

 vi ~/.wgetrc

Ajouter les lignes suivantes:
http_proxy = http://192.168.102.103:8080/
proxy_user = D_NT_UEM\<USER>
proxy_password = <PASSWORD>
use_proxy = on

 Ajouter le proxy pour Docker 

 Debian et CentOS 
Pour pouvoir faire un docker pull d'une image il faut ajouter le proxy dans un fichier.

1. On créer le dossier qui va contenir le fichier.

 mkdir -p /etc/systemd/system/docker.service.d

2. On créer le fichier qui va contenir les informations du proxy.

 vi /etc/systemd/system/docker.service.d/http-proxy.conf

Dans ce fichier on lui donne les informations suivante :
[Service]
Environment="HTTP_PROXY=http://D_NT_UEM\<USER>:<PASSWORD>@192.168.102.103:8080/" "NO_PROXY=localhost,127.0.0.1,wikefluid,eartifact,usinelogicielle,ldsanbld2"

3. Redémarrer les services 

 systemctl daemon-reload && systemctl restart docker

 Fedora 

Pour pouvoir faire un docker pull d'une image il faut ajouter le proxy dans un fichier.

1. On créer le dossier qui va contenir le fichier.

 mkdir -p /etc/systemd/system/docker.service.d

2. On créer le fichier qui va contenir les informations du proxy.

 vi /etc/systemd/system/docker.service.d/http-proxy.conf

Le proxy ne fonctionnera pas sous Fedora si on ajoute D_NT_UEM\ dans l'url du proxy http.  
Dans le fichier on lui donne les informations suivante :

[Service]
Environment="HTTP_PROXY=http://<USER>:<PASSWORD>@192.168.102.103:8080/" "NO_PROXY=localhost,127.0.0.1,wikefluid,eartifact,usinelogicielle,ldsanbld2"

3. Redémarrer les services 

 systemctl daemon-reload && systemctl restart docker

Source : http://download.virtualbox.org/virtualbox/UserManual_fr_FR.pdf<!!!>Category:Outil
Category:Intégration Continue
Category:Jenkins

 Evénements de guichet de paramétrage 

13.3.100 : 183867
13.4.100 : 189613
13.5.100 : 193436
13.6.100 : 201685
13.7.100 : 209294

 Cas de livraison des guichets de paramétrage 

Qu'est ce qu'un zip de remplacement ?
Lors de la montée de version sqlMigrator, il est possible de préciser, avec des options maven, un zip de remplacement. Celui-ci sera utilisé à la place de celui d'origine ramassé pendant la release. Il est alors possible de supprimer, modifier ou d'ajouter des scripts dans ce zip. Vous trouverez ci-dessous la manière de procéder en fonction de la version ramassée.

 RC impaire 

Lors des RC impaires, de nouveaux scripts de paramétrages sont livrés par MBA dans l'événement suivefluid associé au guichet de paramétrage en cours.
Il faut donc mettre en place un nouveau zip de remplacement.
 Télécharger les zip sql-database de la RC pour les trois projets : efluid, migefluid, efluidnet
 Unzipper le zip et se rendre dans "sql/database/parametrage/dml/upgrade/13/changelog/parametrage/changeLog.xml"
 Ajouter un changeSet par fichier de paramétrage sous la forme : 
	

 Le fichier doit être placé dans le dossier "sql/database/parametrage/dml/upgrade/13/changelog/parametrage/guichet/"
 Faire attention à bien ajouter le context=erdf et mettre un id qui prend en compte le numéro de la RC
 Le fichier xml de régionalisation doit être intégré de la manière suivante : 
 Dans le fichier changeLog.xml : 
 Dans le fichier de régionalisation, rajouter les balises de createProcedure
 Zipper l'ensemble des dossiers en ajoutant dans la nomination un "_2" (en fonction du cas, il faudra incrémenter selon si d'autres zip de remplacement ont été livrés)
 Déployer les 3 zips dans artifactory en tant que maven artifact (faire attention à la version qui doit bien compoter le "_2")
 Modifier le fichier dans eroom permettant de connaître le delta livré : http://wperoom3.uem.lan/eRoom/Production/PlanificationIntegrationEfluid/0_1c623b
 Envoyer un mail à l'it, la coordination, l'ul, MBA, HDE, MNA pour prévenir de la mise à disposition du zip

 RC paire 

Lors des RC paires, il n'y a pas de nouvelle livraison des guichets de paramétrage. Il faut cependant mettre à disposition un zip de remplacement incluant le paramétrage de la RC précédente (notamment pour l'IT qui peut faire des montée de version nécessitant ce zip).
Par exemple pour une RC4 : 
 Prendre les zip de remplacement efluid-sql-database-13.7.100.RC3_2, efluidnet-sql-database-13.7.100.RC3_2, migefluid-sql-database-13.7.100.RC3_2
 Prendre les zip sql de la RC actuelle : efluid-sql-database-13.7.100.RC4, efluidnet-sql-database-13.7.100.RC4, migefluid-sql-database-13.7.100.RC4
 Copier le dossier paramétrage des anciens zip de remplacement dans les zip de la RC actuelle. Puis les renommer en ajoutant un "_2".
 Déployer les 3 zips dans artifactory en tant que maven artifact (faire attention à la version qui doit bien compoter le "_2")
 Modifier le fichier dans eroom permettant de connaître le delta livré : http://wperoom3.uem.lan/eRoom/Production/PlanificationIntegrationEfluid/0_1c623b
 Envoyer un mail à l'it, la coordination, l'ul, MBA, HDE, MNA pour prévenir de la mise à disposition du zip

 Release 

Lors d'une release, aucun zip de remplacement n'est mis en place.
Les guichets de paramétrage doivent être inclus dans le code d'efluid, efluidNet et migefluid.

Prérequis : avoir git-lfs sur son poste

 Se placer dans le repo du projet concerné
 Ouvrir le fichier "sql/database/parametrage/dml/upgrade/13/changelog/parametrage/changeLog.xml"
 Ajouter un changeSet par fichier de paramétrage sous la forme : 
	

 Le fichier doit être placé dans le dossier "sql/database/parametrage/dml/upgrade/13/changelog/parametrage/guichet/"
 Faire attention à bien ajouter le context=erdf et mettre un id qui prend en compte le numéro de la RC
 Le fichier xml de régionalisation doit être intégré de la manière suivante : 
 Dans le fichier changeLog.xml : 
 Dans le fichier de régionalisation, rajouter les balises de createProcedure
 Faire un commit et publier sur gerrit. Les fichiers seront traités par git-lfs lors du transfert sur gerrit et seront stockés sur artifactory.

 Procédure pour passer les scripts directement sur la BDD 

 Modifier les scripts de Marc pour faire pointer son spool faire le dossier /tmp/
 Placer les scripts sur le serveur de BDD dans le dossier /tmp/ : LRBDDEDT7
 Se connecter à la machine en ssh avec le compte oracle (passer par root)

Pour efluid : 

sqlplus INT_ERDF_PARAM_FLD_13/Its4bdd_INT_ERDF_PARAM_FLD_13@RPAREDT2 @/tmp/${lenomdetonfichierdeparametrage.sql} INT_ERDF_PARAM_FLD_13_DATA INT_ERDF_PARAM_FLD_13_INDEX INT_ERDF_PARAM_FLD_13_PARAM INT_ERDF_PARAM_FLD_13_BLOB

Pour efluidNet : 
sqlplus INT_ERDF_PARAM_NET_13/Its4bdd_INT_ERDF_PARAM_NET_13@RPAREDT2 @/tmp//${lenomdetonfichierdeparametrage.sql}   INT_ERDF_PARAM_NET_13_DATA INT_ERDF_PARAM_NET_13_INDEX INT_ERDF_PARAM_NET_13_PARAM INT_ERDF_PARAM_NET_13_BLOB

 Ensuite tu récupères les fichiers de logs et tu mets tout dans l’événement 189613.
 Tu redémarres l’application via le job de pilotage :
http://usinedeploiement.uem.lan/job/Finstallation/job/FsuiteEfluid/job/Ferdf/job/Fpilotage/job/suite-efluid.orchestrate-pilotage-int-param-v13/

 Si tout est ok, tu peux check avec cette page ou les liens sont cliquables pour faire les vérifInstallations : Environnements DEV_NB

 – Tu peux envoyer le mail avec le job : http://usinedeploiement.uem.lan/job/Finstallation/job/FsuiteEfluid/job/Ferdf/job/Fdeploiement/job/Fparam/job/suite-efluid.orchestrate-deploy-int-param-v13/
Tu décoches tout et tu coches seulement « envoiMailFin » (en faisant gaffe parce qu’il faut une validation via jenkins pour que le mail parte).<!!!> liens 
 http://asciidoctor.org/docs/asciidoc-syntax-quick-reference/

Category:outil
Category:ASCIIDoc<!!!> Troubleshooting 
 The absolute guide ! 
https://docs.cloudbees.com/docs/cloudbees-jenkins-enterprise/latest/admin-guide/troubleshooting#troubleshooting-build-agent-provisioning

https://docs.cloudbees.com/docs/cloudbees-jenkins-enterprise/latest/admin-guide/troubleshooting#troubleshooting-mesos-marathon

 Les slaves des différentes usines ne prennent plus aucun job, la file des jobs en attente ne cesse de grossir 

Il faut suspecter un souci au niveau du composant responsable de la gestion de l'offre de ressources dans le cluster pour Jenkins : palace
 Contrôler la page sous Mesos permettant de voir l'état de l'offre de ressources physiques dans le cluster de workers Jenkins :
 http://mesos-cje.efluid.uem.lan/#/offers

En nominal, on observe les serveurs Workers disponibles et la quantité de ressources proposées :

1000px|

Si la page est vide, il faut alors redémarrer depuis Marathon le composant palace : 
 http://marathon-cje.efluid.uem.lan/ui/#/apps/%2Fjce%2Fpalace
 Sélectionner jce_palace.xxxxxxxxx
 Cliquer sur Restart dans la barre de boutons au dessus

1000px|

Les serveurs Workers devraient réapparaitre dans la page offers sous Mesos.

 Documentation 

 TROUBLESHOUTING CJE : https://go.cloudbees.com/docs/cloudbees-documentation/admin-cje/troubleshooting/

 COMMANDES UTILES pour admin depuis CJE Workstation : https://support.cloudbees.com/hc/en-us/articles/360001976792-How-to-use-cje-command-line-tools

 Guide d'administration du cluster CJE: https://go.cloudbees.com/docs/cloudbees-documentation/pse-admin-guide/

 Génération d'un Bundle dans le cadre d'un ticket auprès du support Cloudbees: https://go.beescloud.com/docs/cloudbees-documentation/admin-cje/reference/cli/#generating-support-bundle

 Migration d'un Master sans upgrade CJE : https://support.cloudbees.com/hc/en-us/articles/115000078131-How-to-configure-last-CloudBees-Jenkins-docker-images-manually-without-upgrade

 https://go.cloudbees.com/docs/cloudbees-documentation/admin-cje/reference/cli/

 https://go.beescloud.com/docs/cloudbees-documentation/admin-cje/operating/

 https://www.cloudbees.com/products/cloudbees-jenkins-platform/team-edition/features/role-based-access-control-plugin

 [cjoc] 1. https://support.cloudbees.com/hc/en-us/articles/223406287-I-changed-the-security-realm-and-since-then-bees-pse-operations-are-failing

 [cjoc] 2. https://support.cloudbees.com/hc/en-us/articles/229298168-How-to-disable-CJOC-authorization-in-PSE

 https://jenkins.io/doc/book/pipeline/shared-libraries/#using-third-party-libraries

 Scripts groovy pour diverses tâches d'admin: https://github.com/cloudbees/jenkins-scripts

 Configuration SSL: https://support.cloudbees.com/hc/en-us/articles/222098288-Set-up-SSL-on-a-CJOC-environment-with-a-self-sign-SSL-certificate

 Accès à Docker registry (Using private Docker registries): https://go.cloudbees.com/docs/cloudbees-documentation/admin-cje/cd-as-a-service/

 Libérer une VM master worker des managed masters qui tournent dessus: https://support.cloudbees.com/hc/en-us/articles/115000738111?input_string=cje%3A+managed+masters+dispatching+among+master+workers

 LTS upgrade Guide : https://jenkins.io/doc/upgrade-guide/

 Préparation des serveurs 

Sur les serveurs hébergeant les Masters Jenkins, et les Slaves, configurer les précos de Cloudbees relatives à NFS :

  /etc/nfsmount.conf (en cours de validation)
 Defaultvers=3
 /etc/sysconfig/nfs 
 RPCNFSDCOUNT=16 (8 par défaut)

 /etc/sysctl.d/30-nfs.conf : à créer avec les paramètres suivants :
 sunrpc.tcp_slot_table_entries = 128   ( => was set to 2 in our environment.)
 sunrpc.tcp_max_slot_table_entries = 65536
 net.core.rmem_default = 262144
 net.core.rmem_max = 16777216
 net.core.wmem_default = 262144
 net.core.wmem_max = 16777216
 net.ipv4.tcp_rmem = 4096 262144 16777216
 net.ipv4.tcp_wmem = 4096 262144 16777216
 net.ipv4.tcp_window_scaling = 1
 net.ipv4.tcp_sack = 0
 net.ipv4.ip_local_port_range = 1024 65000

Test en cours 12/12/2018 sur lpcjemtredt1 à 3, pour ne plus swapper alors qu'il reste de la mémoire

https://success.docker.com/article/node-using-swap-memory-instead-of-host-memory

 /etc/sysctl.conf
 vm.swappiness=0
 vm.overcommit_memory=1

Pour prise en compte immédiate
 sysctl vm.swappiness=0
 sysctl vm.overcommit_memory=1

Pour vérifier 
 sysctl -a | grep -E 'vm.swappiness|vm.overcommit_memory'

 Platforms identification 

 CJE Workstation 

Controller Workstation pour environnement de Prod et de Test: lpcjewstedt1 (192.168.119.81)

Se connecter et passer ulouser

Pour travailler sur le CJE de prod : 
 setProd

Pour travailler sur le CJE de test tstcje1 : 
 setTst

Quelques fonctions/alias utiles (des fonctions sont dispos dans ~/.libUtilsCJE) :
  help

 cje Prod 

 CJE : http://cje.efluid.uem.lan
 Mesos : http://mesos-cje.efluid.uem.lan/#/
 Marathon : http://marathon-cje.efluid.uem.lan/

CJE project: sur lpcjewstedt1 (ulouser) 
 => /data/EDT/files/cje_prod_1_project/

Secrets: ${PROJECT}/.dna/secrets

DNS : http://wikefluid/index.php/Dns_efluid

Controller Workstation: lpcjewstedt1 (192.168.119.81)
Load balancer 1: lpcjelbaedt1 (192.168.119.61) 
Load balancer 2: lpcjelbaedt2 (192.168.119.80) 

    cje_prod_1_project  Core  RAM Controller-1  lpcjecntedt1 (192.168.106.66)    Controller-2  lpcjecntedt2 (192.168.106.67)   Controller-3  lpcjecntedt3 (192.168.106.106)  Worker-1  lpcjemtredt1 (192.168.106.233) (Master jenkins) 6  35 Go  Worker-2  lpcjemtredt2 (192.168.106.234) (Master jenkins) 6   35 Go  Worker-3  lpcjemtredt3 (192.168.106.235) (Master jenkins) 6   35 Go  Worker-4  lpdrogonedt1 (192.168.119.78) (Agent Worker)   96  188 Go Worker-5  lpviseriedt1 (192.168.119.79) (Agent Worker) 96   188 Go  Worker-6  lpcjeelkedt1 (192.168.119.58)    Worker-7  lpcjeelkedt2 (192.168.119.59)   Worker-8  lpcjeelkedt3 (192.168.119.60)   Worker-9  NONE (mauvaise manip)   Worker-10  lprhaegaedt1 (192.168.119.88)  (Agent Worker) 64  188 Go  Worker-11  lptyrionedt1 (192.168.118.8)  (Agent Worker)28  188 Go  Worker-12  lpcjewkredt1 (192.168.119.115)(Agent Worker)  176   187 Go  Worker-13  lpcjewkredt2 (192.168.119.116)(Agent Worker)  176   187 Go Worker-14  lpcjewkredt3 (192.168.119.117)  (Agent Worker)176  187 Go  Worker-15  lpcjewkredt4 (192.168.119.118)  (Agent jenkins) 176  187 Go  Worker-16  lprededt2 (192.168.106.101)  (Master Worker) 32  190 Go 

 cje tst1 

 CJE : http://tstcje1.efluid.uem.lan
 Mesos : http://mesos-tstcje1.efluid.uem.lan/#/
 Marathon : http://marathon-tstcje1.efluid.uem.lan/

Se reporter à http://wikefluid/index.php/Suivi_des_stages_Process_de_Fabrication pour détails sur mise en oeuvre pltf de tests.

Controller Workstation: lpcjewstedt1 (192.168.119.81)

Load balancer 1: lpcjelbaedt2 (nginx) (192.168.119.80) 

     tstcje1_project Controller-1   lpsrvulo1 (192.168.106.230) Worker-1   lrcjemtredt1 (192.168.119.82) Worker-2   lrcjemtredt2 (192.168.119.83) Worker-3  DELETED - lprhaegaedt1 (192.168.119.88)  (Agent Worker) Worker-4  DELETED - lpsrvulo2 (192.168.106.231) (ELK) Worker-5  DELETED - lrcjewkredt1 (192.168.119.98)  (Agent Worker) Worker-6  DELETED - lrcjewkredt2 (192.168.119.99)  (Agent Worker) Worker-7  DELETED - lpcjeredwkr1 (192.168.106.139)  (Agent Worker)

DNS primaire: lrcjemtredt1 (192.168.119.82) (bind, webmin: https://lrcjemtredt1:10000/, mdp dans keypass PerfUl)
 docker ps: 
 85eff8efb2ca        sameersbn/bind:9.9.5-20170129   "/sbin/entrypoint...."   4 weeks ago         Up 4 weeks          0.0.0.0:53->53/tcp, 0.0.0.0:10000->10000/tcp, 0.0.0.0:53->53/udp   cjetest_primary_dns

DNS secondaire: lrcjemtredt2

MESOS : mesos-tstcje1.efluid.uem.lan/
MARATHON : marathon-tstcje1.efluid.uem.lan/

 Pilotage des noeuds 

  usage: dna [-h] COMMAND ...

  optional arguments:
    -h, --help       show this help message and exit

  COMMAND
    init-project   Initialize a project directory
    create         Create a new server
    status         Show the status of the project or servers
    servers        List servers managed by DNA
    start          Start a new server
    stop           Stop a running server
    init           Initialize a server
    reinit         Synonym for init command (backward compatibility)
    bind           Bind a server to a server instance
    unbind         Unbind a server from a server instance
    run            Run a script on one or more servers
    start-job      Start a job on a server
    connect        Connect to a server
    convert        Convert an old-style DNA document to a new server
    init-env       Initialize a DNA environment
    render         Render a script for a server
    shell          Start a DNA shell
    encrypt        Encrypt a file
    decrypt        Decrypt a file
    reencrypt      Reencrypt a file
    render-template
                   Render a template
    volumes-analyze
                   Analyze volume(s)
    volumes-attach
                   Attach existing volume(s) to the instance
    volumes-detach
                   Detach volume(s) attached to the instance
    ansible        Invoke ansible against the server

 Ouvrir un ticket au support Cloudbees 
Pour ouvrir 1 ticket au support Cloudbees : https://support.cloudbees.com/hc/en-us

Il faut systématiquement joindre un Bundle Support à l'ouverture du ticket.
Si problème sur 1 master particulier, alors générer le bundle à partir du Master en cause. La page est disponible depuis la page principale du master, menu à gauche : "Support".
 ex : http://cje.efluid.uem.lan/usinelogiciellecompilation/support/

Si problème plus vaste, alors joindre un pse support bundle en suivant la procédure ci-dessous.

 Obtenir un threadDump 

Suivre cette procédure : https://support.cloudbees.com/hc/en-us/articles/115001626467-Extract-a-Heap-Thread-Dump-of-a-tenant-on-PSE

 Generate a CloudBees Jenkins Enterprise Support Bundle 
A CloudBees Support Engineer may ask you to generate a CloudBees Jenkins Enterprise Support Bundle. This is a common step in triaging problems with your CloudBees Jenkins Enterprise instance. This 
bundle will include information that will be helpful in troubleshooting problems. To do so, run the following command:
 cje prepare pse-support
 pse-support is staged - review pse-support.config and edit as needed - then
 run `cje apply` to perform the operation.

Edit the pse-support.config file before executing the operation to choose what packages to include in the bundle.
 Note By default, this operation will generate a bundle with all of the options included. If you wish to exclude a package from the bundle, indicate 'no' instead of 'yes'.

 [tiger]
  ## Include the Support Core Plugin Bundle.
  cjoc_support_bundle = yes
  ## Include logs of this cluster's controllers and workers.
  cluster_task_logs = yes
  ## Include PSE Workspace Specific Files
  pse_workspace = yes

Then apply the operation with cje apply.

The Support Bundle archive file will be saved to the current working directory to a time-stamped file called cloudbees-pse-support-YYYY-MM-DD-HH-MM-SS.tgz.

You can then submit this Support Bundle archive file to a CloudBees Support Engineer.

NOTE
 If your archive is larger than 20Mb please use this new service to send it to us: https://uploads.cloudbees.com/#/login. This service works best in Chrome or Firefox.

 Installation procédure 

Suivre les prérequis demandés là: http://wperoom3.uem.lan/eRoom/Prod6/ProcessFabricationEfluid/0_9e8a

Le jour J:
 Se connecter ulouser sur la cjeWorkstation.

 Vérifier que les clés publiques générées sur le poste cjeWorkstation sont bien déployées sur tous les serveurs du cluster (depuis le répertoire .ssh):
  [ulouser@lpcjewstedt1 .ssh]$ ssh -i cje-ssh-key ulouser@lpcjecntedt1 id
  uid=500(ulouser) gid=988(docker) groupes=988(docker),5000(fldgrp)
  [ulouser@lpcjewstedt1 .ssh]$ ssh -i cje-ssh-key ulouser@lpcjecntedt2 id
  uid=500(ulouser) gid=988(docker) groupes=988(docker),5000(fldgrp)

 Se placer dans le répertoire cd pse_1.7.0/

  [ulouser@lpcjewstedt1 pse_1.7.0]$ cje version
  PSE Project: 5
  PSE Release: 20
  CloudBees PSE: 1.7.0
  AWS AMI Version: 1.7.0	
  Build: 140
  Tiger Storage: cloudbees/pse-castle:1.6.3
  Tiger CJOC: cloudbees/cje-oc:2.46.3.2
  Tiger CJE: cloudbees/cje-mm:2.46.3.2
  Tiger Search: cloudbees/pse-elasticsearch:1.3.1
  Tiger Router: cloudbees/pse-router:1.6.3
  Tiger Logstash: cloudbees/pse-logstash:1.6.0
  PSE SSH Gateway: cloudbees/pse-sshgateway:1.3.1
  Tiger Scheduler: cloudbees/pse-palace:1.6.4
  Mesos: 0.28.2\*
  zookeeper: 3.4.6\*
  Marathon: 0.15.3\*
  Docker: 1.13.1\*
  Topbeat: 1.1.0\*
  Pip: 1.5.4\*
  Terraform: 0.9.4-cloudbees (a5bb70b48113618e056d4039b4da5db36f50c2c7)

 En tant que root: mkdir /data/EDT/files/cje-project et chown -R ulouser:docker /data/EDT/files

 En tant que ulouser, reprendre: 

  [ulouser@lpcjewstedt1 file]$ export PROJECT=/data/EDT/file/cje_project
  [ulouser@lpcjewstedt1 file]$ cje init-project $PROJECT anywhere
  Using anywhere template
  Project created in /data/EDT/file/cje_project
  Please run
  $ cd /data/EDT/file/cje_project
  $ cje prepare cluster-init
  to start creating the cluster.

  [ulouser@lpcjewstedt1 file]$  cd /data/EDT/file/cje_project

  [ulouser@lpcjewstedt1 cje_project]$ cje prepare cluster-init
  cluster-init is staged - review cluster-init.config and cluster-init.secrets and edit as needed - then run 'cje apply' to perform the operation.

  [ulouser@lpcjewstedt1 cje_project]$ vim cluster-init.config
La configuration sauvegardée se trouve sous : /data/EDT/files/cje_prod_1_project/operations/20170628T082234Z-cluster-init/config

  [ulouser@lpcjewstedt1 cje_project]$ vim cluster-init.secrets
La configuration sauvegardée se trouve sous : /data/EDT/files/cje_prod_1_project/operations/20170628T082234Z-cluster-init/secrets

  [ulouser@lpcjewstedt1 cje_project]$ cje verify
  cluster-init is ready to apply

  [ulouser@lpcjewstedt1 cje_project]$ cje apply
  Setup 3 controllers
  Setup 8 workers
  Updating project configuration
  Updating project secrets
  Initializing DNA project at /data/EDT/file/cje_project/.dna
  Extending initialization to /home/ulouser/pse_1.7.0/share/setup-templates/core
  Creating credentials
  unable to write 'random state'
  unable to write 'random state'
  ...

 Si problème, relancer individuellement: 
  dna init worker-1
  dna init controller-1

 Une fois tout OK:
  cje run display-outputs
  
 Start a browser

 Sur la CJEWorkstation:
  cje run echo-secrets cjoc_username
  cje run echo-secrets cjoc_password

Utiliser ces infos pour se connecter admin à Jenkins

 Login to the browser

 Upgrade procédure (ex: de 1.7.0 vers 1.7.1) 

 Préparation 

Demande de support à migration : https://support.cloudbees.com/hc/en-us/articles/115001919212?q=assisted%20upgrade

En cas de mise à jour recommandée:
600px|

 
https://support.cloudbees.com/hc/en-us/articles/115000112392-How-to-upgrade-CJE

 Avant de lancer l'upgrade, vérifier que:
 Le cluster elasticSearch est (script checkElkStatus.sh depuis lpcjewstedt1 -> connecté ulosuer -> setProd ou setTst):
 Green sur Prod
 Yellow sur Test
Si pas dans l'état attendu, lancer le script fixEsShards.sh puis relancer le checkElkStatus.sh.
 (plus utile) /nfs/cjoc appartient bien à ulouser:docker, surtout pas à jenkins4docker. Si ce n'est pas le cas, réaffecter le bon user et groupe.
 Se placer dans le répertoire du projet à migrer sur lpcjewstedt1 (/data/EDT/files/cje_prod_1_project ou /data/EDT/files/tstcje1_project) et faire un git difftools de manière à s'assurer qu'il n'y a pas de modification non souhaitée sur la configuration de référence avant de lancer l'upgrade. Ex : s'assurer que dans les fichiers .dna/servers/worker-XX/dna.config, dans la section [castle], le paramètre user_name est bien à 1000 (user jenkins) et pas à 500 (user ulouser). 

 Télécharger la dernière version pse_x.y.z requise sur le site: https://downloads.cloudbees.com/pse/latest/ (depuis lpcjewstedt1, ulouser)
 export https_proxy=http://lpsrvpxy.uem.lan:8080/
 export http_proxy=http://lpsrvpxy.uem.lan:8080/
 wget https://downloads.cloudbees.com/pse/latest/pse_1.X.Y_linux_amd64.tar.gz

 Extraire l'archive dans le home de ulouser: 
  $ cd ~
  $ tar zxvf pse_1.7.1_linux_amd64.tar.gz
  $ export PATH=~/pse_1.7.1/bin:$PATH
  $ Vérifier que le PATH est OK : which cje => doit résoudre avec le nouveau répertoire extrait.

 Désactiver dans le PSE désarchivé la mise à jour docker : 
 Editer le fichier ./share/setup-templates/core/templates/tiger/configure-docker et supprimer le contenu de la fonction docker-config(). Remplacer par l'instruction 
  echo "CUSTO efluidsas : NOP"

 Redéfinir la valeur 8 pour l'algorithme de pack des masters dans Marathon : 
 Editer le fichier ./share/setup-templates/core/templates/palace/marathon.json et dans la section env, ajouter entre 2 variable existantes (cf https://docs.cloudbees.com/docs/cloudbees-jenkins-enterprise/latest/references/#_binpack_algorithm) :
  "CONSTRAINTS": "8",

Note : fait sur pse_1.11.22 , pse_1.11.26 et pse_1.11.29

 Migration 

 Se placer dans le projet à migrer et positionner la variable PROJECT: 
  $ cd /data/EDT/files/cje_prod_1_project
  * export PROJECT=$PWD

 Préparer l'upgrade: 
  $ cje upgrade-project

Note: la plupart du temps, cette commande semble sans effet (upToDate). Poursuivre avec l'upgrade.

(plus utile) Mettre à jour le fichier .dna/secrets afin d'y mettre le login et mot de passe d'un compte administrateur pouvant être utilisé le temps de l'upgrade (propriétés cjoc_username et cjoc_password).

 Lancer l'upgrade: 
  $ cje upgrade

 Avant de migrer les MASTERs, vérifier que:
 sur le cjoc, dans la configuration système globale, partie paramètres avancés de la section "Mesos Master Provisioning" : le champ "Docker Container User" vaut 1000 et pas 500. Si valeur 500, le changer avant de migrer les Master, sinon, ils ne pourront pas démarrer en raison de "Permission denied" sur /nfs/<master>/jenkins_home/configure-jenkins.groovy.d.

 Migrer 1 managed master à la main afin de s'assurer que la montée de version se passe bien sur 1 des masters au choix, via l'interface du CJOC:
 ** manage -> stop du master, 
 ** configure => mettre à jour la version d'image. 
 ** save => redémarre automatiquement le master.
 
 Une fois l'upgrade du managed master de test terminé, mettre à jour depuis l'interface CJOC tous les autres masters Jenkins en utilisant le jobs suivant (version cible à configurer dans le job par édition):
 ** http://cje.efluid.uem.lan/cjoc/view/All/job/Fadmin/job/managed-masters.upgrade-images/configure

NOTE: éviter, lors de la mise à jour de plugin, d'avoir une version non supportée par le CAP (Cloudbees Assurance Program).

 Editer le fichier .bashrc du compte ulouser pour remettre à jour le PATH pour cet envirnnement.

 Note: 
 Le répertoire /home/ulouser/cjeConfigurations est un clone du repo Git cjeConfigurations permettant l'archivage des confs au fil des montées de version.
 Le répertoire /data/EDT/files/cje_prod_1_project est un lien symbolique qui pointe le référentiel: /home/ulouser/cjeConfiguration/cje_prod_1_project.
 De la même manière, le répertoire /data/EDT/files/tstcje1_project est un lien symbolique qui pointe le référentiel: /home/ulouser/cjeConfiguration/tstcje1_project.
 Avant une migration, s'assurer que le clone est synchroniser avec le remote.
 Après migration, publier les modification dans gerrit pour archivage: git commit -a -m "98875: Montée de version sur [Prod|Test] en version pse_1.x.0";git pub

 Contrôles post-migration  

Après migration du CJOC, points à vérifier :
 http://cje.efluid.uem.lan/cjoc/credentials/store/system/domain/_/credential/mesos/ : remettre le scope en Global si repassé en Système.

Les jobs de validation à lancer pour contrôler une nouvelle version :
 Fadmin (backups):
 backup jobs
 backup system
 cjoc  agent templates backup
 cjoc backup system

 usine Validation Jenkins :
 jenkinsConfigurations validation 
 sharedLibrary compilation
 sharedLibrary  validation gerrit

 usine Recette :
 validation plugins: les jobs du dossier http://cje.efluid.uem.lan/usinerecette/job/FnonRegressionTests/job/FdslExample/

 Patchs sur PSE 

Problème avec les repos Docker arrêtés : 

Docker has restored the repositories that were unavailable causing CloudBees Jenkins Enterprise (CJE) 1.x controllers and workers to not initialize earlier today.  However, Docker will be permanently shutting down repositories on which CJE1.X relied on March 31, 2020 

In order to avoid controller and worker failures, you must either:
 upgrade to version 1.11.27  OR 
 patch controllers and workers on older versions (The patch can only be applied to versions 1.11.15 and higher.)

For more details, please see the CloudBees Knowledge Base article: https://support.cloudbees.com/hc/en-us/articles/360040577951-Workers-do-not-complete-a-restart-operation.

If left in its current state, after Docker shuts down these repositories on March 31, 2020 CJE 1.X controllers and workers will not initialize. To clarify, these are the virtual machines that are created and managed by the ‘cje’ command line tool, not Operations Center, Managed Masters, nor build agents.

Patch récupéré : 
https://cloudbees-jenkins-scripts.s3.amazonaws.com/cplt2-6275/1.11.26.patch

Appliqué manuellement sur :
 pse_1.11.22
 pse_1.11.26

 Tests de migration d'un Master sur une nouvelle version CJE 

Faire la migration CJE souhaitée sur TSTCJE1

Importer un Master de Prod sur l'environnement de test en suivant la procédure décrite (connecté jenkins4docker sur lpcjewstedt1): http://wikefluid/index.php/Suivi_des_stages_Process_de_Fabrication#Import_Master_de_Prod_vers_Test

Exemple : ./launchCreateManagedMaster.sh envRecette.properties jenkinsDEV:f4c519dda39d9ef1ca3de610cb06a5af47

Dans le fichier FileDesc à passer en paramètre, bien mettre la version de Master de prod. C'est ce Master, une fois importé, qui sera monté en version cible de l'environnement de test.

Une fois le Master importé, le migrer en suivant la procédure de montée de version des Masters.

Et maintenant, y a plus qu'à tester !!!

 Exploitation 

 Modifier la durée de rétention des conteneurs de build sur les workers Mesos slaves 

 Pour ce besoin, l'option à configurer au niveau mesos-slave est docker_remove_delay. Sa valeur par défaut est 6hrs. Pour toutes les options possibles, se reporter à http://mesos.apache.org/documentation/latest/configuration/agent/

 Se connecter sur chacun des serveurs mesos-slaves et passer root : lpcjewkredtX, lpdrogonedt1, lpviseriedt1, lptyrionedt1, lprhaegaedt1

 Se placer dans le répertoire /etc/mesos-slave. Il y a là 1 fichier par option configurable pour un agent mesos, le fichier contenant la valeur du paramètre.

 Lancer la commande :
 echo "1hrs" > docker_remove_delay
ou 
 echo "30mins" > docker_remove_delay
 
 Redémarrer le service mesos-slave : 
 systemctl restart mesos-slave

 Contrôler que le paramètre est bien actif :
 ps -ef | grep delay    => le paramètre doit apparaitre sur la ligne correspondant au process mesos-slave

 Intégrer un nouveau worker 

 Se connecter ulouser sur lpcjewstedt1

 Se placer dans le bon projet : setTst pour TSTCJE1 et setProd pour Prod

 cje prepare worker-add
worker-add is staged - review worker-add.config and edit as needed - then run 'cje apply' to perform the operation.
%

 vim worker-add.config
 Définir worker-x pour identifier le ou les workers à intégrer et y configurer les paramètres demandés:
  count = 1
  workload_type = build
  addresses = <ip address>
  #ssh_identity_file = default
  #ssh_user = ulouser (jenkins4docker pour l'environnement de test)

 cje apply

 cje status pour contrôle

 Reinitialisation d'un worker 

Cette procédure permet de remettre complètement à jour 1 worker, si on détecte une ano ou 1 service qui a disparu (ex : logstash conteneur plus présent).
Attention : cette procédure redémarre docker => éviter de la lancer si activité dessus

 dna init <worker-X>

 Redémarrage d'un worker 

 Se connecter ulouser sur lpcjewstedt1

 Se placer dans le bon projet : setTst pour TSTCJE1 et setProd pour Prod

 dna stop <worker-X>

 dna start <worker-X>

 Redémarrage logstash sur worker 

Le conteneur logstash est démarré par 1 service => si problème, passer par les commandes systemctl

 systemctl status cloudbees-logstash

 systemctl restart cloudbees-logstash

 Arrêt d'un worker temporairement 

La procédure permettant de désactiver un worker du cluster n'existe plus (cje prepare worker-disable)
Il faut désormais passer par des commandes transmises directement au master Mesos afin de sortir un noeud du cluster (https://mesos.apache.org/documentation/latest/maintenance/).

Pour vérifier si des opérations de maintenance sont déjà en cours sur le Cluster, lancer l'URL suivante : 
  
  http://lpcjecntedt1:5050/maintenance/status

Si pas d'opération en cours, retourne {}
Si des opérations sont en cours, on a une info du genre :
  {"down_machines":[{"hostname":"192.168.118.8","ip":"192.168.118.8"}]}

Cela indique que le serveur d'IP indiquée est down au niveau Cluster, donc sorti.

Les étapes à suivre pour faire une maintenance sur 1 serveur du cluster sont :

 Se connecter sur lpcjewstedt1, compte ulouser, et se placer dans le bon contexte prod ou test : setProd ou setTst

 Préparer le serveur à sortir du cluster : drainage. Pour cela, utiliser le script ./prepare.sh <nomServer>  (ex : ./prepare.sh lprhaegaedt1). Il n'y a rien de spécial à attendre, on peut passer à l'étape suiante (observations : cette étape semble ne servir à rien).
 Sortir le serveur du cluster : mis en mode down. Plus aucun job ne sera provisionné sur ce serveur, si des conteneurs tournent dessus, ils seront stoppés, et reventilés ailleurs par Marathon. Pour cela, utiliser le script ./down.sh <nomServer>  (ex : ./down.sh lprhaegaedt1)
 Une fois la maintenance terminée :
 Remettre le serveur en état : dna init <worker-??> (vérifier le nom du worker via la commande lswkr). Cette étape permet notammet de relancer logstash et redémarrer docker.
  Remettre le serveur dans le cluster : ./up.sh <nomServer>  (ex : ./up.sh lprhaegaedt1)

 Suppression d'un worker 

 Se connecter sur lpcjewstedt1, compte ulouser, et se placer dans le bon contexte prod ou test : setProd ou setTst

 cje prepare worker-remove
> NE MARCHE PAS DU TOUT >
https://support.cloudbees.com/hc/en-us/articles/360015997052-Recycling-CJE-1-Anywhere-servers

 Sortir le serveur du cluster maraton : prepare/down
 Stopper tous les servies :
   systemctl disable mesos-slave
   systemctl stop mesos-slave
   systemctl disable cloudbees-logstash
   systemctl stop cloudbees-logstash
   docker rm -v $(docker ps -a -q)
   docker rmi -f $(docker images -qa)

 Stopper docker : 
  systemctl disable docker
  systemctl stop docker

Supprimer les fichiers :
 /etc/.*-installed

Supprimer mesosphere & co :
 yum remove mesosphere-zookeeper-3.4.6-0.1.20141204175332.centos7.x86_64
 yum remove mesos-0.28.2-2.0.27.centos701406.x86_64 mesosphere-el-repo-7-3.noarch

Supprimer les répertoires et tous les contenus:
 rm -rf /etc/zookeeper /etc/jce /etc/mesos* /var/log/router /var/log/mesos /var/lib/mesos*

 Faire un up.sh de fin de maintenance
 Contrôler que l'agent a bien été supprimé de la liste : 
 cje run support-mesos ms-slaves | grep hostname

Voir commande cje run support-mesos pour plus de détails

Supprimer le worker côté workstation
 cje prepare worker-remove

 Supprimer les montages NFS :
 umount /nfs /nfstst1
 cat /etc/fstab et supprimer : 
  lpnfssan1-edt:/NFS_SHARE_UL_DTA2_A04 /nfstst1 nfs defaults,soft,intr 0 0
  lpnfssan1-edt:/NFS_SHARE_UL_DTA_A04 /nfs nfs defaults,soft,intr 0 0

Pour purger les "DELETED" workers : 
 https://support.cloudbees.com/hc/en-us/articles/115000634632-How-to-purge-deleted-workers-

 Redémarrage d'un worker temporairement 

 Se connecter ulouser sur lpcjewstedt1

 Se placer dans le bon projet : setTst pour TSTCJE1 et setProd pour Prod

 cje prepare worker-enable

 vim worker-enable.config
 Définir worker-x pour identifier le ou les workers à remettre en service.

 cje apply

 dna start <worker-X>

 cje status pour contrôle

 Réinitialiser le LV thinpool docker sur un worker (OBSOLETE avec overlay2) 

Cette procédure est à suivre en cas de survenue trop fréquente d'alertes Nagios indiquant une saturation du thinpool docker (cf lvs)

 Sortir le noeud en question du cluster CJE
 Se connecter ulouser sur lpcjewstedt1
 Se placer dans le bon projet : setTst pour TSTCJE1 et setProd pour Prod
 cje prepare worker-disable
 worker-disable is staged - review worker-disable.config and edit as needed - then run 'cje apply' to perform the operation.
 %
 vim worker-disable.config
 Définir worker-x pour identifier le serveur à sortir 
 cje apply
  Supprimer le LV sur le serveur cible
 Se connecter sur le serveur en question et arrêter docker
 sudo systemctl stop docker 
 Supprimer le VG (ca détruira le LV aussi)
 sudo vgremove docker
 => répondre yes aux deux questions
 sudo rm -rf /var/lib/docker/*
  Recréer le LV sur le serveur cible grâce au playbook ansible
 Depuis un serveur Linux du repo oracleTools, se placer dans le répertoire ansible et jouer le playbook dockerservers.yml
 cd ansible
 ansible-playbook -i inventory/common --ask-vault-pass -e @inventory/ul/vault.yml -e "variable_host=<serveurCible>" dockerservers.yml

 Réintégrer le noeud dans le cluster CJE
 Se connecter ulouser sur lpcjewstedt1

 Se placer dans le bon projet : setTst pour TSTCJE1 et setProd pour Prod

 cje prepare worker-enable
worker-enable is staged - review worker-enable.config and edit as needed - then run 'cje apply' to perform the operation.
%
 vim worker-enable.config
 Définir worker-x pour identifier le serveur à sortir 

 cje apply

 dna init worker-x (permet de remettre le worker à niveau après ré-intégration)

 Monitoring 

Evaluation en cours : https://blog.eleven-labs.com/fr/monitorer-ses-containers-docker/

 cadvisor 
Attention, ne pas lancer cadvisor en mode docker, car lock les volumes et empêche les conteneurs monitorés d'être supprimés => DEAD

Pour les serveurs CJE : cadvisor binaire Go présent sur /nfs/cje-share-dir/tools

Démarrage de cadvisor sur chaque serveur à monitorer :
   sudo /nfs/cje-share-dir/tools/cadvisor -port 8085&

Déjà en place :

 http://lpcjemtredt1:8085/containers
 http://lpcjemtredt2:8085/containers
 http://lpcjemtredt3:8085/containers
  http://lpcjewkredt1:8085/containers
  http://lpcjewkredt2:8085/containers
  http://lpcjewkredt3:8085/containers
  http://lpcjewkredt4:8085/containers
  http://lpviseriedt1:8085/containers
  http://lpdrogonedt1:8085/containers
  http://lprhaegaedt1:8085/containers
  http://lptyrionedt1:8085/containers

Et pour les env NB/INT :
  http://noobaleine:8085/containers/
 http://ironwhale:8085/containers/
  http://lpdocedt8:8085/containers/

 prometheus 

https://prometheus.io/docs/prometheus/latest/installation/

A installer...

 Suivi des versions 

 CJE workstation  Version CJOC  Version Master  cje_prod_1_project  tstcje1_project pse_1.7.0  2.46.3.2-rolling  cloudbees/cje-mm:2.46.3.2  X n/a pse_1.7.1  2.60.1.1-rolling  cloudbees/cje-mm:2.60.1.1  X X pse_1.8.0  2.60.2.2-rolling  cloudbees/cje-mm:2.60.2.2  X X pse_1.9.0  2.60.3.1-rolling  cloudbees/cje-mm:2.60.3.1  X X pse_1.9.1  2.73.1.2-rolling  cloudbees/cje-mm:2.73.1.2  X X pse_1.11.16  2.164.1.2-rolling  cloudbees/cje-mm:2.164.1.2  X X pse_1.11.22  2.176.4.3-rolling  cloudbees/cje-mm:2.176.4.3  X X pse_1.11.26  2.204.4.2-rolling  cloudbees/cje-mm:2.202.4.2  A finaliserX pse_1.11.29  2.222.1.1-rolling  cloudbees/cje-mm:2.222.1.1  TODO test en cours

 Résumé migration à 2.176.4 

Actions

Monter de version  permissive security script en version 0.6
Ajouter -Dpermissive-script-security.enabled=no_security en argument de la JVM des masters

Impacts étudiés de la montée de version :

Guide de migration : https://jenkins.io/doc/upgrade-guide/2.176/

 Montée de Git client plugin en 2.7.7.1 préconisée (beekeeper assistant) => depuis update center master : vérifier puis appliquer et redémarrer le master. A faire sur chaque master.  Ne pas faire car problème de dépendances ensuite sur pas mal de plugins du bundle

 Montée du  Security Plugin en version 1.63: 
Impacts
 Nouvelles signatures définies comme problématiques
 Le permissive security script verison 0.3 loggue à présent toutes les signatures problématiques même celles whitelistées. 
Résolution
 Monter la version du permissive security script en version 0.6 afin de ne pas logguer les signatures whitelistées. 
 Passer la propriété -Dpermissive-script-security.enabled=no_security à la JVM des master afin d'accepter les scripts dont les signatures ne sont pas whitelistés sans log dans la console.

 Amélioration de la protection CSRF:
"CSRF tokens (crumbs) are now only valid for the web session they were created in to limit the impact of attackers obtaining them. Scripts that obtain a crumb using the /crumbIssuer/api URL will now fail to perform actions protected from CSRF unless the scripts retain the web session ID in subsequent requests."  
La crumbIssuer  api est utilisée dans les jobs d'action sur les master (stop/start/restart...) et dans le job verifiant les job en cours depuis trop longtemps.
Les tests sur tstcje1 en 2.176.4 montrent que ces jobs ne sont pas impactés par l'amélioration de la protection CSRF.

Autres problèmes rencontrés lors des tests : (problèmes de configuration de la plateforme de tests)
- Evt 299037 : git Host key verification failed.
- Credentials non présents ou mot de passe erroné
- Manque settings-frontal.xml dans /nfs/cje-share-dir/tool/maven/conf
- Manque des settings d'authentification à rartifactorydocker.efluid.uem.lan dans /nfs/cje-share-dir/registryCredentials/.docker/config.json
- Problème de time zone :  set user.timezone=Europe/Paris dans les system properties du master
- Tests nécessitant l'usineLogicielleRelease de Prod: exclusion des ces tests dans les changes gerrit 116523 et 116499

 Configuration pour environnement efluid 

 Using private Docker registries 

 Pour permettre de "puller" nos images customs dans l'Artifactory depuisles Agent CJE, il faut préparer un "credential package" et le rendre accessible de tous les Workers via le partage NFS.
  * Pour créer le "credentials package": il faut archiver dans un fichier docker.tar.gz le contenu du répertoire .docker (.docker/config.json) contenant le credential suivant: 
 [root@lprhaegaedt1 nfs]# cd
 [root@lprhaegaedt1 ~]# cd .docker/
 [root@lprhaegaedt1 .docker]# ll
 total 4
 -rw------- 1 root root 173 13 juil. 21:34 config.json
 [root@lprhaegaedt1 .docker]# cat config.json
 {
   "auths": {
     "partifactorydocker.uem.lan": {
       "auth": "dXNpbmVsb2dpY2llbGxlOkFQOXh4VzU4ZEJKWkRBTFNLbjdua1A2NlM2TA==",
       "email": "info@xyz.com"
     }
   }
 }

=> 
 $ tar -tvf ~/docker.tar.gz
   drwx------ root/root         0 2015-07-28 02:54 .docker/
   -rw------- root/root       114 2015-07-28 01:31 .docker/config.json
   
Ensuite, il faut configurer au niveau de chaque "Docker Agent Template" le champ "Additional URI" avec le chemin d'accès à ce "credential package" et sélectionner "Extract":

400px|

Note: cette archive est disponible dans le repo Git: cjeConfigurations/registryCredentials

 Volumes requis dans les images Agent template docker 

Chaque image doit définir les volumes dont elle aura besoin en fonction des tâches à exécuter, celles-ci ayant besoin de disposer de ressources locales dans les conteneurs. Les volumes suivants ont été identifiés (motivation donnée pour chaque volume):

 L'utilisateur dans l'image doit être jenkins

 Remote root directory = /home/jenkins

 /nfs/cje-share-dir/tools/maven/conf:/opt/mvnDockerDefault/conf
=> requis pour la configuration Maven permettant l'authentification de UsineLogicielle à l'Artifactory (settings.xml) dans les jobs.

 /nfs/cje-share-dir/cloneReferentiels:/data/EDT/cloneReferentiels 
=> pour des raisons de performance, permetde faire des Git shallow clone dans les jobs.

 /nfs/cje-share-dir/registryCredentials/.ssh_jenkins:/home/jenkins/.ssh
=> permet de disposer des clés ssh du user d'id 1000 dans son HOME (monté) pour permettre les commandes ssh (ssh, scp, git): toute commande n'utilisant pas les "credentials Jenkins via plugin SSH-agent"

 /nfs/workspaces/workspaceTST1:/home/jenkins/work  (+ définir "Remote root directory"=/home/jenkins/work)
=> permet de disposer d'un workspace sur partage NFS (hors conteneur). Ce workspace sert de synchro entre PROD et TST1: sur le PROD, /nfs/workspaces/workspaceTST1 est un lien vers /nfstst1/workspaces/workspaceTST1).

 Dans le cas ou l'on veut faire du docker depuis un agent template 

 Dans ce cas de figure l'utilisateur de l'image doit etre root, car il faut pouvoir écrire dans /mnt/mesos/sandbox/jenkins

 Remote root directory = /mnt/mesos/sandbox/jenkins

 /nfs/cje-share-dir/registryCredentials/.docker:/root/.docker  en readonly
=> permet de disposer des credentials pour les commandes docker faites depuis les pipelines dans les conteneurs (pull, push)

 /var/run/docker.sock:/var/run/docker.sock 
=> permet de contacter le démon docker de l'hote depuis un slave contenant le client Docker pour ne pas faire du "Docker in Docker".

 CJOC Update Center 

Pour permettre la mise à jour régulière du CJOC (interrogation de l'UpdateCenter Cloudbees), il est nécessaire de configurer le proxy HTTP. Pour cela, aller dans "Administrer Jenkins" -> "Gestion des plugins" -> Onglet "Avancé"

Saisir les informations suivantes (user pour le proxy: usinelogicielle):

1200px|

 Update Credentials 

Une fois une installation CJE terminée (depuis poste lpcjewstedt1) et LDAP configuré (depuis GUI CJOC), le mot de passe administrateur utilisé lors de l'installation n'est plus valide au niveau du projet sur le poste CJE Workstation. Il faut alors mettre à jour les credentials avec un user permettant une connexion au CJOC.

Se connecter ulouser sur poste lpcjewstedt1 et sélectionner le projet dans lequel on souhaite faire la mise à jour.

Depuis $PROJECT:
 [ulouser@lpcjewstedt1 tstcje1_project]$ cje prepare credentials-update

Modifier le fichier .dna/secrets pour reconfigurer les properties cjoc_username et cjoc_password

Appliquer la modification:
 [ulouser@lpcjewstedt1 tstcje1_project]$ cje apply

 Update cjoc et managed masters UID 

Ticket 51488: https://support.cloudbees.com/hc/en-us/requests/51488?page=1

Se connecter ulouser sur la CJE Workstation. Se placer dans le bon projet à modifier.

Editer le fichier .dna/project.config et modifer les valeurs attendues. 

Ensuite, appliquer :
 [ulouser@lpcjewstedt1 tstcje1_project]$ cje upgrade --config-only --force

 Système de synchronisation des templates de jobs 

Les templates sont centralisés: 
 http://cje.efluid.uem.lan/usineadministration/job/FjobsTemplate/

De plus, 1 job permet de synchroniser ces templates de UA vers UsineTarget à préciser: 
 http://cje.efluid.uem.lan/usineadministration/job/FjobsTemplate/job/jenkins.synchronize-template-between-master/. 

Ce job est à appeler par toute usine souhaitant récupérer une synchro de template.
Exemple: 
 http://cje.efluid.uem.lan/usinedeploiement/job/FjobsUtilitaires/job/template.orchestrate-template-synchronized-from-UA-to-UD/build?delay=0sec

 Backups 
Tous les Masters sont backupés au travers de jobs "Cluster Operations", qui permettent de lancer une opération d'exploitation depuis le CJOC sur l'ensemble des Managed Masters créés dans le Cluster Mesos.
Les backups de chaque master sont faits en 2 étapes: 
 Backup système: http://cje.efluid.uem.lan/cjoc/view/All/job/backup.system/
 Backup des jobs: http://cje.efluid.uem.lan/cjoc/view/All/job/backup.jobs/

Les backups réalisées (format tar.gz) sont transférés sur NFS (/nfs/backups/<cjeCluster>) via sftp sur le serveur lpcjewstedt1, en utilisant le compte jenkins4docker (credential créé avec sa clé privée saisie directement depuis l'interface Jenkins).

Ces 2 jobs sont déclenchés tous les soirs avec une rétention de 7 jours.

 Logger des masters 

Pour arrêter les logs de niveau INFO trop verbeux sur les masters (/var/lib/dockre/containers/*.json) du Logger "org.jenkinsci.plugins.permissivescriptsecurity.PermissiveWhitelist", un script Groovy a été ajouté dans /nfs/cje-share-dir/init.groovy.d-all/logging.groovy :
 import java.util.logging.LogManager
 import java.util.logging.Logger
 import jenkins.model.Jenkins
 import java.util.logging.Level
 def logger = Logger.getLogger("org.jenkinsci.plugins.permissivescriptsecurity.PermissiveWhitelist")
 logger.setLevel(Level.WARNING)
 LogManager.getLogManager().addLogger(logger)

Un lien (hard) a ensuite été créé dans le jenkins_home de chaque Master pour profiter de ce fichier :
 Se connecter sur 1 lpcjemtredtX en tant que jenkins4docker
 cd /nfs/usinerecette/jenkins_home/init.groovy.d
 ln /nfs/cje-share-dir/init.groovy.d-all/logging.groovy logging.groovy

 Tips 
 Création d'un nouveau MASTER 

La création d'un nouveau Master se fait depuis l'interface du CJOC. Mais lors de la mise en oeuvre du Cluster CJE, le user utilisé était ulouser (uid:500).
Ce user a ensuite été modifié à jenkins4docker (uid=1000) pour le démarrage des Master et Agent. Mais lors de la création d'un nouveau Master, le répertoire sur NFS de ce nouveau Master est toujours fait avec ulouser comme propriétaire: cd /nfs; ls -altr
Ceci empêche le Master de démarrer car il n'est pas propriétaire (jenkins4docker) du répertoire mis à disposition (ulouser). Pour observer ce phénomène:
 Créer 1 nouveau Master
 Se connecter root sur lpcjemtredt1 ou 2 ou 3
 docker ps -a => on voit les tentatives de démarrage du master créé, mais en echec
 docker logs <contenerId du dernier failed>
 
 Curl_http_done: called premature == 0
 Connection #0 to host localhost left intact
 DEPLOY GROOVY INIT SCRIPTS
 ln: failed to create symbolic link '/var/jenkins_home/configure-jenkins.groovy.d': Permission denied
 cp: cannot create directory '/var/jenkins_home/configure-jenkins.groovy.d': Permission denied
 
CORRECTION: chown -R jenkins4docker /nfs/<nomDuNouveauMaster>

=> Le master démarrera automatiquement.

 Complément à ajouter dans le jenkins_home des masters 
Le plugin LogParserPublisher nécessite la présence d'un fichier de règles, qui ne sera lu qu'à partir du master.
 création d'un lien physique du fichier se trouvant sur le nfs vers le jenkins_home. Permet de partager le même fichier de règle entre les masters.
 se connecter en tant que root : 
 mkdir -p /nfs/<nomMaster>/jenkins_home/interneUL/rules 
 chown -R ulouser:docker /nfs/<nomMaster>/jenkins_home/interneUL
 se connecter en tant que ulouser
 ln /nfs/cje-share-dir/interneUL/rules/bdd_parse_error.rule /nfs/<nomMaster>/jenkins_home/interneUL/rules/bdd_parse_error.rule

 Copier le fichier /nfs/<nomMaster>/jenkins_home/.groovy/grapeConfig.xml d'un autre master vers le nouveau master
 Créer le repertoire (avec les bon droits) /nfs/<nomMaster>/jenkins_home/.groovy/grapes
 Vérifier la présence du répertoire /nfs/<nomMaster>/jenkins_home/.groovy/grapes/com.oracle après le lancement du premier job car apparemment celui-ci n'est pas encore récupéré automatiquement

 Copier le répertoire ssh d'un master existant vers le nouveau : /nfs/<nomMaster>/jenkins_home/.ssh

 Configuration à faire dans le master (dans jenkins) 
 Au premier démarrage du master, fermer la fenêtre d'installation des plugins car on ne peut pas le faire par la, il faut se rendre directement sur l'interface classique de jenkins
 Activer l'update automatique au démarrage des plugins dans le beekeeper assistant (sinon les plugins bundles genre Pipeline API ne seront pas la et rien ne marchera)
 Ajouter les plugins nécessaires en se basant sur une liste de plugins d'un autre master
 Recopier la configuration de l'administration du master à partir d'un autre master
 Recopier la configuration du serveur Gerrit du master à partir d'un autre master
 Recopier la configuration de l'administration de sécurité du master à partir d'un autre master

 Redémarrage 
Une fois toutes les opérations précédentes réalisées, il faut absolument faire un redemarrage du master sinon des configurations de plugins par exemple ne seront pas prise en compte, et on obtiendra des erreurs bizarres

 Configuration TimeZone 

 Sur un Master 
Au niveau de la configuration d'un Master, dans les paramètres avancés de l'image, ajouter dans la partie System Properties (param pris en compte par la JVM): org.apache.commons.jelly.tags.fmt.timeZone=Europe/Paris

Redémarrer le Master pour prise en compte depuis l'interface CJOC => Manage->Restart

 Sur le CJOC 

Se connecter ulouser sur lpcjewstedt1 et se placer dans le bon projet (ex: /data/EDT/files/cje_prod_1_project).

S'assurer que la variable d'environnement PROJECT pointe bien sur ce répertoire.

Préparer la configuration CJOC
  $ cje prepare cjoc-update

Editer le fichier cjoc-update.config et modifier la valeur jvm_options:
  jvm_options = -Xmx1024m -XX:+PrintGCDetails -Dorg.apache.commons.jelly.tags.fmt.timeZone=Europe/Paris

Sauver et quitter.

Lancer la configuration :
  $ cje apply

=> Le cjoc sera redémarré en utilisant le TimeZone configuré.

 Create a Jenkins Slave Docker Image 

To create a slave image. The image should contain the following minimum configurations to act as a slave.
 sshd service running on port 22.
 Jenkins user with password.
 All the required application dependencies for the build. For example, for a java maven project, you need to have git, java, and maven installed on the image.

Make sure sshd service is running and can be logged into the containers using a username and password. Otherwise, Jenkins will not be able to start the build process.

 Comment configurer le proxy HTTP sur le CJOC pour l'updateCenter ? Car var dans le conteneur 

600px|

 Composants actifs sur une plateforme CJE 

700px|

 Déclencher un build via API REST 

Il faut que l'utilisateur existe sur le master où on souhaite déclencher le build. 
L'api token se génère via le profil d'utilisateur dans Jenkins.

 Récupérer le crumb pour pouvoir utiliser l'api
  curl -sS -XGET --user <username>:<api token> '<masterUrl>/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)' > crumb

 Déclencher le build 
  curl -sS -XPOST --user <username>:<api token> -H "$(cat crumb)" "<masterUrl>/<job>/build"

 Pipeline from scm 
 Attention à la petite case à cocher "lightweight checkout" dans les pipeline from scm du CJE, ça risque de foutre la merde, par défaut il ne faut pas la cocher, et voir ensuite au cas par cas (en gros il ne fait pas de checkout du référentiel et il recupere un jenkinsfile directement dans un cache certainement)

 Faire du JmxRemote dans un job CJE en parant d'un Jenkinsfile 
 Pour cela il faut valoriser les paramètres suivants dans le job (attention un seul job peut-etre lancé à la fois à cause du mapping de port)
 activeJmxRemote : true
 jmxRemotePort : une valeur entre 31000 et 32000 (si non saisi alors c'est 31010 par défaut)
 jmxRemoteHost : le nom du Worker ou va être lancé le job (pas facile à connaitre à priori ...) => si cette valeur n'est pas positionné alors jenkins la valorise tout seul
 nodeLabel : utiliser le nom d'un template Docker dans lequel un mapping de port a été effectué en prenant la valeur de jmxRemotePort

 Cache des masters pour la jenkinsSharedLibrary 

Après redémarrage d'un master, on peut avoir des erreurs de load de driver jdbc qui utilise l'annotation Grab en groovy.
Il semblerait que cette annotation se base sur le cache du master. Au redémarrage ce dernier étant vidé, les drivers ne sont plus présent.
Pour palier à ce soucis, il faut recharger les driver de manière brute comme dans l'exemple suivant : http://gerrit.uem.lan/#/c/69213/18/src/com/efluid/bdd/legacy/BddFunc.groovy

 ELK en erreur 
Si les dashboards CJE (build analytics) ne fonctionne plus alors voici des astuces/pages d'aides à la résolution du problème
 Case Cloudbees de référence : https://support.cloudbees.com/hc/en-us/requests/57029
 Faire un check de l'état du cluster
 Se rendre sur la workstation lpcjewstedt1
 Se mettre en utilisateur ulouser
 Se mettre sur la prod : setProd
 Lancer le script de check :  ./checkElkStatus.sh
 Ce script va donner un retour de l'état du cluster, dans l'atribut json status, si c'est rouge pas bon !
 Il faut ensuite regarder s'il y a des shared non assignés, et pour cela faire un cat shards.txt et regarder les lignes UNASSIGNED, s'il y en a => pas bon !
 Il existe une procédure pour les ré-assigner ici : https://support.cloudbees.com/hc/en-us/articles/115000089811-ElasticSearch-troubleshooting-guide  => rubrique Unassigned Shards, cela peut corriger le problème => ./fixEsShards.sh , il faut ensuite refaire un check status pour voir si ça repasse vert
 Si c'est toujours pas vert alors il va falloir supprimer les index en erreur, pour cela la doc précédente propose une solution, qui consiste à lancer le script  ./deleteRedIndices.sh (sans le mode DRY RUN)
 DRY_RUN=false ./deleteRedIndices.sh

Après tout cela l'état du cluster doit redevenir vert, mais par contre les dashboard Kibana sont potentiellement plus la, pour les reconstruire il faut redemarrer le CJOC.
Si besoin de réindexer les masters alors il faut lancer une tache de réindexation via le CJOC comme décrit ici : “Reindexing for CloudBees Jenkins Analytics” section here: https://go.cloudbees.com/docs/cloudbees-documentation/cjoc-user-guide/index.html#analytics

 ELK : suppression de SNAPSHOTS si saturation des inodes/espace 
 Se rendre sur la workstation lpcjewstedt1
 Se mettre en utilisateur ulouser
 Se mettre sur la prod : setProd
 Lancer la suppression en indiquant en paramètre le nombre de SNAPSHOT que l'on souhaite garder (si pas de param, 30 par défaut)  :  ./deleteEsSnapshots.sh [NBtoKEEP]

 Inspect d'un Master dans CJE 

Montages volumes:

 Source: /var/lib/mesos/slaves/a68f9de8-c267-42c5-94a4-683c5a4072e2-S11/frameworks/a68f9de8-c267-42c5-94a4-683c5a4072e2-0000/executors/masters_demomaster2.744bd770-5c17-11e7-a4e8-02425ce5de65/runs/8339d825-6eaf-43df-a797-16da03775581
 Destination: /mnt/mesos/sandbox

  Source: /var/lib/docker/volumes/fd8d6aba668f0904510926214b5bb76ee810e7438039513fd53ddee553856608/_data
  Destination: /var/jenkins_home

Variables: 
            "Env": [
                "MARATHON_APP_VERSION=2017-06-28T15:35:46.222Z",
                "HOST=192.168.106.234",
                "MARATHON_APP_RESOURCE_CPUS=0.1",
                "PORT_10012=31077",
                "MARATHON_APP_DOCKER_IMAGE=cloudbees/cje-mm:2.46.3.2",
                "MESOS_TASK_ID=masters_demomaster2.744bd770-5c17-11e7-a4e8-02425ce5de65",
                "PORT=31075",
                "MARATHON_APP_RESOURCE_MEM=1024.0",
                "PORT_10011=31076",
                "PORTS=31075,31076,31077",
                "PORT1=31076",
                "MARATHON_APP_RESOURCE_DISK=0.0",
                "MARATHON_APP_LABELS=",
                "PORT_10010=31075",
                "MARATHON_APP_ID=/masters/demomaster2",
                "PORT0=31075",
                "PORT2=31077",
                "MESOS_SANDBOX=/mnt/mesos/sandbox",
                "MESOS_CONTAINER_NAME=mesos-a68f9de8-c267-42c5-94a4-683c5a4072e2-S11.8339d825-6eaf-43df-a797-16da03775581",
                "EVALUATION_MODE=no",
                "JAVA_OPTS=-Xmx716m -Xms716m -DMASTER_GRANT_ID=\"512692bb-175b-42cf-bd07-eacfb39ca063\" -Dhudson.slaves.NodeProvisioner.initialDelay=\"0\" -DMASTER_INDEX=\"1\" -DMASTER_OPERATIONSCENTER_ENDPOINT=\"http://cje.efluid.uem.lan/cjoc/\" -DMASTER_NAME=\"demomaster2\" -DMASTER_ENDPOINT=\"http://cje.efluid.uem.lan/demomaster2/\"",
                "JENKINS_OPTS=--prefix=/demomaster2/",
                "SIGNATURE=ak0VNh+/zNVIoUHQj63fIzQxR1eVLTD05UyK8ZavOuBCGKJ78JQVQKx6euyWajD4qDVdJURSiXAwJ2eKt7a9YGdlGcfPpHcAl6DAF8XJxTLTfoLy2FURcUg/dqncaPF9zE0w/mK2qRP2ZCx2FnOnmTmMK93s9A+jUzELaYl9lKDMx8OVtBFsxShZQNdoWG8aWFNlnGHI+EA+MckH0YIqARU+LWSnd15UEKLOq9A+w1GaUhLMI2sjpTaWFlujwfc13HwOD3GlGGGUFGQL7wylRUvJxGs90dCANDIDys+Vr7U1oA0O2cxMcBgUrbDXAqNqewn7+Ny9UGb15pe91M2cWQ==",
                "TENANT=demomaster2",
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
                "LANG=C.UTF-8",
                "JAVA_HOME=/docker-java-home",
                "JAVA_VERSION=8u131",
                "JAVA_DEBIAN_VERSION=8u131-b11-1~bpo8+1",
                "CA_CERTIFICATES_JAVA_VERSION=20161107~bpo8+1",
                "JENKINS_HOME=/var/jenkins_home",
                "JENKINS_SLAVE_AGENT_PORT=50000",
                "TINI_VERSION=0.9.0",
                "TINI_SHA=fa23d1e20732501c3bb8eeeca423c89ac80ed452",
                "MESOS_VERSION=0.28.2*",
                "REF_DIR=/usr/share/jenkins/ref",
                "VOLUME_SERVICE=http://localhost:31080",
                "HOME_DIR=/usr/share/jenkins/home",
                "CACHE_DIR=/tmp/jenkins",
                "COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log",
                "TRY_UPGRADE_IF_NO_MARKER=true"
            ],

 Monitoring 
 Charge des Workers : Perf_cje

 Jacoco 
Mettre en place la couverture de test sur un master : 

 modifier les paramètres Java du master et ajouter :  -javaagent:/var/jenkins_home/jacoco/jacocoagent.jar=destfile=/var/jenkins_home/jacoco/result/jacoco.exec,append=false,includes=com.efluid.*,excludes=*#*cps*,classdumpdir=/var/jenkins_home/jacoco/classDumpDir
 Essayer avec excludes=**/*/___cps*

Puis générer le rapport de test (après avoir récupérer le .exec et le classDumpDir)

 java -jar lib/jacococli.jar report result/jacoco.exec --classfiles classDumpDir/ --html html

 Docker agent template (sur CJE PROD) 
 efluid : 20 CPU, 10Go RAM
 migefluid : ?
 suivefluid / enercom / ethaque / eldap / ael / efluid.net : 5 CPU, 4Go RAM
 archi / edk / ecore / edoc : 5 CPU, 4Go RAM
 base : 1CPU, 1Go RAM

=> en terme de agent docker template
 XXX-base => 1CPU, 1G RAM
 XXX-2G => 3CPU, 2G RAM
 XXX-4G => 6CPU, 4G RAM
 XXX-6G => 10CPU, 6G RAM
 XXX-10G => 20CPU, 10Go RAM

 agent-socle-jenkins-maven-docker-13-10G
Fonctionne avec USER ROOT, et le workspace arrive sous /data au niveau du Host
 labels : socle-jenkins-maven-docker-13-10G
 CPU : 1
 Memory : 40000
 JVM Memory : 512
 Filesystem : /mnt/mesos/sandbox/jenkins
 Image : partifactorydocker.uem.lan/socle-jenkins-maven-docker:3.3.9.6-1
 Additional URI : file:///nfs/cje-share-dir/registryCredentials/docker.tar.gz + Extract
 Volumes :
 /nfs/cje-share-dir/tools/maven/conf:/opt/mvnDockerDefault/conf
 /nfs/cje-share-dir/cloneReferentiels:/data/EDT/cloneReferentiels
 /nfs/cje-share-dir/registryCredentials/.ssh_jenkins:/root/.ssh
 /nfs/cje-share-dir/registryCredentials/.docker:/root/.docker RO
 /var/run/docker.sock:/var/run/docker.sock
 /dev/shm:/dev/shm
 Parameters :
 cpus : 8.0

 agent-socle-jenkins-maven-docker-13-4G 
Fonctionne avec USER ROOT, et le workspace arrive sous / au niveau du Host
 labels : socle-jenkins-maven-docker-13-4G
 CPU : 1
 Memory : 4096
 JVM Memory : 512
 Filesystem : /mnt/mesos/sandbox/jenkins
 Image : partifactorydocker.uem.lan/socle-jenkins-maven-docker:3.3.9-6
 Additional URI : file:///nfs/cje-share-dir/registryCredentials/docker.tar.gz + Extract
 Volumes :
 /nfs/cje-share-dir/tools/maven/conf:/opt/mvnDockerDefault/conf
 /nfs/cje-share-dir/cloneReferentiels:/data/EDT/cloneReferentiels
 /nfs/cje-share-dir/registryCredentials/.ssh_jenkins:/root/.ssh
 /nfs/cje-share-dir/registryCredentials/.docker:/root/.docker  RO
 /var/run/docker.sock:/var/run/docker.sock
 /dev/shm:/dev/shm

 agent-socle-jenkins-maven-docker-13-10G-NONROOT 
Fonctionne avec USER Jenkins, et le workspace arrive sous /data au niveau du Host
 labels : socle-jenkins-maven-docker-13-10G-NONROOT
 CPU : 1
 Memory : 40000
 JVM Memory : 512
 Filesystem : /home/jenkins
 Image : partifactorydocker.uem.lan/socle-jenkins-maven-docker-nonroot:3.3.9-6
 Additional URI : file:///nfs/cje-share-dir/registryCredentials/docker.tar.gz + Extract
 Volumes :
 /nfs/cje-share-dir/tools/maven/conf:/opt/mvnDockerDefault/conf
 /nfs/cje-share-dir/cloneReferentiels:/data/EDT/cloneReferentiels
 /nfs/cje-share-dir/registryCredentials/.ssh_jenkins:/home/jenkins/.ssh
 /nfs/cje-share-dir/registryCredentials/.docker:/home/jenkins/.docker  RO
 /var/run/docker.sock:/var/run/docker.sock
 /dev/shm:/dev/shm
 /data/mesos/sandbox/workspaces:/home/jenkins/workspace

 agent-socle-jenkins-maven-13-2G 
Fonctionne avec USER Jenkins, et le workspace arrive sous / au niveau du Host
 labels :socle-jenkins-maven-13-2G
 CPU : 1
 Memory : 2048
 JVM Memory : 128
 Filesystem : /home/jenkins
 Image : partifactorydocker.uem.lan/socle-jenkins-maven:3.3.9-6
 Additional URI : file:///nfs/cje-share-dir/registryCredentials/docker.tar.gz + Extract
 Volumes :
 /nfs/cje-share-dir/tools/maven/conf:/opt/mvnDockerDefault/conf
 /nfs/cje-share-dir/cloneReferentiels:/data/EDT/cloneReferentiels
 /nfs/cje-share-dir/registryCredentials/.ssh_jenkins:/home/jenkins/.ssh
 /dev/shm:/dev/shm

 Recette CJE post migration 

 Valider que les backup fonctionnent toujours : 
 http://cje.efluid.uem.lan/cjoc/view/All/job/Fadmin/job/backup.system/
 http://cje.efluid.uem.lan/cjoc/view/All/job/Fadmin/job/backup.jobs/

 Valider que la sharedLibrary compile toujours :
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FsharedLibrary/job/jenkinsSharedLibrary.compile/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FsharedLibrary/job/jenkinsSharedLibrary.validation-gerrit/

 Valider que les jobs du jenkinsConfiguration fonctionnent toujours:
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid-create-dynamic-bdd/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid-UEM_NONREG_JENKINSCONFIG_EMBEDDED_V13_restart_application/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid.changeCacheApplication-DEV_UEM_NONREG_JENKINSCONFIG_EMBEDDED/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid.compile-dev_nonRegJenkins_141100/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid.launch-batchs-UEM_NONREG_JENKINSCONFIG_EMBEDDED/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid.testIntegration-dev_nonRegJenkins_141100/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid.upgradeBddDev-dev_nonRegJenkins_141100/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluid.validation-gerrit/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/efluidnet.validation-gerrit/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/enercom.launch-batchs-DEV_UEM_MOB_NB_MAINTENANCE_V13/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/suite-efluid.orchestrate-deploy-weblogic-DEV-ENEDIS-TST-SUP-1/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/suite-efluid.orchestrate-nightly-build-workflow-develop-UEM-NONREG_JENKINSCONFIG_WEBLOGIC/
 http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/FjenkinsConfigurationNonReg/job/suite-efluid.orchestrate-nightly-build-workflow-develop-UEM_NONREG_JENKINSCONFIG_EMBEDDED/

 Valider que la validation du jenkinsConfiguration fonctionne toujours:
 tous les jobs qui commencent par http://cje.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/job/jenkinsConfiguration.test-

 Validation des plugins:
 les jobs du dossier http://cje.efluid.uem.lan/usinerecette/job/FnonRegressionTests/job/FdslExample/<!!!> Installation du JDK 11 

Récuperez la version du jdk 11.0.8 dans  (il s'agit d'un zip) puis dézippez l'archive dans le répertoire d'installation classique des jdks :
 D:\Programs\jdk\jdk11.0.8

Fichier:DezipJDK8_01.jpg

 Mettre à jour la variable d'environnement JAVA_HOME  
N'oubliez pas de mettre vos variables d'environnements à jour en suivant la procédure ici<!!!>Catégorie:Eclipse

Category:outil
Category:eclipse

 préalable  

Cette version d'éclipse a été packagé pour pouvoir développer avec le JDK 11.

Si vous n'avez pas le JDK 11, suivez les étapes ci-dessous :  
 Téléchargez le JDK 11 packagé (zip) : JDK 11.0.1
 Rendez vous sur le tutoriel pour l'installation : http://wikefluid/index.php/Jdk11#Installation_du_JDK_11

 Récupération du zip eclipse et de son master workspace associé 

Afin de simplifier et d'harmoniser les versions d'Eclipse au sein des environnements développeurs; une solution packagée d'Eclipse à été constituée à partir :
 d'une version d'Eclipse : (actuelle: 2018-09 (4.9.0))
 d'une version du masterWorkspace photon ; (actuel: 2.0.0)

La version d'Eclipse peut être récupérée sur Artifactory en suivant ce lien : Eclipse 2018-09 (4.9.0)

La version du masterWorkspace associée à cet version d'eclipse peut être récupéré sur Artifactory en suivant ce lien : masterWorkspace 2.0.0

 Installation Eclipse/JDK  

 Installer le JDK 
Aller dans Window > Java > Installed JREs puis cliquez sur Add...
Fichier:ImportJDK11.PNG
Choissiez Standard VM puis appuyez sur Next
Fichier:ImportAddJDK11.PNG
dans JRE home : il faut aller chercher le chemin du JDK 11.0.1
appuyez sur Finish
Fichier:JDKDefinition.PNG

 mettre le "Compiler compliance level" à 11 
 pour ce faire aller dans Window > Preferences > Java > Compiler
Fichier:CompilerComplanceLevel.PNG

 Tester efluid avec JDK 11 
En attendant le merge définitif du JDK 11 pour tester il faut : 
 récupérer le code du change gerrit : http://gerrit.uem.lan/#/c/72256/27
 Créer ou modifier le Webby 
 provider : tomcat9x
 JRE : Alternate JRE : choisir JDK11.0.1
 Lancer

 Gestion des sources pour les tests 

Dans photon, le connecteur m2e censé gérer l'ajout de sources au projet ne fonctionne, a priori, plus.
Il faut donc les ajouter à la main au build path et indiquer qu'il s'agit de test<!!!>Category:outil

 Généralités 

L'IDE étant l'outil de travail de base du développeur, c'est à lui qu'en incombe la responsabilité première. Chaque développeur doit maitriser son IDE.
Le développeur peut choisir l'IDE qu'il souhaite sous réserve qu'il soit dans la liste des logiciels validés au niveau licence (si ce n'est pas le choix, il faut alors que le développeur fasse le formulaire d'ajout du logiciel en question). Si le logiciel est payant (exemple : IntelliJ) alors il faut également que le développeur s'assure qu'il ait aussi la licence pour la partie payante.

 Support 

Le premier niveau de support doit se faire au niveau de l'équipe du développeur. Celui-ci doit en effet commencer par demander dans son équipe (section/division/filière) de l'aide s'il ne s'en sort pas tout seul. La plupart du temps cela suffit à régler la majorité des problèmes.
Le second niveau de support est l'équipe "d'expert IDE" qui est constituée de personnes volontaires (dont les noms sont écrits dans l'encadré en haut à droite) qui n'hésites pas à faire de la veille techno pour améliorer leurs connaissances et compétences sur ce sujet. Cette équipe est à contacter via le forum dans cette catégorie : http://eforum.uem.lan/viewforum.php?f=16). Veillez déjà à regarder si votre problème n'est pas déjà présent dans le forum.

 Packaging 

Le packaging des IDE est fourni pour certains IDE, pas pour tous. Cf la liste suivante.

 Eclipse 

Le zip d'eclipse "officiel" est constitué par l'équipe "expert IDE" assistée de l'équipe "Usine logicielle". Le détail de constitution de ce zip est présent ici : Zip_d%27%C3%A9clipse.<!!!>Category:outil
Category:arthas

Créé par Alibaba, Arthas est un outil open source qui permet aux développeurs de diagnostiquer des applications Java en cours d'exécution sans les redémarrer ni les suspendre.

 Guide d'utilisation 
 Guide d'utilisation d'Arthas
 Eforum vidéo
 Vidéo de présentation

 Liens externes 
  Documentation officielle
 Description de JM Doudoux
 Github arthas<!!!>Category:java

 Utilisation d'une version SNAPSHOT de Quarkus 

 Récuperez le projet github https://github.com/quarkusio/quarkus (éventuellement sur une branche particulière si l'on veut builder une PR)
 Changez le distributionManagement (afin de pointer sur artifactory) dans les pom.xml suivants : 
 pom.xml
 independent-projects/arc/pom.xml
 independent-projects/bootstrap/pom.xml
 Lancer la commande maven : 
 mvn clean deploy -Denforcer.skip=true -DskipTests
 Cela déploiera une version 999-SNAPSHOT de quarkus<!!!>Category:Outil

Application permettant le contrôle des communications réseau d'un poste de travail.

Lien pour télécharger l'installateur de Wireshark : 

https://www.wireshark.org/download.html

 Documentation de Wireshark : 

https://gitlab.com/wireshark/wireshark/-/wikis/home

 Utilisation de Wireshark : 

Conformément à la documentation de Wireshark (https://gitlab.com/wireshark/wireshark/-/wikis/CaptureSetup/CapturePrivileges), afin d'utiliser Wireshark de la manière la plus sécurisée qui soit, il est nécessaire de démarrer le driver npcap (ou npf sur les versions de Wireshark < 3.0) manuellement avant la capture, puis de le stopper une fois que Wireshark n'est plus utilisé.

Les droits de lancement et d'arrêt de ce service sont conditionnées par une GPO spécifique non appliquée par défaut. Un profil eldap est nécessaire afin de disposer de cette GPO: LOCAL_ADMIN_ NPCAP, se rapprocher de sa hiérarchie pour en faire la demande.
Commande de lancement du service au sein d'une invite de commande cmd: sc start npcap
Commande d'arrêt du service: sc stop npcap<!!!>Page wiki décrivant les bonnes pratiques de sécurité liées à docker dédiées à l'éditeur.
Voir la page XXX pour les bonnes pratiques dédiées aux administrateurs système.

 Image Docker 

 Confidentialité 

Ne laisser aucun secret directement lisible dans les Dockerfile

Si une image docker doit utiliser un secret au moment de son build, celui-ci devra être "passé" dans un fichier temporaire supprimé dans le Dockerfile.

L’historique de l’image peut être retrouvé grâce à la commande docker history --no-trunc <image>

 Confiance 

N’installer que des paquets vérifiés

En utilisant uniquement des sources sûres et sécurisées (repository interne ou public mais vérifié)

 Minimisation 

Ne garder que les paquets nécessaires dans les images

Supprimer les paquets inutiles dans le Dockerfile

Éviter de laisser un serveur ssh dans le conteneur, préférer un autre moyen pour se connecter comme docker exec

 Source 

 Version 

Vérifier que l’image utilisée est bien dans la version souhaitée

En utilisant des mécanismes qui garantissent qu’il s’agit bien de la dernière version et non d’une ancienne en cache

Éviter Le tag latest est aussi vulnérable à ce genre de problème

 Confiance 

S’assurer que les images, y compris celles de base, sont vérifées

En activant le Docker Content Trust avec la variable d’environnement DOCKER_CONTENT_TRUST=1

 Conteneur 

 Réseau 

N’ouvrir que les ports nécessaires et les mapper préférablement sur une interface précise

En utilisant le format 10.20.30.40:5000:6000

Eviter aussi de mapper des port privilégiés (<1024) de l’hôte sur le conteneur, sauf cas utiles

 Privilèges 

Lancer le conteneur avec un utilisateur qui n’a pas de droits avancés, ni dans le conteneur ni sur l’hôte

En utilisant l’instruction USER dans le Dockerfile et en créant si besoin l’utilisateur avec les bons IDs

Si possible exploiter le paramètre --security-opt=no-new-privileges (ou setpriv une fois les paramétrages terminés)

 Intégrité 

Dans la mesure du possible, lancer le conteneur avec le système de fichiers racine en lecture seule

En utilisant le paramètre --read-only

Les dossiers montés ne sont pas affectés par ce paramètre, aussi l’option --tmpfs peut être utile

 Disponibilité 

 Surveillance 

Monitorer l’état du service du conteneur

En rajoutant une instruction HEALTHCHECK dans le Dockerfile ou en utilisant le paramètre --health-cmd

Cela permet à Docker de remonter l’état à un outil de supervision ou encore de redémarrer le conteneur automatiquement

 Restriction 

Paramétrer des limites pour l’utilisation des ressources

En utilisant les paramètres --memory, --cpu-shares et --pids-limit

Les valeurs doivent être bien étudiées et potentiellement fournies par les administrateurs comme pour le processeur

 Gestion 

Faire le ménage dans les images et les conteneurs

En listant les images et les conteneurs présents sur l’hôte et en retirant tous ceux qui ne sont plus nécessaires

Cela évite de monopoliser des ressources inutilement et d’instancier d’anciennes versions potentiellement vulnérables

 Autres recommandations 

 Construction 

Ne pas laisser les instructions de mise à jour du Dockerfile être en cache

En rajoutant des mécanismes pour que l’instruction ne soit pas placée dans le cache

Il est aussi possible d’utiliser docker build avec le paramètre --no-cache pour ne pas utiliser le cache

 Mises à jour 

Surveiller et corriger les vulnérabilités de Docker

En faisant une veille et en mettant à jour Docker

Si besoin prendre contact avec les administrateurs

 Exclusion 

Ne jamais utiliser certaines fonctionalités

En excluant les paramètres --privileged, --net=host, --pid=host, --ipc=host, --uts=host, --userns=host

Ces options ne sont normalement absolument pas nécessaires pour ce genre de service<!!!>Page renommée en JenkinsCore<!!!>Category:outil
Category:BDD
Category:docker

 Oracle XE 
 Téléchargement 
 Vous déconnecter du VPN et désactiver le proxy via les paramètres de votre navigateur et télécharger l'utilitaire : https://cloud.uem-metz.fr/index.php/s/7ZtTogFpmgwgWXp
 Faire une installation docker si votre poste est en windows 10. Cette solution est en cours d'étude de notre côté pour vous apporter la procédure. Pour le moment seul la première procédure est validée.

 Installation 
Prérequis
 Etre administrateur du poste
 Si le 6000 vous fait l'installation, demander à ce que ce soit la ligne suivante qui soit exécuté pour lancer l'installation : 

 setup.exe /v"CHAR_SET=WE8ISO8859P15" 

Attention à bien mettre un mot de passe que vous noterez lors de l'installation, ce sera le mdp du compte SYS. Il vous servira à vous connecter par exemple via sql developper.
 Configuration 
Une fois Oracle XE installé, il va falloir configurer la base pour utiliser les bons formats de date / langage.

 Connectez vous avec le compte sys via SQLPlus dans une invite de commande windows :
 sqlplus sys/VOTREMDP@//localhost:1521/XEPDB1 as sysdba 

 Entrez ces requêtes successivement :

 ALTER system SET NLS_DATE_FORMAT            ='DD/MM/RR HH24:MI' scope=spfile; 
 ALTER system SET NLS_TIMESTAMP_FORMAT       ='DD/MM/RR HH24:MI:SSXFF' scope=spfile; 
 ALTER system SET NLS_TERRITORY              ='FRANCE' scope=spfile; 
 ALTER system SET NLS_LANGUAGE               ='FRENCH' scope=spfile; 
 ALTER system SET "_replace_virtual_columns" =false scope=spfile; 
 ALTER system SET QUERY_REWRITE_ENABLED      =false scope=spfile; 
 ALTER system SET STAR_TRANSFORMATION_ENABLED=false scope=spfile; 
 SHUTDOWN IMMEDIATE; 
 STARTUP; 

 Procédure pour utiliser Oracle en local 

 Création d'un schéma oracle 
Prérequis
 Il faut créer le dossier "tablespace" dans l'arborescence : "D:\java\"

 Une fois installée, il faut se connecter sur sqlDevelopper sur notre instance oracle :

 Nom de connexion : le nom que vous souhaitez donner à votre connexion
 Nom utilisateur : sys
 Mot de passe : le mot de passe défini pendant l'installation
 Rôle : SYSDBA
Dans intellij : internal_logon=sysdba
Fichier:sysdbaintellij.png
 URL JDBC : jdbc:oracle:thin:@127.0.0.1:1521/XEPDB1
 
Fichier:OracleXE.PNG

 Executer le script suivant, ce dernier va créer un schéma EFLUID_DEVELOP :

 CREATE TABLESPACE EFLUID_DEVELOP_BATCH_DATA DATAFILE 
  'D:\java\tablespace\EFLUID_DEVELOP_BATCH_DATA01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_BATCH_INDEX DATAFILE 
  'D:\java\tablespace\EFLUID_DEVELOP_BATCH_INDEX01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_BLOB DATAFILE 
  'D:\java\tablespace\EFLUID_DEVELOP_BLOB01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_DATA DATAFILE 
  'D:\java\tablespace\EFLUID_DEVELOP_DATA01.dbf' SIZE 100M AUTOEXTEND ON NEXT 50M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_INDEX DATAFILE 
  'D:\java\tablespace\EFLUID_DEVELOP_INDEX01.dbf' SIZE 100M AUTOEXTEND ON NEXT 50M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_PARAM DATAFILE 
  'D:\java\tablespace\EFLUID_DEVELOP_PARAM01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_UPGRADE DATAFILE 
  'D:\java\tablespace\EFLUID_UPGRADE_01.dbf' SIZE 1G AUTOEXTEND OFF;
 
 alter session set "_ORACLE_SCRIPT"=true;
 CREATE ROLE HERMES_OWNER_ROLE NOT IDENTIFIED;
 -- Object privileges granted to HERMES_OWNER_ROLE
 GRANT EXECUTE ON DBMS_CRYPTO TO HERMES_OWNER_ROLE;
 GRANT EXECUTE ON DBMS_PARALLEL_EXECUTE TO HERMES_OWNER_ROLE;
 GRANT EXECUTE ON DBMS_RANDOM TO HERMES_OWNER_ROLE;

 -- System privileges granted to HERMES_OWNER_ROLE
 GRANT CREATE JOB TO HERMES_OWNER_ROLE;
 GRANT CREATE MATERIALIZED VIEW TO HERMES_OWNER_ROLE;
 GRANT CREATE SYNONYM TO HERMES_OWNER_ROLE;
 GRANT CREATE VIEW TO HERMES_OWNER_ROLE;
 GRANT QUERY REWRITE TO HERMES_OWNER_ROLE;

 -- Roles granted to HERMES_OWNER_ROLE
 GRANT CONNECT TO HERMES_OWNER_ROLE;
 GRANT RESOURCE TO HERMES_OWNER_ROLE;

 CREATE USER EFLUID_DEVELOP
  IDENTIFIED BY "efluid"
  DEFAULT TABLESPACE EFLUID_DEVELOP_DATA
  TEMPORARY TABLESPACE TEMP
  PROFILE DEFAULT
  ACCOUNT UNLOCK;
  
 GRANT HERMES_OWNER_ROLE TO EFLUID_DEVELOP;
 ALTER USER EFLUID_DEVELOP DEFAULT ROLE ALL;

 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_BATCH_DATA;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_BATCH_INDEX;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_BLOB;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_DATA;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_INDEX;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_PARAM;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_UPGRADE;

Une fois la base de donnée dans cet état 2 possibilités :

 Créer un schéma à partir des DLL (= job de création des bases de TI)
 Importer une base de PARAM.

 Créer un schéma à partir des DLL (= job de création des bases de TI) 

 ATTENTION EN LANCANT CE SCRIPT, IL PREND LES INFOS DE CONNECTION BDD DEPUIS config-dev/.../framework2.properties, si ça pointe sur une BDD autre que localhost vous allez la tuer.

 Mettre les infos de connections dans votre framework2.properties avec les infos de création du premier script passé sur votre BDD en tant que SYS :

 JDBC_CONNECT_STRING=jdbc:oracle:thin:@127.0.0.1:1521/XEPDB1
 JDBC_USER=EFLUID_DEVELOP
 JDBC_PASSWORD=efluid

 ORACLE_TABLESPACE_DATA=EFLUID_DEVELOP_DATA
 ORACLE_TABLESPACE_INDEX=EFLUID_DEVELOP_INDEX
 ORACLE_TABLESPACE_BATCH_DATA=EFLUID_DEVELOP_BATCH_DATA
 ORACLE_TABLESPACE_BATCH_INDEX=EFLUID_DEVELOP_BATCH_INDEX
 ORACLE_TABLESPACE_PARAM=EFLUID_DEVELOP_PARAM
 ORACLE_TABLESPACE_BLOB=EFLUID_DEVELOP_BLOB
 ORACLE_TABLESPACE_MIGRATION=EFLUID_UPGRADE

 Lancer le script : scripts/UL-destructionBDD.sh. En vous mettant dans le projet que vous souhaitez. Efluid develop vous montera une BDD en develop. Efluid maintenance_13 une BDD en 13.22.100-SNAPSHOT etc...
 A noter en v13 ajouter dans votre commande -DskipCopyFramework2SqlInit=true

 Votre schéma est désormais prêt à l'utilisation

 Ajout du paramétrage ENEDIS sur votre base (EN COURS, NE PAS UTILISER) 

Prérequis
 Télécharger le zip de paramétrage disponible ici  https://cloud.uem-metz.fr/index.php/s/FrTJp9xMwHrKdjZ

 Une fois téléchargé dans la version souhaité (actuellement dispo 14.8.100 ou 13.11.1800), se connecter à la base de données que vous avez montée préalablement avec l'UL-destruction et les jouer dans l'ordre

 Procédure pour utiliser Oracle en mode docker [EN COURS]  

Prérequis
 Avoir le poste sous windows 10
 Avoir un compte sur docker hub : https://hub.docker.com

Installation* : 

 Avant de commencer l'installation, faire un clic gauche sur l'icone docker dans la barre des tâches et aller dans settings.
Il faut configurer le proxy afin de pouvoir sortir sur internet. (Vous pouvez récupérer ces paramètres sur la configuration proxy de votre naviguateur)

vignette|Proxy Docker.PNG

 FAQ 
 Problèmes connus 
 Message d'erreur :
 The target version 5.123.1100.73d8de4a does not match the target version in BDD 4.210.100.3c5ca7b5. You do not have the rights to make this update 
 Solution : 
 Si vous êtes dans le cas d'un upgrade, vous pouvez ajouter à la commande ./sql-upgrade.sh -DaccessMode=INT. Attention cependant à bien être conscient de la base de données ciblée.

 Message d'erreur : 
 [ERROR] Failed to execute goal com.efluid.utils.sql:sqlmigrator-maven-plugin:2.218.1:drop-init (ddl-dml-init) on project efluid-racine: Failed to execute SQLMigrator: java.sql.SQLException: ORA-12899: valeur trop grande pour la colonne "EFLUID_DEVELOP_14"."TACTIONPREDEFINIE"."LIBELLE" (réelle : 82, maximum : 80) 
 Solution : 
 L'installation oracle XE n'a pas du être faite avec la commande /v"CHAR_SET=WE8ISO8859P15". Les scripts sont donc interprété avec le mauvais encodage. Pour vérifier cette hypothèse, connecter vous à votre instance et lancer la commande : 
 select * from nls_database_parameters where parameter like '%SET%';
Si le résultat obtenu fait état de l'UTF8, il va falloir modifier ces valeurs. Pour ce faire :
 Ouvrir un sqlPlus
 Lorsqu'on vous demander en tant que quel utilisateur renseigner : sys as sysdba
 Renseigner le mot de passe saisie pendant l'installation
 Executer les lignes suivantes :
 SQL> shutdown immediate;
 SQL> startup restrict
 SQL> select name from v$database;
 SQL> ALTER DATABASE CHARACTER SET INTERNAL_USE WE8ISO8859P15;
 SQL> select value from NLS_DATABASE_PARAMETERS where parameter='NLS_CHARACTERSET';
 SQL> shutdown immediate;
 SQL> startup
 SQL> select value from NLS_DATABASE_PARAMETERS where parameter='NLS_CHARACTERSET';

En relancant votre montée de version, si vous êtes bien en ISO, tout devrait fonctionner.

 J'essaye de supprimer l'utilisateur EFLUID_DEVELOP :
 drop user EFLUID_DEVELOP cascade; 

Message d'erreur:
 ERROR at line 1: 
 ORA-28014: cannot drop administrative users 

Solution:
 alter session set "_oracle_script"=true; 
 drop user EFLUID_DEVELOP cascade;

 User dropped.

 Problème pour désinstaller/réinstaller OracleXE 
 Je dois réinstaller OracleXE mais au moment de réinstaller il me dit qu'un service Oracle existe encore : 
Suivre le tuto suivant : <!!!>Category:outil

 VM Oracle 19 
 Téléchargement 
 Télécharger VirtualBox puis l'image ova depuis Oracle (il faut un compte Oracle créable facilement) : 

 Installation 
Prérequis
 Les droits d'admin pour installer Virtual Box
 La virtualisation activée dans le BIOS 
Pour savoir si la virtualisation est activée : 
Fichier:virtualisationActivee.PNG
Si vous n'avez pas ça vous aurez l'erreur suivante au démarrage de la VM :
Fichier:erreurVTXBIOS.png
Si ce n'est pas le cas, appelez le 6000 pour qu'ils vous communique le mdp du BIOS (vérifiez qu'il y en a un avant) : 
Fichier:VTXBIOS.png
 désactiver Hyper-V virtualization 
Comme indiqué ici : https://superuser.com/questions/1153470/vt-x-is-not-available-but-is-enabled-in-bios
Lancer une console en mode administration (bouton droit) :
Fichier:consoleModeAdministration.PNG
Et lancer la commande :
 dism.exe /Online /Disable-Feature:Microsoft-Hyper-V 
Fichier:desactiverHyperV.PNG

 Procédure pour utiliser l'image  
 Importation et configuration de la VM 

 Importer la VM depuis VirtualBox

 Configurer le réseau en "hôte seulement" pour ne pas l'exposer sur le LAN mais juste sur votre machine

Fichier:OracleVMReseau.PNG

 Ajouter un dossier partagé entre votre machine et la VM :

OracleVMDossierPartage.PNG

 Configuration de la VM et de la BDD 

Vous trouverez sur [eRoom] Mes eRooms > efluid - Qualité Développement > Guides et procédures > guides base de données > VM Oracle 19 les 3 fichiers à placer dans votre home (/home/oracle). Le script SH et les fichiers SQL de configuration de la BDD. Il suffit de lancer le script ./configurationVM.sh. 

Les 3 fichiers doivent être dans le même répertoire

 Initialiser la BDD 

 Initialisez la BDD (toujours dans le terminal via sqlplus en SYS as SYSBA) :
 CREATE TABLESPACE EFLUID_DEVELOP_BATCH_DATA DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_DEVELOP_BATCH_DATA01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_BATCH_INDEX DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_DEVELOP_BATCH_INDEX01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_BLOB DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_DEVELOP_BLOB01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_DATA DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_DEVELOP_DATA01.dbf' SIZE 100M AUTOEXTEND ON NEXT 50M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_INDEX DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_DEVELOP_INDEX01.dbf' SIZE 100M AUTOEXTEND ON NEXT 50M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_DEVELOP_PARAM DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_DEVELOP_PARAM01.dbf' SIZE 500M AUTOEXTEND ON NEXT 500M MAXSIZE UNLIMITED;
 CREATE TABLESPACE EFLUID_UPGRADE DATAFILE
 '/u01/app/oracle/oradata/ORCLCDB/orcl/EFLUID_UPGRADE_01.dbf' SIZE 5G AUTOEXTEND OFF;

 alter session set "_ORACLE_SCRIPT"=true;
 CREATE ROLE HERMES_OWNER_ROLE NOT IDENTIFIED;
 -- Object privileges granted to HERMES_OWNER_ROLE
 GRANT EXECUTE ON DBMS_CRYPTO TO HERMES_OWNER_ROLE;
 GRANT EXECUTE ON DBMS_PARALLEL_EXECUTE TO HERMES_OWNER_ROLE;
 GRANT EXECUTE ON DBMS_RANDOM TO HERMES_OWNER_ROLE;
 -- System privileges granted to HERMES_OWNER_ROLE
 GRANT CREATE JOB TO HERMES_OWNER_ROLE;
 GRANT CREATE MATERIALIZED VIEW TO HERMES_OWNER_ROLE;
 GRANT CREATE SYNONYM TO HERMES_OWNER_ROLE;
 GRANT CREATE VIEW TO HERMES_OWNER_ROLE;
 GRANT QUERY REWRITE TO HERMES_OWNER_ROLE;
 -- Roles granted to HERMES_OWNER_ROLE
 GRANT CONNECT TO HERMES_OWNER_ROLE;
 GRANT RESOURCE TO HERMES_OWNER_ROLE;

 CREATE USER EFLUID_DEVELOP
 IDENTIFIED BY "efluid"
 DEFAULT TABLESPACE EFLUID_DEVELOP_DATA
 TEMPORARY TABLESPACE TEMP
 PROFILE DEFAULT
 ACCOUNT UNLOCK;

 GRANT HERMES_OWNER_ROLE TO EFLUID_DEVELOP;
 ALTER USER EFLUID_DEVELOP DEFAULT ROLE ALL;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_BATCH_DATA;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_BATCH_INDEX;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_BLOB;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_DATA;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_INDEX;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_DEVELOP_PARAM;
 ALTER USER EFLUID_DEVELOP QUOTA UNLIMITED ON EFLUID_UPGRADE;

 2 cas possible à partir de là 
 Cas 1 :  Script UL-Destruction / SQLUpgrade 

Lancer les différents scripts voulus en prenant bien soin de mettre votre ip LAN de votre VM dans framework2.propeties.

 Cas 2 : Import d'un dump 

Vous pouvez changer les différents noms (schéma, tablespaces) si vous adaptez le script d'initialisation ci-dessus et le parfile (qui est un fichier de mapping schéma source -> schéa dest).

Dans un sqlplus en SYSDBA :

create directory ORACLE_DIR as '/home/oracle';

GRANT READ,WRITE ON DIRECTORY ORACLE_DIR to system;

Ensuite faites un locate impdp, placez vous dans le dossier bin trouvé et lancez

impdp system/oracle@localhost:1521/orcl DIRECTORY=ORACLE_DIR DUMPFILE=EXPDP_RPAREDT6_PARAM_ERDF_FLD_14_14.9.100.RC1_5175.dmp LOGFILE=dump.log PARFILE=par.par SCHEMAS=PARAM_ERDF_FLD_14

 Utiliser la BDD 
 Faites un ifconfig et cherchez l'IP de de eth0, ceci sera l'ip à configurer dans votre IJ/SqlDev sur votre poste (pas sur la VM hein). 
ex: 192.168.56.101, si jamais il n'y a pas d'ip v4 : sudo dhclient
Ce qui donnera comme jdbc connect string : jdbc:oracle:thin:@192.168.56.101:1521/orcl

 Exemple de framework2.properties :

 JDBC_CONNECT_STRING=jdbc:oracle:thin:@192.168.56.101:1521/orcl 
 JDBC_USER=EFLUID_DEVELOP 
 JDBC_PASSWORD=efluid 

 ORACLE_TABLESPACE_DATA=EFLUID_DEVELOP_DATA 
 ORACLE_TABLESPACE_INDEX=EFLUID_DEVELOP_INDEX 
 ORACLE_TABLESPACE_BATCH_DATA=EFLUID_DEVELOP_BATCH_DATA 
 ORACLE_TABLESPACE_BATCH_INDEX=EFLUID_DEVELOP_BATCH_INDEX 
 ORACLE_TABLESPACE_PARAM=EFLUID_DEVELOP_PARAM 
 ORACLE_TABLESPACE_BLOB=EFLUID_DEVELOP_BLOB 
 ORACLE_TABLESPACE_MIGRATION=EFLUID_UPGRADE

 Infos diverses 
 Les mdp (root et compagnie) : oracle
 SID : orcl
 User (schéma) si utilisation du script d'initialisation ci-dessus : EFLUID_DEVELOP/efluid<!!!>Category:kubernetes

 Guide d'utilisation 
  Cheat Sheet kubernetes v1.23

  Cheat Sheet kubernetes

 Procédures internes 
 Rotation des certificats kubernetes

 Migration HELM V2 vers HELM V3

 Upgrade cluster kubernetes via kubeadm

 Répartition des usines Jenkins pour upgrade Kubernetes

 Upgrade cluster kubernetes via Ansible

 Sauvegarde/restauration d'un cluster Kubernetes (en cours)

  Migration Docker vers CRI-O

 Accès API server (kubectl) - authentification par certificat client, autorisations restreintes

 Formation 

 Liens internes 

 Playbook ansible pour kubernetes

 Présentation Kubernetes faite en réunion de services

 Liens externes 
  Documentation officielle de la version v1.23

  Documentation officielle<!!!> Jenkins
 PROD 

 https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/cjoc/

instance ID : d79e3954e18079156b0532b15d09c2a0

 REC
 
 https://rec-k8s-cje-cloudbees-core.efluid.uem.lan/cjoc/

instance ID : b1eff3fe62adbd0baffe592a82fb51d2

 Exploitation 

 Drainage des Pods dans le cluster  
 http://wikefluid.uem.lan/index.php/JenkinsCore_-_exploitation#Maintenance_d.27un_Worker_kubernetes_.28Master_and_worker_Jenkins_Core_product.29

Voir page : http://wikefluid.uem.lan/index.php/JenkinsCore_-_exploitation#Kubernetes

 Kubernetes platfom 

Evènement infra : 280813 

 Core Prod environment 
 Namespaces 
 cloudbees-core-prod
contient les pods du Cjoc, des masters et des pods servant a executer les jobs

 cloudbees-sidecar-injector 
Permet l'utilisation de certificats autosignés ou un rootCA custom en injectant les certificat bundles dans tous les containers de tous les pods schedulés par jenkins

 Serveurs 

    core_prod_1_project  Mem  CPU Pointeur DNS  pro-k8s-cje.uem.lan (192.168.106.128)  - - HA proxy en HA  lpcjelbaedt3 : (192.168.106.73)  4Go  1 vCPU HA proxy en HA  lpcjelbaedt4 (192.168.106.76)   4Go  1 vCPU Master-k8s  lpcjemtrkubedt1(192.168.106.53)  10Go  2 vCPU Master-k8s  lpcjemtrkubedt2(192.168.106.64)   10Go  2 vCPU Master-k8s  lpcjemtrkubedt3(192.168.106.69)   10Go  2 vCPU Worker-k8s  lpkubwkredt1.uem.lan (192.168.106.151)   502Go  64 vCPU Worker-k8s  lpkubwkredt2.uem.lan (192.168.106.152)  502Go  64 vCPU Worker-k8s  lpdrogonedt1 (192.168.119.78)  188 Go 96  Worker-k8s lpviseriedt1 (192.168.119.79)  188 Go96   Worker-k8s  lptyrionedt1 (192.168.118.8) 188 Go 28  Worker-k8s  lpcjewkredt1 (192.168.119.115)187 Go 176 Worker-k8s  lpcjewkredt2 (192.168.119.116)187 Go 176 Worker-k8s  lpcjewkredt3 (192.168.119.117) 187 Go 176 > Worker-k8s  lpcjewkredt4 (192.168.119.118) 187 Go  176  Montage NFS  /NFS_SHARE_KUBE_UL_DTA1_A06  -  -

Montage NFS pour l'ensemble des master/worker kubernetes: /nfsk8s

 Core Test environment 
 Namespace 
 cloudbees-core-test
contient les pods du Cjoc, des masters et des pods servant a executer les jobs

 cloudbees-sidecar-injector 
Permet l'utilisation de certificats auto signés ou un root CA custom en injectant les certificat bundles dans tous les containers de tous les pods schedulés par jenkins

 Serveurs 

    core_test_1_project  Mem  CPU pointeur DNS  rec-k8s-cje.uem.lan (192.168.106.129)  -  - HA proxy en HA  lrcjelbaedt3 : (192.168.106.126)  4Go  1 vCPU HA proxy en HA  lrcjelbaedt4 (192.168.106.127)   4Go  1 vCPU Master-k8s  lrcjemtrkubedt1 (192.168.106.18)   10Go  2 vCPU Master-k8s  lrcjemtrkubedt2 (192.168.106.33)   10Go  2 vCPU Master-k8s  lrcjemtrkubedt3 (192.168.106.48)   10Go  2 vCPU Worker-k8s  lrkubwkredt1 (192.168.106.80)   8 Go  2 vCPU Worker-k8s  lrkubwkredt2 (192.168.106.81)   8 Go  2 vCPU Worker-k8s  lrkubwkredt3 (192.168.106.135)   8 Go  2 vCPU Worker-k8s  lprhaegaedt1 (192.168.119.88) 188 Go 64 Montage NFS  /NFS_SHARE_KUBE_UL_DTA2_A06  -  -

Pour utiliser l'interface API kubernetes : se connecter root sur lrcjemtrkubedt1 ou 2 ou 3

Montage NFS pour l'ensemble des master/worker kubernetes: /nfsk8s

 Monitoring 
 Cluster K8s de prod 
 Dashboard de monitoring pertinents PROD (dashboards fournis par VCA): 
 http://pro-k8s-cje-grafana.efluid.uem.lan/d/efa86fd1d0c121a26444b636a3f509a8/kubernetes-compute-resources-cluster?orgId=1&refresh=10s&from=now-12h&to=now
 http://pro-k8s-cje-grafana.efluid.uem.lan/d/200ac8fdbfbb74b39aff88118e4d1c2c/kubernetes-compute-resources-node-pods?orgId=1&refresh=10s
 http://pro-k8s-cje-grafana.efluid.uem.lan/d/a87fb0d919ec0ea5f6543124e16c42a5/kubernetes-compute-resources-namespace-workloads?orgId=1&refresh=10s
 Etat système des nodes du cluster : http://pro-k8s-cje-grafana.efluid.uem.lan/d/hb7fSE0Zz/1-node-exporter-for-prometheus-dashboard-en-v20200628?orgId=1
 Vue Master Jenkins : http://pro-k8s-cje-grafana.efluid.uem.lan/d/8Z9-POHWz/jenkins-masters?orgId=1&refresh=30s&var-service=All&var-namespace=cloudbees-core-prod

 Accès dashboard Kubernetes Perf (mot de passe dans keypass pour token)
 https://pro-k8s-cje-dashboard.efluid.uem.lan

 Accès Grafana Kubernetes Perf (mot de passe dans keypass pour compte admin)
 http://pro-k8s-cje-grafana.efluid.uem.lan/login

 Monitoring HAProxy (mot de passe dans keypass pour compte admin)
 http://pro-k8s-cje.efluid.uem.lan:9000/haproxy_stats

 Cluster K8s de test 
 Accès dashboard Kubernetes Perf (mot de passe dans keypass pour token)
 https://rec-k8s-cje-dashboard.efluid.uem.lan

 Accès Grafana Kubernetes Perf (mot de passe dans keypass pour compte admin)
 http://rec-k8s-cje-grafana.efluid.uem.lan/login

 Monitoring HAProxy (mot de passe dans keypass pour compte admin)
 http://rec-k8s-cje.efluid.uem.lan:9000/haproxy_stats

 Jenkins Core product 
 Documentation 
 Architecture cloudbees core: https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-reference-architecture/ra-for-onprem/
 Demande de licence temporaire pour migration 
 https://support.cloudbees.com/hc/en-us/requests/180795

 Support : https://support.cloudbees.com/hc/en-us#
 guide de migration : https://docs.cloudbees.com/docs/cloudbees-core/latest/cje1-to-core/

 Migration 

Voir historique CJE-> JenkinsCore : https://wikefluid.efluid.uem.lan/index.php/JenkinsCore_-_points_techniques#Migration

 Montées de versions  
Les montées de versions de Jenkins Core se font via Helm. Des playbooks ont été développés pour permettre ces montées de versions :
 Scripts : http://wikefluid/docInstalleur/documentationScriptsOracleTools/kubernetes.html, paragraphe "Cloudbees Core".

A chaque montée de version, l'inventaire (REC puis PRO) doit être mis à jour avec les versions cibles des composants :
 Cloudbees Core version 2.xxx.y.z
 Cloudbees Sidecar Injector version a.b.c

Exemple : https://gerrit.efluid.uem.lan/c/oracleTools/+/178073

Lorsque les montées de versions sont trop éloignées, il est possible d'ouvrir un ticket d'assistance à la montée de version via le support. L'objectif est d'identifier avec le support les sauts de versions recommandés pour aller de la version courante à la dernière dispo, et d'avoir des infos sur les risques liés à la version.

 https://support.cloudbees.com/hc/en-us/articles/115001919212-Required-Data-Assisted-Update

Template à remplir disponible dans la room Process de Fabrication : http://WPEROOM4.uem.lan/eRoom/Prod6/ProcessFabricationEfluid/0_17187 ( efluid – Process de fabrication > Projets > Jenkins Enterprise - CJOC- CBC / AssistedUpdateTemplate.odt)

Pour identifier les dernières versions dispoinbles pour les 2 composants donnés plus haut, se connecter par exemple sur lrcjemtrkubedt1 en tant que root. Et consulter le repo HELM pour voir son contenu puis le mettre à jour :
 [root@lrcjemtrkubedt1 ~]# helm list -A
 NAME                            NAMESPACE                       REVISION        UPDATED                                         STATUS          CHART                                   APP VERSION
 cloudbees-core                  cloudbees-core-test             28              2021-04-28 15:20:02.540688858 +0200 CEST        deployed        cloudbees-core-3.23.4+53a9d5818d07      2.249.3.3
 cloudbees-sidecar-injector      cloudbees-sidecar-injector      1               2021-06-10 11:08:45.230278549 +0200 CEST        deployed        cloudbees-sidecar-injector-2.1.3        2.1.3

Pour mettre à jour le repo HELM et synchroniser avec les dernières versions dispos chez Cloudbees :

 [root@lrcjemtrkubedt1 ~]# export https_proxy=http://usine-logicielle:XXXXXXX@lpsrvpxy:8080/                       
 [root@lrcjemtrkubedt1 ~]# helm search repo cloudbees-core
 NAME                            CHART VERSION           APP VERSION     DESCRIPTION
 cloudbees/cloudbees-core        3.37.2+7390bf58e3ab     2.303.3.3       Enterprise Continuous Integration with Jenkins

 [root@lrcjemtrkubedt1 ~]# helm repo update
 Hang tight while we grab the latest from your chart repositories...
 ...Successfully got an update from the "cloudbees" chart repository
 ...Successfully got an update from the "stable" chart repository
 Update Complete. ⎈Happy Helming!⎈

 [root@lrcjemtrkubedt1 ~]# helm search repo cloudbees-core
 NAME                            CHART VERSION           APP VERSION     DESCRIPTION
 cloudbees/cloudbees-core        3.39.7+58091444e7ae     2.319.2.7       Enterprise Continuous Integration with Jenkins

 version 2.249.2.4 
https://usinelogicielle.slack.com/archives/C3S3RSRKP/p1614851701002500
CFOU : Résumé de la migration sur REC:
 j'ai installé la version 2.249.2.4 de cloudbees core. C'est la dernière version qui supporte encore helm2.
 la migration du cjoc s'est bien passée
 les masters encore dans l'ancienne version sont toujours up et  fonctionnent mais n'arrivent plus a communiquer avec le cjoc (je suis encore en train de regarder ce point pour éviter de devoir migrer tous les masters en même temps sur la prod)

 la migration des master se fait sans problèmes.
 exécution des tests de la sharedlib sont tous ok .

 > j'ai eu des erreurs 503 mais c'était des erreurs temporaires
 > des erreurs docker dû au fait que j'ai lancé les tests alors qu'ils tournaient deja sur la prod
 > il y a toujours une erreur slack mais on a les meme sur la prod => a fixer quand meme
  Slack#callSlackApiWithGet response : {"ok":false,"error":"method_deprecated","response_metadata":{"messages":["[ERROR] This method is retired and can no longer be used. Please use conversations.history instead. Learn more: https:\/\/api.slack.com\/changelog\/2020-01-deprecating-antecedents-to-the-conversations-api."]}}
- execution des tests de jenkins conf RAS
- les backups fonctionnent toujours
- j'ai créé deux jobs pour la copie de l'usine de prod vers l'usine REC (en se basant sur le dernier backup de l'usineValidationJenkins de prod) http://wikefluid.uem.lan/index.php/JenkinsCore_-_exploitation#Tester_la_migration_sur_la_REC

 version 2.319.2.9 
Opérations faites sur REC : 
 Préparation 
Depuis CJOC, Beekeeper Upgrade Assistant :
 Problèmes remontés sur plugins 
  https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/cjoc/beekeeper/plugins/
  => clic sur restart => redémarrage CJOC => OK sur REC et PROD

 Fix de soucis remontés sur UpdateCenter 
  https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/cjoc/beekeeper/updateCenter/ 
 => clic sur Fix => OK

https://rec-k8s-cje-cloudbees-core.efluid.uem.lan/cjoc/beekeeper/upgrade/ :
  Your current CloudBees CI Cloud Operations Center version is 2.249.3.3. Version 2.319.2.7 is available.

Demande d'assistance migration Cloudbees : https://docs.cloudbees.com/docs/cloudbees-common/latest/support-policies/cloudbees-ci
 Installer ce plugin : https://plugins.jenkins.io/support-core/

Procédure de migration faite en suivant la "Procedure avec un playbook Ansible"

Environnement REC -> 2.319.2.9-rolling

 Montée CJOC
 ansible-playbook -i inventory/rec_kub_cje_cluster --ask-vault-pass -e @inventory/rec_kub_cje_cluster.vault playbooks/kubernetes/helm-deploy-cloudbees-core.yml -l lrcjemtrkubedt1
 Montée sidecar-injector 2.1.3 -> 2.2.0

Dans inventaire ansible, modification de la version 2.1.3 en 2.2.0 :
 [carriers@lpswaulo1 inventory]$ grep sidecar *rec_kub_cje*
 rec_kub_cje_cluster:cloudbees_sidecar_injector_helm_chart_version=2.2.0
 rec_kub_cje_cluster:kubernetes_cloudbees_sidecar_injector_namespace=cloudbees-sidecar-injector
  
Reinstallation
  [carriers@lpswaulo1 ansible]$ pwd
  /home/D_NT_UEM/carriers/oracleTools/ansible
  [carrier@lpsrvprf11 ansible]$ ansible-playbook -i inventory/rec_kub_cje_cluster --ask-vault-pass -e @inventory/rec_kub_cje_cluster.vault playbooks/kubernetes/helm-deploy-cloudbees-sidecar-injector.yml -l lrcjemtrkubedt1

Après installation (04/09/2022) : 
 [root@lrcjemtrkubedt2 ~]# helm list -A
 NAME                            NAMESPACE                       REVISION        UPDATED                                         STATUS          CHART                                   APP VERSION
 cloudbees-core                  cloudbees-core-test             29              2022-02-06 16:55:40.378439667 +0100 CET         deployed        cloudbees-core-3.39.9+62d7abf51fb1      2.319.2.9
 cloudbees-sidecar-injector      cloudbees-sidecar-injector      2               2022-09-04 15:21:59.397349173 +0200 CEST        deployed        cloudbees-sidecar-injector-2.2.0        2.2.0
 ingress-nginx                   kube-system                     1               2022-08-23 09:59:41.328348044 +0200 CEST        deployed        ingress-nginx-4.1.4                     1.2.1
 kube-prometheus-stack           monitoring                      1               2022-08-23 10:22:22.181550171 +0200 CEST        deployed        kube-prometheus-stack-36.6.1            0.57.0
 kubernetes-dashboard            kube-system                     2               2022-08-23 10:26:27.8928038 +0200 CEST          deployed        kubernetes-dashboard-5.7.0              2.6.0
 metrics-server                  kube-system                     2               2022-08-23 10:27:44.972365397 +0200 CEST        deployed        metrics-server-3.8.2                    0.6.1
 nfs-subdir-external-provisioner kube-system                     1               2022-08-23 09:58:08.852146492 +0200 CEST        deployed        nfs-subdir-external-provisioner-4.0.16  4.0.2

 CJOC (migré)
 jobs  status mgration  commentaires tests job back up   ok  doivent etre resauvegarder
 usinevalidation (migré) 

Test post migration :
 Monter la version de l'usineValidationJenkins
 Lancer les tests suivants :
 SharedLibrary : https://rec-k8s-cje-cloudbees-core.efluid.uem.lan/usinevalidationjenkins/job/FsharedLibrary/job/jenkinsSharedLibrary.compile/
 JenkinsConfiguration  : tous les tests du doxxier https://rec-k8s-cje-cloudbees-core.efluid.uem.lan/usinevalidationjenkins/job/FjenkinsConfigurations/
 Validation process : tous les tests du dossier https://rec-k8s-cje-cloudbees-core.efluid.uem.lan/usinevalidationjenkins/job/FvalidationProcess/
Vérifier que les jobs de backup fonctionnent toujours

 jobs  status mgration  commentaires tests sharedLib   ok    tests jenkinsConf  ok 

PROD -> 2.319.2.9-rolling 

Migration CJOC le 02/04/2022, depuis serveur lpswaulo1 (user carriers) pour exécution du playbook :
  ansible-playbook -i inventory/pro_kub_cje_cluster --ask-vault-pass -e @inventory/pro_kub_cje_cluster.vault playbooks/kubernetes/helm-deploy-cloudbees-core.yml -l lpcjemtrkubedt1

 Change : https://gerrit.efluid.uem.lan/c/oracleTools/+/226647

 Suivi d'installation via playbook : http://WPEROOM4.uem.lan/eRoom/Prod6/ProcessFabricationEfluid/0_17515

Actions manuelles
Sur le CJOC : 
 Editer et sauvegarder les jobs de backup présents sous : https://pro-k8s-cje-cloudbees-core.efluid.uem.lan/cjoc/view/All/job/Fadmin/job/Fbackups/
 Reconfigurer la version de l'image utilisée pour l'agent JNLP par défaut, au niveau du CJOC, objet " kubernetes-shared-cloud ", "pod template : default-java". Par défaut, l'image est indiquée à latest. A chaque migration, penser à configurer la version qui vient d'être installée.
 Done

Sur chaque Managed Controller migré : 

 Avertissement sur NODE_NAME : fin du terme master -> built-in
1200px|
=> Applied

Etat de migration des Managed Controllers"
 usine  status migration usineAdministration   Done  UsineDeploiement    Done  UsineInfraExploitation  Done   UsineLogicielleCompilation  Done     UsineLogicielleCompilationTools   Done  UsineLogicielleNightlyBuild 14/04/2022  UsineLogicielleQuality  Done  UsineLogicielleRelease   Done   UsinePerformance   Done  UsineRecette   Done UsineValidationApplications   Done   UsineValidationComposants   14/04/2022 UsineValidationJenkins Done  

Problemes rencontrés pendant la migration 
 - problème des jobs lancés en double :
	=> du à IE, pas de problèmes si on utilise firefox (https://issues.jenkins.io/plugins/servlet/mobile#issue/JENKINS-66081)
  - problème de temps de lancement des pods :
	=> les usines ont été redémarrées et on a plus de problème pour le moment
  - problème org.csanchez.jenkins.plugins.kubernetes.pipeline.ContainerExecDecorator.websocketConnectionTimeout currently set at 30 seconds :
	=> Augmentation du nombre de connections (à 42)comme préconisé dans https://support.cloudbees.com/hc/en-us/articles/36)0054642231-Considerations-for-Kubernetes-Clients-Connections-when-using-Kubernetes-Plugin
  - java.lang.ClassNotFoundException: com.cloudbees.opscenter.server.model.ConnectedMasterPropertyDescriptor en boucle
	=> Apres un redémarrage des usine, l'erreur est présente qu'au lancement de l'usine puis on ne les voit plus
  - problème d'affichage en double dasn els jobs free style:
        => du au plugin publish over ssh 
            upgrade du plugin 1.20.1 -> 1.24<!!!>Category:outil
Category:BDD

 Développeurs 
 Installations 
 Sur son poste de DEV 
 En mode service Windows (démarrage automatique au démarrage du PC)
 Une demande d'installation de PG est à faire au 6000, car les droits admin sont nécessaires
 En mode zip (pas de service Windows, donc démarrage à la demande par le développeur)
 Installation à faire manuellement par le développeur, en récupérant le zip sur le site officiel : https://www.enterprisedb.com/download-postgresql-binaries
 Via Docker
 Pour les postes sous Windows 10 avec Docker installé (demande 6000 sinon pour l'obtenir) il est aussi possible d'utiliser les images officielles : https://hub.docker.com/_/postgres

 Documentation 
 Doc du projet de migration Oracle -> PG : http://wperoom4.uem.lan/eRoom/Production/GestionProjetEfluid/0_1f527e
 Syntaxe PG vs Oracle : http://wperoom4.uem.lan/eRoom/Production/GestionProjetEfluid/0_203f6c
 Règles de code pour la syntaxe BDD : Règles de code SQL pour la compatiblité multi-SGBD
 Conventions SQLMigrator

 Utilisation d'une BDD mise à disposition par l'équipe PERF 

 Bases actuellement disponibles : 
 Base de données  Port  Schéma  version PG de15pgedt1  5432  dev_pgsql_fld  12.4

 Utilisation 
 Connexion à une BDD PG : 
 Connexion à une base de données Postgres sous Sqldeveloper
 Fonctionne aussi nativement avec IntelliJ : https://www.jetbrains.com/help/idea/connecting-to-a-database.html#connect-to-postgresql-database

 Infrastructure 
 Installation du moteur
 Création d'une instance
 Création d'une base de données
 Création d'un role et schema
 Sauvegarde/Export d'un schema
 Restauration/import d'un schema

 Performances 
 Outils 
 TemBoard :  permet de centraliser la supervision et l’administration d’un parc d’instances PostgreSQL.
 https://temboard.efluid.uem.lan  (les identifiants de connexion sont dans le keepass)
 pgAdmin : interface web d'administration pour postgreSQL
 PoWA : analyseur de trafic PostgreSQL permettant une analyse en temps réel et à postériori de l'activité de la BDD, et notamment des requêtes SQL
 https://powa.efluid.uem.lan (les identifiants de connexion sont dans le keepass)
 pg_profile : extension pour PostgreSQL permettant d'effectuer une analyse à postériori de l'activité d'une BDD, via la génération de rapports de même type que les rapport AWR côté Oracle
 pgBadger : analyseur de log pour PostgreSQL permettant de générer des rapports html présentant l'activité de la BDD.
 pg_activity : Permet de lister les sessions en cours sur une instance
 [postgres@ldbddedt1 ~]$ pg_activity -p $portInstance
 pev2 : Permet de visualiser les plans d'executions des requêtes SQL
 http://pev2.efluid.uem.lan:8001/

 Configuration 

 Surcharge de configuration par défaut 
Sur le serveur hébergeant la base de donnée PG, le fichier postgresql.conf contient les valeurs par défaut mis en place par le socle DALIBO.
Il est parfois nécessaire de surcharger certaines valeurs lors de tests de performances. Pour ce faire il faut ajouter en fin de fichier postgresql.conf afin d'inclure une surcharge de configuration. 

Exemple:
   include 'surcharge.conf'

NOTE: Toutes les clés alors présentes dans le fichier surcharge.conf sont alors prioritaires par rapport au valeurs du fichier postgresql.conf.

 Configuration spécifique aux performances 

Exemple de configuration:
  # Activation du module auto_plan
  shared_preload_libraries = 'pg_qualstats, pg_stat_kcache, pg_stat_statements, pg_wait_sampling, powa, auto_explain'
  auto_explain.log_min_duration = '0'
  auto_explain.log_analyze = true
  auto_explain.log_timing = true
  auto_explain.log_buffers = true
  # Activation log_statement 
  log_statement = 'mod'

module auto_plan

Voir plus de detail ici: https://docs.postgresql.fr/10/auto-explain.html

Ce module permet de logger les plans d'exécutions de toutes les requêtes joués dans les fichier de log postgres. Cela permet par le suite à pgBadger d'afficher les plans d'exécutions dans son rapport.

log_statement 

Voir plus de detail ici: https://www.postgresql.org/docs/13/runtime-config-logging.html

Permet de spécifier quels requêtes sont loggées. les valeurs possibles utilisées par l'équipes perf sont 'ddl' par défaut et 'mod'.
 ddl log les CREATE, ALTER, DROP, SELECT
 mod log en plus les INSERT, UPDATE, DELETE<!!!>Category:SQL

Outils
 sql developper

Liens utiles

  Règles de code SQL pour assurer la compatibilité entre plusieurs systèmes de SGBDR
 Les fonctions analytiques
 Livrer un script SQL
 Base de données clients
 Bases de données de développement
 Trucs et astuces SQL
 Fil d'annonce ds scripts SQL passés sur les bases de développement
 Procédure_de_validation_d'anonymisation_des_param%C3%A8tres_techniques
 Résumé des opérations possibles
 Analyse automatique de logs SQL

Scripts SQL récurrents

 Affaire générique
 Contrat
 Mensualisation
 Offre
 Requêteur
 Consommation
 Workflow

 Vider le cache Oracle 
alter system flush shared_pool;
alter system flush buffer_cache;

http://www.aide-oracle.net/2009/04/vidage-de-cache.html

 Comparaison de chaîne de caractères 
 UTL_MATCH Levenshtein Distance
 Soundex (phonétique)
 Conversion du jeu de caractères
 Ne récupérer que les chiffres : regexp_replace regexp_replace(chaîne, '[^[:digit:]]', ' ')
 Comparer avec une expression régulière : REGEXP_LIKE

  Comparaison et concaténation 
 Attention à bien forcer au niveau SQL le typage pour avoir des deux côté de la comparaison le même type => TO_CHAR(xxx) si jamais on compare une chaine de caractère et une concaténation de VARCHAR et types numériques.

Les jointures
Inner Join
Cette jointure renvoie les lignes lorsqu’il y a au moins une correspondance dans les 2 tables.

Inner Join|250px

Left Join
Cette jointure est un alias de Left Outer Join et retourne toutes les lignes de la table de gauche en lien avec les lignes correspondantes de la table de droite. S'il n'y a pas de colones correspondantes dans la table de droite, cela renvoie des valeurs NULL.

Left Join|250px

Right Join
Cette jointure est un alias de Right Outer Join et retourne toutes les lignes de la table de droite en lien avec les lignes correspondantes de la table de gauche. S'il n'y a pas de colones correspondantes dans la table de gauche, cela renvoie des valeurs NULL.

Right Join|250px

 Fonctions d'aggrégation 
 Concaténation

 Gestion du format des dates 
Liste des options pour formater une date :
http://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements004.htm

Modifier l'affichage par défaut du format des dates : 
La commande ci-dessous permet d'afficher des colonnes de type date au format souhaité par défaut.
Elle permet également d'exécuter des requêtes en passant directement la date au format String sans utiliser des to_date(...). 
Exemple : 
 ALTER SESSION SET NLS_DATE_FORMAT='dd/mm/yyyy hh24:mi:ss';
 update maTable set maDate='21/01/2011 10:01:15' /* ça fonctionne */

 Les séquences 
 Modifier le nextval d'une séquence 
http://fadace.developpez.com/oracle/sequences/

 Il n'y a pas de commande ALTER SEQUENCE SEQ_TEST MODIFY LAST_NUMBER=1000. Afin d'incrémenter la séquence, il y a 3 options, dont certaines sont plus élégantes que les autres
 Exécuter des nextval successifs
 Supprimer, puis recréer la séquence
 Modifier l'incrément
declare nseq number, nincr number ;
SELECT SEQ_MaSequence.nextval, increment  INTO nseq, nincr FROM USER_SEQUENCES WHERE SEQUENCE_NAME='SEQ_MASEQUENCE';
ALTER SEQUENCE SEQ_MASEQUENCE INCREMENT BY &NouvelleValeur - nseq ;
SELECT SEQ_MaSequence.nextval from dual ;
ALTER SEQUENCE SEQ_MASEQUENCE INCREMENT BY nincr ;

Modèles de scripts
Requêtes pratiques
Script pour rechercher un id référencé dans toute table, toute colonne
(testé sur toad et sql developper (dernière version 3.0.04))
set autoprint on;
set serveroutput ON;
DECLARE
  CURSOR CUR_ IS 
    SELECT atc.TABLE_NAME, atc.COLUMN_NAME
    FROM user_tab_columns atc
    where   (   atc.COLUMN_NAME like '%%ID'
            or  atc.COLUMN_NAME = 'SRC'
            or  atc.COLUMN_NAME = 'SOURCE'
            or  atc.COLUMN_NAME = 'DEST' );

  Trouve number;
  sqlstr varchar2 (4000);
  SearchValue varchar2(255):='CRITNAT8'; --  rentrer l’id recherché ici
BEGIN
  FOR C IN CUR_ LOOP 
    sqlstr:='select /*+ PARALLEL('||C.TABLE_NAME||') */ count(*) from '||C.TABLE_NAME||' where '|| c.COLUMN_NAME ||' = '''|| SearchValue ||'''';
    begin
      execute immediate sqlstr INTO Trouve;
    exception when others then Trouve:=0;
    end;
    IF Trouve<>0 then
      dbms_output.put_line('Valeur trouvée dans '||C.TABLE_NAME||'.'||c.COLUMN_NAME ||' par la requete '|| sqlstr);
    end IF;
  END LOOP;
END;
Version avec inhibition du parallélisme en cas de présence d'un index sur la colonne visée et non prise en compte des tables batch et vues du requêteur :
set autoprint on;
set serveroutput on;
declare
  cursor cur is 
    select UTC.TABLE_NAME, UTC.COLUMN_NAME, listagg(UIDD.INDEX_NAME, ';') within group (order by UTC.TABLE_NAME, UTC.COLUMN_NAME) INDEX_NAME
      from USER_TAB_COLUMNS UTC
        left join USER_INDEXES_DESCRIPTION UIDD on UIDD.TABLE_NAME = UTC.TABLE_NAME
                                                  and UIDD.COLUMN_NAME = UTC.COLUMN_NAME
      where UTC.TABLE_NAME not like 'BATCH_%'
        and UTC.TABLE_NAME not like 'MV_%'
        and UTC.TABLE_NAME not like 'VRS_%'
        and UTC.TABLE_NAME not like 'VRP_%'
        and (UTC.COLUMN_NAME like '%%ID'
        or UTC.COLUMN_NAME = 'SRC'
        or UTC.COLUMN_NAME = 'SOURCE'
        or UTC.COLUMN_NAME = 'DEST')
      group by UTC.TABLE_NAME, UTC.COLUMN_NAME;
  v_trouve number;
  v_requete varchar2(4000);
  v_valeurRecherchee varchar2(255) := '#TeIJ9'; --  rentrer l’id recherché ici
begin
  for enr in cur loop 
    if enr.INDEX_NAME is null then
      v_requete := 'select /*+ ENABLE_PARALLEL_DML PARALLEL('||enr.TABLE_NAME||') */ count(*) from '||enr.TABLE_NAME||' where '|| enr.COLUMN_NAME ||' = '''|| v_valeurRecherchee ||'''';
    else
      v_requete := 'select count(*) from '||enr.TABLE_NAME||' where '|| enr.COLUMN_NAME ||' = '''|| v_valeurRecherchee ||'''';
    end if;
    begin
      execute immediate v_requete into v_trouve;
    exception
      when others then v_trouve := 0;
    end;
    if v_trouve > 0 then
      dbms_output.put_line('Valeur trouvée dans '||enr.TABLE_NAME||'.'||enr.COLUMN_NAME ||' par la requete '|| v_requete);
    end if;
  end loop;
end;
/

Le résultat indique la table et la colonne concernée pour chaque ligne trouvée ainsi qu'un bout de requête permettant d'aller voir directement ce qui se passe
Valeur trouvée dans TVALEURENUMERE.CRITERE_ID par la requete select count(*) from TVALEURENUMERE where CRITERE_ID='CRITNAT8'

Script pour trouver toutes les modification apportée en base par une action TP
SET autoprint ON;
SET serveroutput ON;
DECLARE
  cursor cur IS 
    SELECT UTC.TABLE_NAME, UTC.COLUMN_NAME
      FROM USER_TAB_COLUMNS UTC
      WHERE UTC.TABLE_NAME NOT LIKE 'BATCH_%'
        AND UTC.TABLE_NAME NOT LIKE 'MV_%'
        AND UTC.TABLE_NAME NOT LIKE 'VRS_%'
        AND UTC.TABLE_NAME NOT LIKE 'VRP_%'
        AND (UTC.COLUMN_NAME = 'DATEMODIFICATION'
        OR UTC.COLUMN_NAME = 'DATECREATION')
      GROUP BY UTC.TABLE_NAME, UTC.COLUMN_NAME;
  v_trouve NUMBER;
  v_requete varchar2(4000);
  v_dateMin varchar2(255) := '05/09/2019 10:10'; -- ? date au format dd/mm/yyyy hh24:mi
  v_dateMax varchar2(255) := '05/09/2019 10:25'; -- ? date au format dd/mm/yyyy hh24:mi
BEGIN
  FOR enr IN cur loop 
      v_requete := 'select /*+ ENABLE_PARALLEL_DML PARALLEL('||enr.TABLE_NAME||') */ count(*) 
                      from '||enr.TABLE_NAME||' 
                      where '|| enr.COLUMN_NAME ||' > to_date('''|| v_dateMin ||''', ''dd/mm/yyyy hh24:mi'') 
                        and '|| enr.COLUMN_NAME ||' < to_date('''|| v_dateMax ||''', ''dd/mm/yyyy hh24:mi'')';
    BEGIN
      EXECUTE immediate v_requete INTO v_trouve;
    exception
      WHEN others THEN v_trouve := 0;
    END;
    IF v_trouve > 0 THEN
      dbms_output.put_line('Valeur trouvée dans '||enr.TABLE_NAME||'.'||enr.COLUMN_NAME ||' par la requete '|| v_requete);
    END IF;
  END loop;
END;
/

Script pour insérer ou updater une valeur
MERGE INTO NOMDETABLE OBJET 
USING (SELECT
	valeur1 as nomcolonne1
	valeur2 as nomcolonne2
	valeur3 as nomcolonne3...
       FROM DUAL) NEW_OBJECT 		-- valeur finale de l'objet que l'on veut soit insérer, soit mettre à jour
ON (comparaison de valeur entre l'OBJET s'il existe et la nouvelle valeur NEW_OBJECT) –- condition qui permet de distinguer l’insert de l’update
WHEN NOT MATCHED THEN 			–- si la condition ON () est non remplie  ? on fait un INSERT classique en utilisant les nouvelles valeurs
INSERT (nomcolonne1           , nomcolonne2           , nomcolonne3...  )
VALUES (NEW_OBJECT.nomcolonne1, NEW_OBJECT.nomcolonne2, NEW_OBJECT.nomcolonne3...)
WHEN MATCHED THEN 			--si la condition  ON () est      remplie ? on fait un UPDATE classique en utilisant les nouvelles valeurs
UPDATE SET 
  OBJET.nomcolonne1 = NEW_OBJECT.nomcolonne1,
  OBJET.nomcolonne2 = NEW_OBJECT.nomcolonne2,
  OBJET.nomcolonne3 = NEW_OBJECT.nomcolonne3...
;

Exemple : Requete SQL Exemple Merge Into

Table
Script pour renommer la colonne MACOLONNE de la table MATABLE
alter table MATABLE rename column MACOLONNE to MACOLONNE3;

Script pour modifier le type de la colonne MACOLONNE de la table MATABLE
 Cas 1 : augmentation de la taille de la colonne

alter table MATABLE modify
(
    MACOLONNE VARCHAR2(120)
);

 Cas 2 : diminution de la taille de la colonne
-- on ajoute une nouvelle colonne ayant la bonne taille
alter table MATABLE add ( MACOLONNE_NEW VARCHAR2(50) );

-- on met à jour la nouvelle colonne avec les données de l'ancienne
-- la fonction pour "diminuer" les valeurs existantes sont
-- chaine de caractère : substr
-- nombre : trunc
update MATABLE set MACOLONNE_NEW = substr(MACOLONNE, 1, 50);
commit;

-- on vide l'ancienne colonne pour pouvoir la supprimer
update MATABLE set MACOLONNE = NULL;
commit;

-- on supprime l'ancienne colonne
alter table MATABLE drop (MACOLONNE);

-- on renomme la nouvelle colonne
alter table MATABLE rename column MACOLONNE_NEW to MACOLONNE;

 Cas 3 : modification du type de la colonne (exemple : passage d'un VARCHAR2 à un NUMBER)
-- on ajoute une nouvelle colonne ayant la bonne taille
alter table MATABLE add ( MACOLONNE_NEW NUMBER(2) );

-- on met à jour la nouvelle colonne avec les données de l'ancienne
-- la fonction pour "migrer" les valeurs existantes sont
-- chaine de caractère --> nombre : to_number
-- nombre --> chaine de caractère : to_char
-- chaine de caractère --> date : to_date
update MATABLE set MACOLONNE_NEW = to_number(MACOLONNE);
-- ou règle particulière
update MATABLE set MACOLONNE_NEW = 1 where MACOLONNE='TOTO';

commit;

-- on vide l'ancienne colonne pour pouvoir la supprimer
update MATABLE set MACOLONNE = NULL;
commit;

-- on supprime l'ancienne colonne
alter table MATABLE drop (MACOLONNE);

-- on renomme la nouvelle colonne
alter table MATABLE rename column MACOLONNE_NEW to MACOLONNE;

 Script générique de migration 
WHENEVER SQLERROR EXIT SQL.SQLCODE

DEFINE TBS_DATA=&1
DEFINE TBS_INDEX=&2
DEFINE TBS_PARAM=&3
DEFINE TBS_BLOB=&4

DEFINE TBS_TMP='MIG_EFLUID12_TM' -- nom du TBS a utiliser pour la migration
DEFINE TBS_DATA=&TBS_TMP

SET SERVEROUTPUT ON

/**
 * procedure stockee permettant d'afficher la requete executee dans la log, son temps d'execution ainsi que
 * le nombre de lignes affectees. Ne fonctionne pas avec les requetes retournant un resultat grace a une
 * clause INTO ou BULK COLLECT INTO. Si quelqu'un a une idee a ce sujet... x)
 * p_sql_request : la requete
 * p_debug_mode  : Optionnel (TRUE par defaut) . TRUE si on veut afficher la requete sans l'executer, FALSE sinon.
 */
CREATE OR REPLACE PROCEDURE log_and_execute (p_sql_request CLOB, p_debug_mode BOOLEAN DEFAULT TRUE) AUTHID CURRENT_USER IS
v_sql_request_start_time PLS_INTEGER;
v_sql_request_end_time   PLS_INTEGER;
v_rowcount               PLS_INTEGER;
v_log_message            CLOB;
v_sql_request_type       VARCHAR2(6);
BEGIN
  DBMS_OUTPUT.PUT_LINE('INFO      : ' || p_sql_request);
  
  IF NOT p_debug_mode THEN
    v_sql_request_start_time := DBMS_UTILITY.GET_TIME;
    v_sql_request_type := SUBSTR(p_sql_request, 1, 6);
    
    EXECUTE IMMEDIATE p_sql_request;
    
    v_rowcount := SQL%ROWCOUNT;
    COMMIT;
    v_sql_request_end_time   := DBMS_UTILITY.GET_TIME;
    v_log_message := 'Requete terminee en ' || ((v_sql_request_end_time - v_sql_request_start_time) / 100) || ' seconde';
    
    IF ((v_sql_request_end_time - v_sql_request_start_time) / 100) >= 2 THEN
      v_log_message := v_log_message || 's';
    END IF;
    
    v_log_message := v_log_message || '. ' || v_rowcount; 
  
    IF v_rowcount > 1 THEN
      v_log_message := v_log_message || ' lignes' || 
        CASE v_sql_request_type 
          WHEN 'INSERT' THEN 
            ' inserees' 
          WHEN 'UPDATE' THEN 
            ' mises a jour' 
          WHEN 'DELETE' THEN 
            ' supprimees' 
          WHEN 'SELECT' THEN 
            ' selectionnees'
          ELSE
            ' affectees'
        END || '.';
    ELSE
      v_log_message := v_log_message || ' ligne' || 
        CASE v_sql_request_type 
          WHEN 'INSERT' THEN 
            ' inseree' 
          WHEN 'UPDATE' THEN 
            ' mise a jour' 
          WHEN 'DELETE' THEN 
            ' supprimee' 
          WHEN 'SELECT' THEN 
            ' selectionnee'
          ELSE
            ' affectee'
        END || '.';
    END IF;
    
    DBMS_OUTPUT.PUT_LINE('INFO      : ' || v_log_message);
  END IF;
END;
/

DECLARE
  v_sql_request_insert_part LONG;
  v_sql_request_select_part LONG;
  
  TYPE table_column_list IS TABLE OF VARCHAR2(30);
  TYPE table_column_list_item IS RECORD (
    column_list table_column_list,
    override_global BOOLEAN DEFAULT FALSE
  );
  
  TYPE table_list IS TABLE OF table_column_list_item INDEX BY VARCHAR2(30);
  v_table_list table_list;
  v_table_column_list table_column_list;
  
  TYPE global_table_list IS TABLE OF VARCHAR2(30);
  TYPE global_table_column_list IS TABLE OF VARCHAR2(30);
  v_global_table_list global_table_list;
  v_global_column_list global_table_column_list;
  
  v_table VARCHAR2(30);
  v_column VARCHAR2(30);
  
  TYPE junction_table_column_list IS TABLE OF LONG INDEX BY VARCHAR2(30);
  v_junction_table_column_list junction_table_column_list;
  v_junction_table VARCHAR2(30);
  
  TYPE junction_column_list IS TABLE OF VARCHAR2(30) INDEX BY VARCHAR2(4);
  v_junction_column_list junction_column_list;
  v_junction_table_column VARCHAR2(30);
  v_junction_direction VARCHAR2(30);
  
  TYPE column_special_condition IS RECORD (
    condition LONG,
    override_global BOOLEAN DEFAULT FALSE
  );
  
  v_override_global BOOLEAN;
  TYPE col_special_condition_list IS TABLE OF column_special_condition INDEX BY VARCHAR2(30);
  TYPE special_condition_list IS TABLE OF col_special_condition_list INDEX BY VARCHAR2(30);
  v_special_condition_list special_condition_list;
  v_global_special_condition LONG;
  v_parsed_global_special_cnd LONG;
  v_special_condition LONG;

  v_tmp_table_suffix LONG;
  v_sql_request LONG;
  v_sql_req_ins_tmp_tbl_sel_part LONG;
  v_sql_req_ins_tmp_tbl_ins_part LONG;
  v_sql_req_ins_tmp_tbl_frm_part LONG;
  v_table_size PLS_INTEGER;
  v_free_space PLS_INTEGER;
  v_original_table_line_count PLS_INTEGER;
  v_new_table_line_count PLS_INTEGER;
  v_error_text LONG;
  e_not_enough_free_space EXCEPTION;
  e_different_line_count  EXCEPTION;
  TYPE pk_index_table IS TABLE OF LONG INDEX BY PLS_INTEGER;
  v_pk_index_table pk_index_table;
  
  TYPE constraint_table IS TABLE OF LONG INDEX BY PLS_INTEGER;
  v_constraint_table constraint_table;
  v_constraint LONG;
  
  TYPE index_table IS TABLE OF LONG INDEX BY PLS_INTEGER;
  v_index_table index_table;
  v_index LONG;
  TYPE rebuild_index_sql_req_table IS TABLE OF LONG INDEX BY PLS_INTEGER;
  v_rebuild_index_sql_req_table rebuild_index_sql_req_table;
  v_table_treatment_start_time POSITIVE;
  v_table_treatment_end_time   POSITIVE;
  v_treatment_start_time       POSITIVE;
  v_treatment_end_time         POSITIVE;
  
  v_skip_insert BOOLEAN DEFAULT FALSE;
  v_debug_mode BOOLEAN DEFAULT TRUE;
BEGIN
  v_treatment_start_time := DBMS_UTILITY.GET_TIME;
  --------------------------
  -- parametres du script --
  --------------------------
  
  -- s'il vaut TRUE, alors aucune requête ne sera éxécutée. Seule subsisteront
  -- les traces dans le log. C'est le comportement par défaut.
  -- v_debug_mode := FALSE;
  DBMS_OUTPUT.PUT_LINE('DEBUG_MODE : ' || CASE v_debug_mode WHEN TRUE THEN 'TRUE' ELSE 'FALSE' END);
  
  -- s'il vaut TRUE, alors on passe l'etape d'insertion des liens dans la
  -- table de liens. FALSE par defaut.
  -- v_skip_insert := TRUE;
  DBMS_OUTPUT.PUT_LINE('SKIP_INSERT : ' || CASE v_skip_insert WHEN TRUE THEN 'TRUE' ELSE 'FALSE' END);
  
  -- suffixe des tables temporaires
  v_tmp_table_suffix := '_MIG';
  
  -- nom de la table de liens
  v_junction_table := 'TUTILISATEURINFO';
  
  -- liste des 2 composantes du lien, avec indication du sens
  -- dans lequel on insere les liens manquants, et dans lequel
  -- on fait la migration. Les valeurs trouvees dans les attributs
  -- des tables à migrer seront inserees dans la colonne 'FROM'
  -- de la table de liens. Les valeurs des attributs des tables
  -- a migrer seront remplacees par celles de la colonne 'TO'
  -- de la table de liens
  v_junction_column_list('FROM') := 'IDENTIFIANTLOG';
  v_junction_column_list('TO')   := 'ID';
  
  -- liste des colonnes de la table de liens, avec pour chacune
  -- d'elles la valeur a inserer. Ces valeurs sont utilisees
  -- pour l'insertion des n-uplets manquants dans la table de liens.
  -- La colonne qui n'est pas indiquee est valorisee par une sous-requete
  v_junction_table_column_list('ID')               := 'GestionIdentifiant.genereIdBase62 (''useruniqueid'', '''')';
  v_junction_table_column_list('ETATOBJET')        := '0';
  v_junction_table_column_list('DATEMODIFICATION') := 'NULL';
  v_junction_table_column_list('DATECREATION')     := 'CURRENT_DATE';
  v_junction_table_column_list('DATESUPPRESSION')  := 'NULL';
  
  --/*
  -- liste des tables a migrer
  v_global_table_list := global_table_list(
    'TFACTURE',
    'TCONTEXTEFACTURATION',
    'TDEVIS',
    'TARTICLE',
    'TLIGNEHISTORIQUE',
    'TCHAPITRE'
  );
  
  -- liste des attributs a migrer communs a
  -- toutes les tables
  v_global_column_list := global_table_column_list(
    'ACTEURCREATION', 
    'ACTEURMODIFICATION', 
    'ACTEURSUPPRESSION'
  );
  --*/
  
  -- condition s'appliquant a toutes les colonnes de toutes
  -- les tables a migrer lors de la selection des valeurs
  -- a ajouter dans la table de liens
  v_global_special_condition := 'AND $COLUMN_NAME LIKE ''%|%'' OR $COLUMN_NAME LIKE ''Injecteur%''';
  
  /*
  -- liste permettant d'ajouter une condition specifique a chacune des colonnes
  -- des tables a migrer. Le champ override_global permet de specifier si
  -- cette condition doit surcharger la condition globale. Par defaut, ce champ
  -- est a FALSE, et la condition specifique vient donc s'ajouter en plus de la
  -- condition globale
  v_special_condition_list('TFACTURE')('ACTEURCREATION').condition       := 'OR ACTEURCREATION LIKE ''GBA95219_%''';
  v_special_condition_list('TFACTURE')('ACTEURMODIFICATION').condition   := 'OR ACTEURMODIFICATION LIKE ''GBA95219_%''';
  v_special_condition_list('TFACTURE')('ACTEURSUPPRESSION').condition    := 'OR ACTEURSUPPRESSION LIKE ''GBA95219_%''';
  --*/
  
  /*
  -- indique des colonnes a migrer specifiques a une table. Si override_global est a
  -- TRUE, alors ces colonnes remplaceront les colonnes definies dans
  -- v_global_column_list. Sinon, elles s'ajouteront a ces dernieres.
  v_table_list('TCONTEXTEFACTURATION').column_list := table_column_list(
    'FOO',
    'BAR'
  );
  v_table_list('TCONTEXTEFACTURATION').override_global := TRUE;
  --*/
  
  --/*
  -- on parcours la liste des tables a migrer renseignees dans v_global_table_list.
  -- Pour chacune d'entre elles, on cree une liste de colonnes vide et on regarde
  -- si il y en a une de definie
  IF v_global_table_list IS NOT NULL THEN
    FOR i IN 1..v_global_table_list.COUNT LOOP
      v_table_column_list := table_column_list();
      -- si une liste de colonnes est deja definie pour la table courant dans v_table_list
      -- et si cette liste n'a pas son attribut override_global a FALSE, ou bien s'il
      -- n'existe tout simplement pas d'entree dans v_table_list pour la table courante...
      IF (v_table_list.EXISTS(v_global_table_list(i)) AND NOT v_table_list(v_global_table_list(i)).override_global)
        OR NOT v_table_list.EXISTS(v_global_table_list(i)) THEN
        -- on ajoute les colonnes de v_global_column_list dans la liste de colonnes
        -- nouvellement creee
        FOR j IN 1..v_global_column_list.COUNT LOOP
          v_table_column_list.EXTEND;
          v_table_column_list(j) := v_global_column_list(j);
        END LOOP;
        -- on verifie qu'il y a une entree dans v_table_list pour la table courante
        IF v_table_list.EXISTS(v_global_table_list(i)) THEN
          -- si oui, on ajoute les colonnes definies dans cette entree a la suite de celles
          -- copiees de v_global_column_list dans v_table_column_list
          FOR j IN (v_global_column_list.COUNT + 1)..(v_global_column_list.COUNT + v_table_list(v_global_table_list(i)).column_list.COUNT) LOOP
            v_table_column_list.EXTEND;
            v_table_column_list(j) := v_table_list(v_global_table_list(i)).column_list(j - v_global_column_list.COUNT);
          END LOOP;
        END IF;
        -- on affecte la nouvelle liste a l'entree correspondant a la table courante
        -- dans v_table_list
        v_table_list(v_global_table_list(i)).column_list := v_table_column_list;
      END IF;
    END LOOP;
  END IF;
  
  v_table := v_table_list.FIRST;
  v_sql_request_insert_part := 'INSERT INTO ' || v_junction_table || ' (';
  v_junction_table_column := v_junction_table_column_list.FIRST;
  
  v_sql_request_select_part := 'SELECT ';
  
  -- boucle qui cree la requete d'insertion des n-uplets dans la table de liens.
  LOOP
    -- pour chacune des colonnes de la table de liens...
    EXIT WHEN v_junction_table_column IS NULL;
    -- si on est pas a la premiere iteration, on ajoute une virgule pour separer
    -- les noms des colonnes
    IF v_junction_table_column <> v_junction_table_column_list.FIRST THEN
      v_sql_request_insert_part := v_sql_request_insert_part || ', ';
      v_sql_request_select_part := v_sql_request_select_part || ', ';
    END IF;
    
    -- on ajoute le nom de la colonne sur laquelle on itere dans la partie
    -- INSERT et dans la partie SELECT de la requete d'insertion dans la table de
    -- liens
    v_sql_request_insert_part := v_sql_request_insert_part || v_junction_table_column;
    v_sql_request_select_part := v_sql_request_select_part || v_junction_table_column_list(v_junction_table_column);
    v_junction_table_column := v_junction_table_column_list.NEXT(v_junction_table_column);
  END LOOP;
  
  IF NOT v_junction_table_column_list.EXISTS(v_junction_column_list('FROM')) THEN
    v_junction_direction := v_junction_column_list('FROM');
  ELSE
    v_junction_direction := v_junction_column_list('TO');
  END IF;
  
  v_sql_request_insert_part := v_sql_request_insert_part || ', ' || v_junction_direction || ') ' || v_sql_request_select_part;
  
  LOG_AND_EXECUTE('ALTER SESSION ENABLE PARALLEL DML', v_debug_mode);

  LOG_AND_EXECUTE('ALTER SESSION SET SKIP_UNUSABLE_INDEXES = TRUE', v_debug_mode);
  
  -- pour chacune des tables a migrer...
  LOOP
    EXIT WHEN v_table IS NULL;
    BEGIN
      DBMS_OUTPUT.PUT_LINE('*********** Debut de traitement pour la table ' || v_table || '.');
      v_sql_req_ins_tmp_tbl_frm_part := '';
      v_table_treatment_start_time := DBMS_UTILITY.GET_TIME;
      -- on cree le debut de la requete qui determinera les donnees a inserer dans la table
      -- temporaire
      SELECT 'SELECT /*+ PARALLEL ' || v_table || ' */ ' ||  LISTAGG(table_name || '.' || column_name, ', ') WITHIN GROUP (ORDER BY column_id) || ' FROM ' || v_table || ' ' INTO v_sql_req_ins_tmp_tbl_sel_part FROM user_tab_cols WHERE table_name = v_table AND column_id IS NOT NULL;
  
      v_column := v_table_list(v_table).column_list.FIRST;
      -- pour chacune des colonnes a migrer pour la table courante
      LOOP
        EXIT WHEN v_column IS NULL;
        -- si on veut proceder a l'insertion des n-uplets dans la table de liens
        IF NOT v_skip_insert THEN
          -- on complete la requete d'insertion dans la table de liens, la partie FROM
          -- du SELECT notamment
          v_sql_request_select_part := ', ' || v_table_list(v_table).column_list(v_column)
                                       || ' FROM (' 
                                              || 'SELECT DISTINCT /*+ PARALLEL ' || v_table || ' */ '
                                              || v_table_list(v_table).column_list(v_column)
                                              || ' FROM '
                                              || v_table
                                              || ' WHERE ' || v_table_list(v_table).column_list(v_column) || ' IS NOT NULL';
          
          v_override_global   := FALSE;
          v_special_condition := '';
          
          -- si une condition specifique pour cette table et cette colonne existe, alors on l'ajoute
          -- a la suite du WHERE. On stocke la valeur de l'attribut override_global de la condition
          -- en meme temps
          IF v_special_condition_list.EXISTS(v_table) THEN
            IF v_special_condition_list(v_table).EXISTS(v_table_list(v_table).column_list(v_column)) THEN
              v_special_condition := ' ' || v_special_condition_list(v_table)(v_table_list(v_table).column_list(v_column)).condition;
              v_override_global   := v_special_condition_list(v_table)(v_table_list(v_table).column_list(v_column)).override_global;
            END IF;
          END IF;
          
          -- si une condition globale existe et que l'attribut override_global
          -- precedemment stocke est a FALSE...
          IF v_global_special_condition IS NOT NULL AND NOT v_override_global THEN
            -- on ajoute la condition globale, prealablement parsee (le $COLUMN_NAME est automatiquement
            -- remplace par le nom de la colonne courante)
            SELECT REPLACE(v_global_special_condition, '$COLUMN_NAME', v_table_list(v_table).column_list(v_column)) INTO v_parsed_global_special_cnd FROM DUAL;
            v_sql_request_select_part := v_sql_request_select_part || ' ' || v_parsed_global_special_cnd || v_special_condition;
          END IF;
          
          -- on fini par un MINUS pour ne pas prendre en compte les valeurs deja presentes
          -- dans la table de liens
          v_sql_request_select_part := v_sql_request_select_part || ' MINUS SELECT ' || v_junction_direction
                                       || ' FROM ' || v_junction_table;
          
          v_sql_request_select_part := v_sql_request_select_part || ')';
          
          LOG_AND_EXECUTE(v_sql_request_insert_part || v_sql_request_select_part, v_debug_mode);
        END IF;
        
        -- on termine la boucle en remplacant le nom de la colonne a migrer courante
        -- dans la partie SELECT de la requete d'insertion dans la table temporaire
        -- par un appel a la fonction NVL selectionnant l'autre extremite du lien (v_junction_column_list('TO'))
        SELECT REGEXP_REPLACE(v_sql_req_ins_tmp_tbl_sel_part, '([, ])' || v_table || '.' || v_table_list(v_table).column_list(v_column) || '([, ])', '\1NVL(' || v_table_list(v_table).column_list(v_column) || '_MIG.' || v_junction_column_list('TO') || ', ' || v_table || '.' || v_table_list(v_table).column_list(v_column) || ')\2') INTO v_sql_req_ins_tmp_tbl_sel_part FROM DUAL;
        v_sql_req_ins_tmp_tbl_frm_part := v_sql_req_ins_tmp_tbl_frm_part || 'LEFT OUTER JOIN ' || v_junction_table || ' ' || v_table_list(v_table).column_list(v_column) || '_MIG ON ' || v_table || '.' || v_table_list(v_table).column_list(v_column) || ' = ' || v_table_list(v_table).column_list(v_column) || '_MIG.' ||  v_junction_column_list('FROM') || ' ';
        v_column := v_table_list(v_table).column_list.NEXT(v_column);
      END LOOP;
      
      v_sql_request :=
           'SELECT megs table_size FROM (SELECT SUM (bytes) / 1048576 megs, segment_name FROM user_extents '
            || 'WHERE segment_type in (''TABLE'',''TABLE SUBPARTITION'') '
            || 'GROUP BY segment_name) ue INNER JOIN '
            || 'user_tables ON user_TABLES.TABLE_NAME = ue.segment_name '
            || 'WHERE user_TABLES.table_name = '''
            || v_table
            || '''';
      DBMS_OUTPUT.PUT_LINE('INFO      : ' || v_sql_request);
      
      IF NOT v_debug_mode THEN
        EXECUTE IMMEDIATE v_sql_request INTO v_table_size;
      END IF;
      
      v_sql_request := 'SELECT SUM(bytes)/1024/1024 FROM user_free_space WHERE tablespace_name = ''&TBS_DATA'' GROUP BY tablespace_name';
      
      DBMS_OUTPUT.PUT_LINE('INFO      : ' || v_sql_request);
      IF NOT v_debug_mode THEN
        EXECUTE IMMEDIATE v_sql_request INTO v_free_space;
      END IF;
      
      v_table_size := 0;
      v_free_space := 0;

      IF v_table_size > v_free_space THEN
        RAISE e_not_enough_free_space;
      END IF;
      
      -- on cree la table temporaire comme une copie vide de la table d'origine,
      -- de maniere a en avoir la structure
      LOG_AND_EXECUTE('CREATE TABLE ' || v_table || v_tmp_table_suffix || ' TABLESPACE &TBS_DATA ' || 'AS  SELECT * FROM ' || v_table  || ' WHERE 1 = 0', v_debug_mode);

      -- on ajoute la partie INSERT a la requete de selection et on l'execute
      SELECT 'INSERT /*+ PARALLEL APPEND ' || v_table || v_tmp_table_suffix || ' */ INTO ' || v_table || v_tmp_table_suffix || ' (' || LISTAGG(column_name, ', ') WITHIN GROUP (ORDER BY column_id) || ') ' INTO v_sql_req_ins_tmp_tbl_ins_part FROM user_tab_cols WHERE table_name = v_table AND column_id IS NOT NULL;
      
      LOG_AND_EXECUTE(v_sql_req_ins_tmp_tbl_ins_part || v_sql_req_ins_tmp_tbl_sel_part || v_sql_req_ins_tmp_tbl_frm_part, v_debug_mode);
      
      v_sql_request := 'SELECT /*+ PARALLEL ' || v_table || ' */
           COUNT (*) FROM ' || v_table;
      IF NOT v_debug_mode THEN
        EXECUTE IMMEDIATE v_sql_request INTO v_original_table_line_count;
      END IF;
      
      v_sql_request := 'SELECT /*+ PARALLEL ' || v_table || v_tmp_table_suffix || ' */ '
        || ' COUNT (*) FROM ' || v_table || v_tmp_table_suffix;
      IF NOT v_debug_mode THEN
        EXECUTE IMMEDIATE v_sql_request INTO v_new_table_line_count;
      END IF;
      
      IF v_original_table_line_count <> v_new_table_line_count THEN
        RAISE e_different_line_count;
      END IF;
          
      SELECT 
        dbms_metadata.get_ddl('INDEX', index_name)
      BULK COLLECT INTO 
        v_pk_index_table
      FROM 
        user_indexes
      WHERE 
        table_name = v_table
        AND 
        uniqueness = 'UNIQUE'
      ;
      
      SELECT 
        constraint_name
      BULK COLLECT INTO 
        v_constraint_table
      FROM 
        user_cons_columns
      WHERE 
        table_name = v_table
      ;
      
      -- on desactive toutes les contraintes de la table courante
      FOR i IN 1..v_constraint_table.COUNT LOOP
        v_constraint := v_constraint_table(i);
        LOG_AND_EXECUTE('ALTER TABLE ' || v_table || ' DISABLE CONSTRAINT ' || v_constraint, v_debug_mode);
      END LOOP;
      
      LOG_AND_EXECUTE('TRUNCATE TABLE ' || v_table, v_debug_mode);
      
      SELECT 
        index_name
      BULK COLLECT INTO 
        v_index_table
      FROM 
        user_indexes
      WHERE 
        table_name = v_table
        AND
        uniqueness = 'NONUNIQUE'
      ;

      -- on met les index non uniques a UNUSABLE
      FOR i IN 1 .. v_index_table.COUNT LOOP
        v_index := v_index_table(i);
        LOG_AND_EXECUTE('ALTER INDEX ' || v_index || ' UNUSABLE', v_debug_mode);
      END LOOP;

      SELECT 
        index_name
      BULK COLLECT INTO 
        v_index_table
      FROM 
        user_indexes
      WHERE 
        table_name = v_table
        AND
        uniqueness = 'UNIQUE'
      ;

      -- on supprime les index uniques
      FOR i IN 1..v_index_table.COUNT LOOP
         v_index := v_index_table(i);
         LOG_AND_EXECUTE('DROP INDEX ' || v_index, v_debug_mode);
      END LOOP;
      
      v_sql_request :=
         'INSERT /*+ APPEND PARALLEL */ INTO ' || v_table
      || ' SELECT /*+ PARALLEL ' || v_table || v_tmp_table_suffix || ' */ * FROM ' || v_table || v_tmp_table_suffix;
      
      DBMS_OUTPUT.PUT_LINE ('INFO      : Re-import des donnees dans ' || v_table || '...');
      
      LOG_AND_EXECUTE(v_sql_request, v_debug_mode);
      
      SELECT 
        index_name
      BULK COLLECT INTO 
        v_index_table
      FROM 
        user_indexes
      WHERE 
        table_name = v_table 
        AND 
        partitioned = 'NO'
      ;

      -- reconstruction des index non partitionnes
      FOR i IN 1..v_index_table.COUNT LOOP
        v_index := v_index_table(i);
        LOG_AND_EXECUTE('ALTER INDEX ' || v_index || ' REBUILD PARALLEL COMPUTE STATISTICS', v_debug_mode);
        LOG_AND_EXECUTE('ALTER INDEX ' || v_index || ' NOPARALLEL', v_debug_mode);
        
        DBMS_OUTPUT.PUT_LINE ('INFO      : Index ' || v_index || ' reconstruit.');
      END LOOP;

      SELECT UNIQUE 
        (index_name)
      BULK COLLECT INTO 
        v_index_table
      FROM 
        user_indexes 
        INNER JOIN 
        user_ind_partitions 
        USING (index_name)
      WHERE 
        table_name = v_table 
        AND 
        partitioned = 'YES' 
        AND 
        composite = 'NO'
      ;
  
      -- reconstruction des index partitionnes non composites
      FOR i IN 1..v_index_table.COUNT LOOP
        v_index := v_index_table(i);
        
        SELECT    'ALTER INDEX '
        || INDEX_NAME
        || ' REBUILD PARTITION '
        || PARTITION_NAME
        || ' PARALLEL'
        BULK COLLECT INTO 
          v_rebuild_index_sql_req_table
        FROM 
          user_ind_partitions
        WHERE 
          index_name = v_index;
        
        FOR j IN 1..v_rebuild_index_sql_req_table.COUNT LOOP
          LOG_AND_EXECUTE(v_rebuild_index_sql_req_table(j), v_debug_mode);
        END LOOP;
        
        LOG_AND_EXECUTE('ALTER INDEX ' || v_index || ' NOPARALLEL', v_debug_mode);
        DBMS_OUTPUT.PUT_LINE ('INFO      : Index ' || v_index || ' OK avec ' || v_rebuild_index_sql_req_table.COUNT || ' partitions.');
      END LOOP;
      
      SELECT UNIQUE(index_name)
      BULK COLLECT INTO 
        v_index_table
      FROM 
        user_indexes 
        INNER JOIN 
        user_ind_partitions 
        USING (index_name)
      WHERE 
        table_name = v_table 
        AND 
        partitioned = 'YES' 
        AND 
        composite = 'YES'
      ;
      
      -- alter index sur tous les index partitionnes composites
      FOR i IN 1..v_index_table.COUNT LOOP
        v_index := v_index_table(i);
        
        SELECT    'ALTER INDEX '
        || INDEX_NAME
        || ' REBUILD SUBPARTITION '
        || SUBPARTITION_NAME
        || ' PARALLEL'
        BULK COLLECT INTO 
          v_rebuild_index_sql_req_table
        FROM 
          user_ind_subpartitions
        WHERE index_name = v_index;
        
        FOR j IN 1..v_rebuild_index_sql_req_table.COUNT LOOP
          LOG_AND_EXECUTE(v_rebuild_index_sql_req_table(j), v_debug_mode);
        END LOOP;
      
        LOG_AND_EXECUTE('ALTER INDEX ' || v_index || ' NOPARALLEL', v_debug_mode);
        DBMS_OUTPUT.PUT_LINE ('INFO      : Index ' || v_index || ' OK avec ' || v_rebuild_index_sql_req_table.COUNT || ' sous partitions');
      END LOOP;
      
      FOR i IN 1..v_pk_index_table.COUNT LOOP
        v_sql_request := v_pk_index_table(i);
        SELECT REGEXP_REPLACE(REGEXP_REPLACE(v_sql_request, chr(10), ''), ' {2}', '') INTO v_sql_request FROM DUAL;
        LOG_AND_EXECUTE(v_sql_request, v_debug_mode);
      END LOOP;
      
      SELECT 
        constraint_name
      BULK COLLECT INTO 
        v_constraint_table
      FROM
        user_cons_columns
      WHERE table_name = v_table;

      -- reactivation des contraintes

      FOR i IN 1..v_constraint_table.COUNT LOOP
        v_constraint := v_constraint_table(i);
        LOG_AND_EXECUTE('ALTER TABLE ' || v_table || ' ENABLE CONSTRAINT ' || v_constraint, v_debug_mode);
      END LOOP;

      LOG_AND_EXECUTE('SELECT TO_CHAR (SYSDATE, ''DD/MM/YYYY HH24:MI:SS'') dt_fin FROM DUAL', v_debug_mode);

      LOG_AND_EXECUTE('DROP TABLE ' || v_table || v_tmp_table_suffix, v_debug_mode);
    EXCEPTION
      WHEN e_not_enough_free_space THEN
        DBMS_OUTPUT.PUT_LINE('EXCEPTION : L''espace libre (' || v_free_space || ' Mo) dans le tablespace &TBS_DATA est insuffisant pour migrer la table ' || v_table || ' (' || v_table_size || ' Mo).');
        DBMS_OUTPUT.PUT_LINE('EXCEPTION : La table ' || v_table || ' n''a pas ete migree.');
      WHEN e_different_line_count  THEN
        DBMS_OUTPUT.PUT_LINE('EXCEPTION : Le nombre de lignes dans ' || v_table || ' (' || v_original_table_line_count || ') est different de celui de la table ' || v_table || v_tmp_table_suffix || ' (' || v_new_table_line_count || '). La table source (' || v_table || ') est conservee.');
      WHEN OTHERS THEN
        DBMS_OUTPUT.PUT_LINE('EXCEPTION : ' || SQLERRM);
    END;
    v_table_treatment_end_time := DBMS_UTILITY.GET_TIME;
    IF NOT v_debug_mode THEN
      DBMS_OUTPUT.PUT_LINE('*********** Fin de traitement pour la table ' || v_table || '. Temps total : ' || ((v_table_treatment_end_time - v_table_treatment_start_time) / 100) || ' secondes.'); 
    END IF;
    v_table := v_table_list.NEXT(v_table);
  END LOOP;
  v_treatment_end_time := DBMS_UTILITY.GET_TIME;
  LOG_AND_EXECUTE('ALTER SESSION SET SKIP_UNUSABLE_INDEXES = false', v_debug_mode);
  IF NOT v_debug_mode THEN
    DBMS_OUTPUT.PUT_LINE('Fin de traitement. Temps total : ' || ((v_treatment_end_time - v_treatment_start_time) / 100) || ' secondes');
  END IF;
END;
/

UNDEF TBS_TMP
UNDEF TBS_DATA
UNDEF TBS_INDEX
UNDEF TBS_PARAM
UNDEF TBS_BLOB

EXIT;

 Astuces performances 

 détecter "au moins 1" élément 

Pour savoir si au moins 1 élément correspond à un critère dans une table sans aller lire toute la table (cas ou le critere est vrai dans plus de 10% des cas) / faire des tris en base (via un count) :

/* Renvoi 1 si au moins 1 contrat est lié à cette condition de paiement */
SELECT 1
  FROM DUAL
 WHERE EXISTS
           (SELECT 1
              FROM tcontrat cont2
             WHERE cont2.conditionpaiement_id = '$!35893826');

 filtre "IS NULL" 

Le filtre "IS NULL" dans une clause "WHERE" ne permet pas d'utiliser d'index sur la colonne filtrée.

Astuce : si le nombre de "NULL" dans la table est inférieur à 10% du nombre total de lignes et que ce filtre est le plus discriminant dans la requete, faire créer un index double dont la seconde colonne sera une constante.

Exemple :
Select * from MA_TABLE tab1 where tab1.colonne1 IS NULL;

/* Créer l'index suivant: */
create index IDX_MATABLE_C1 on MA_TABLE(colonne1,1);

 HINT pour les requêtes des batchs 

Les "HINT" sont des commentaires permettant de suggérer des accès à l'optimiseur Oracle (CBO).
Ces derniers se positionnent dans le properties ou le properties2 du batch.

 En phase "init" 

Lors de la phase "init", les requêtes de chargement effectuant nécessairement un "full" sur la table de travail peuvent être améliorées afin de bénéficier de la puissance du serveur de base de données:

Cas d'un INSERT avec un seul select imbriqué :

HINT_REQ_UPD_xxx=i:ENABLE_PARALLEL_DML PARALLEL(nom_de_la_table);s1 PARALLEL(tmp) LEADING(tmp) USE_NL(tmp)

avec :
 nom_de_la_table : nom de la table figurant dans la clause "INTO" de l'INSERT
 tmp : alias de la table de travail dans la partie SELECT
 dans la clause USE_NL, il est possible d'ajouter dans l'ordre logique d’exécution la liste des alias des tables à lire. Exemple : USE_NL(tmp fac1 fac2 cnt)

dans le cas d'un update, le "i" sera remplacé par un "u"

 En phase "MT" 

Lors de la phase "MT",  généralement le plan de la majorité des requetes doit commencer par la table de travail (BATCH_) dont l'alias est souvent "tmp". Afin de garantir ce plan, il est possible d'ajouter les HINT :

HINT_REQ_SEL_xxx=LEADING(tmp) USE_NL(tmp)

avec :
 tmp : alias de la table de travail dans la partie SELECT
 dans la clause USE_NL, il est possible d'ajouter dans l'ordre logique d'execution la liste des alias des tables à lire. Exemple : USE_NL(tmp fac1 fac2 cnt)<!!!>Category:outil
Category:pg_profile

Créé par Andrei Zubkov, pg_profile est une extension open source qui permet de générer des rapport de type AWR côté Oracle.

 Guide d'installation 
Guide d'installation de pg_profile

 Guide d'utilisation 
 Guide d'utilisation de pg_profile

 Liens externes 
  Documentation officielle
  Présentation de pg_profile
  Github pg_profile<!!!>Category:outil
Category:BDD

Créé par Gilles Darold, pgBadger est un outil permettant d'analyser les log produits par une instance PostgreSQL, et de générer des rapports html présentant l'activité de la BDD..

 Guide d'installation 
Guide d'installation de pgBadger

 Guide d'utilisation 
 Guide d'utilisation de pgBadger<!!!>Category:outil
Category:BDD

Créé par Dalibo, PoWA est un outil permettant d'analyseur le trafic en cours sur les instances PostgreSQL par le biais de différents moyen, notamment : graphes présentant les temps d’exécution, graphe des blocks hit/read, ainsi qu’un tableau listant les requêtes les plus coûteuses sur la période sélectionnée.

 Guide d'installation 
Guide d'installation de PoWA

 Guide d'utilisation 
 Guide d'utilisation de PoWA<!!!>Category:outil
 http://www.blogduwebdesign.com/developpement-vim/vim-astuce-pour-le-copier-coller/605
Aujourd'hui, faisons un focus sur une des fonctionnalités basiques de Vim que nous pouvons pousser vraiment loin : le copier/coller
 Les commandes de bases 
 copier/coller 
Une des premières choses que l'on cherche à savoir faire sous Vim, c'est le copier/coller.
 y 
 y permet de copier des choses. Appuyer sur y depuis le mode visuel, avec du texte de sélectionné, pour le copier. Depuis le mode commande, yy ou Y copie la ligne complète sur laquelle nous sommes. Dans ce même mode, il est possible d'associer y avec les commandes de positionnement. y$ copie donc jusqu'à la fin de la ligne, yw copie le mot sur lequel nous sommes, ...
 d 
 d fonctionne de la même manière que y, mais coupe au lieu de copier. La seul commande qui diffère est D, qui imite d$ au lieu de dd.
 p 
 p permet de coller le texte actuellement dans le presse papier de Vim, juste après le curseur. P (la majuscule) va lui, l'insérer avant le curseur. En mode visuel, p remplace la sélection, qui se retrouve dans le presse papier.<!!!>Category:outil
Category:Guide d'installation
Category:échanges
Cet outil Java développé par NRE permet de découper un fichier XML trop volumineux en fichiers plus petits.

Installation

Récupération du programme
 Cloner ou faire un pull du projet échanges repo échanges
 En étant dans le projet echanges:
mvn clean install -f ExtracteurBalisesEchanges 
 L’exécutable est généré dans le répertoire ExtracteurBalisesEchanges/executable

Pour industrialiser un peu
Nécessitant une précompilation
faire un script shell qui vient lancer le programme avec des options de JVM par exemple :

extracteurBalisesEchanges.sh dans ~/.bin/

#!/bin/sh
java -jar -Xms20g -Xmx20g ~/.bin/jar/extracteurBalisesEchanges.jar $*

dans le path git/windows (ce avec quoi vous êtes le plus habitué) rajouter le répertoire ~/.bin

Sans précompilation
faire un script shell qui vient lancer le programme avec des options de JVM par exemple :

extracteurBalisesEchanges.sh dans ~/.bin/

#!/bin/sh
mvn clean install /f /d/java/workspaces/develop/echanges/ExtracteurBalisesEchanges
java -jar -Xms20g -Xmx20g /d/java/workspaces/develop/echanges/ExtracteurBalisesEchanges/executable/extracteurBalisesEchanges.jar $*

dans le path git/windows (ce avec quoi vous êtes le plus habitué) rajouter le répertoire ~/.bin

Utilisation
selon la configuration faite :
 java -jar -Xms20g -Xmx20g extracteurBalisesEchanges.jar -h
 extracteurBalisesEchanges.sh -h

Tutoriel
Le programme peut s'utiliser de plusieurs façon :
faire un split d'un fichier d'entré
agréger en fonction par format plusieurs fichiers généré par split (ne fonctionne correctement que si un split de 1 échange par fichier a été fait. le fichier d'origine ne doit pas être présent dans le répertoire contenant le fichier splitté)
compter le nombre de balises présentes dans le fichier d'entré

Toutes les options sont disponibles via la commande :
extracteurBalisesEchanges.sh -h

Commande d'exécution d'un cas de figure simple

Ouvrir git bash dans le répertoire où se situe le fichier XML volumineux
Lancer la commande suivante :
extracteurBalisesEchanges.sh -f path/du/fichier/pub.xml -s nombre_echange_par_fichier

Résultat

Après son exécution, l'outil doit avoir généré un nombre variable de fichiers XML plus petits, préfixés de leur numéro de création.

Fichier:Exemple split de XML.png<!!!> En bref 
Développeur / Stagiaire efluid - groupe Pole composants transverses

 En cours 
 Développement d'un moteur de recherche pour la documentation interne des développeurs 
200px
 Web crawler
 moteur de recherche inversée (Solr => configuration)
 interface de recherche

 Web scraper 
 récupération de la totalité des liens disponibles sur wikefluid
 suppression automatique des liens à supprimer

 Done 

Category:développeur<!!!> En bref 
Développeur / Stagiaire efluid (JMAR) - groupe Pole composants transverses

 Done 
 Développement d'un moteur de recherche pour la documentation interne des développeurs 
200px

Category:développeur<!!!>category:guide
Procédure de création d'une codification pour un flux
Principe
Pour certains flux clients, la génération de fichiers au format externe (ou la lecture de fichiers externes) nécessite l'utilisation des codifications du client.
Les codifications sont propres à chaque client Efluid (ES, Gedia, ERDF...) et à chaque format de données (factures format F01, F03...).
Leur but est de permettre l'insertion de valeurs par défaut dans les balises des fichiers de sortie.
Préparation
Création du jeu de codification
Ajouter une valeur dans la classe EEfluidNetTypeCodification.java 
Ajouter une valeur dans le fichier EEfluidNetTypeCodification_fr.properties 

Insertion des valeurs
Créer un script SQL ponctuel T_TCODIFICATION_MONFLUX_EVTXXXX.sql où XXX est le numéro d'événement de création/correction du flux. Dans ce script, insérer les différentes valeurs de codification voulues.
INSERT INTO TCODIFICATION ( ID, ETATOBJET, TYPECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, 
VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION ) VALUES ( 
'ECH_COD_CLIENT_MONFLUX_0101', 0, INDICE, 'Y', 'Z', 'java.lang.String', 'X', NULL, NULL, '', '', NULL, NULL);
-- Où INDICE est le numéro de la codification dans la classe EEfluiNetTypeCodification.
-- X est le code efluid
-- Y est la valeur codifiée (code) externe correspondant à X
-- Z est le libellé externe correspondant à X
Appel aux codifications
Dans chacune des classes MonObjetBusinessProcess de mon flux, faire un appel quand nécessaire au nouveau jeu de codification :
Insérer l'appel à 
new GenericCodificationManager(EEfluidNetTypeCodification.FLUX_F03);
Faire des appels aux méthodes getCodeCodifie et getLibelleCodifie pour récupérer les valeurs de codification voulues, en passant en paramètre le code efluid défini dans l'AFD (il peut être déterminé par une règle de correspondance incluant plusieurs attributs d'un objet métier, par exemple);
Les insérer dans les attributs des objets métier format de sortie
Livraison
Pour rendre un jeu de codification valides il faut :
 Demander à l'equipe Efluid IT/au fonctionnel les environnements dans lesquels la codification doit être opérante;
 Créer un événement fils de l'événement de développement/correction du flux impacté ; cet événement sera destiné à la livraison du script SQL;
 Livrer le script dans ebuild en demandant le report sur les versions ultérieures;
 Livrer le développement sur la branche develop et le reporter sur les branches de maintenance concernées.
Utilisation
Visualisation des codifications
Les codifications sont visibles dans le menu administration>paramétrage>codification d'efluidNet.

Fichier:efluidNet.menuParametrage.png

Fichier:efluidNet.codification.png

Surcharge des codifications
Dans la plupart des cas, la surcharge des codifications est prévue. Le plus souvent elle est réalisée par le biais de paramètres entreprise.
Dans ce cas, il faut créer une classe CorrespondanceExterneMonFluxUtils prévoyant les appels aux paramétrages entreprise (se baser sur CorrespondanceExterneF03Utils qui présente le cas des ensembles de paramétrage entreprise, le cas le plus courant et le plus complexe).<!!!>category:guide
category:requêteur
Cette page décrit la procédure de développement d'un écart requêteur ou d'une anomalie requêteur. Cette procédure concerne efluid mais est semblable dans le cas des autres applications (énercom, suivefluid, etc.).

 Procédure 

 Développement d'un écart 

 Actions pour le pôle fonctionnel 

 Identifier les colonnes à requêter pour chaque objet
 le pôle fonctionnel doit déterminer quelles colonnes sont à requêter et sous quel type (normalement c'est évident), dans l'AFD, le tableau des attributs devrait contenir une colonne "requêteur".
 Identifier la structure des vues en fonction des objets à requêter
 le pôle fonctionnel doit déterminer quels liens sont requêtables, dans quel sens, s'assurer qu'il n'y a pas de dépendances cycliques et identifier les jointures externes si nécessaire (avec l'aide du développeur)

 Actions pour le développpement 

 Développer les vues SQL correspondantes
 déterminer quelles colonnes en base sont nécessaires pour chaque attribut de chaque objet identifié et construire la vue SQL correspondante, par exemple : Requêteur#Vues SQL.

 Développer les vues XML correspondantes
 idem pour les vues XML, qui donneront le mapping du type SQL vers le bon type Java, par exemple : Requêteur#Vues XML.

 Actions pour la livraison du code en développement 

 Les vues SQL doivent être livrées dans le dossier sql/vues puis dans le domaine correspondant
 Les vues XML doivent être livrées dans le dossier app/src/main/resources/xml
 Les vues SQL doivent être exécutés sur les bases de développements après commit

 Actions pour la livraison du code pour assemblage 

 Les vues SQL doivent être livrés dans ebuild pour assemblage.
 modèle de livrable : script de migration efluid
 type de script : requeteur
 livrable physique : (comme un script normal, avec les entêtes et terminaisons)

 Actions pour la documentation 

 Mise à jour du fichier de paramétrage : efluid - Documentation Fonctionnelle > AFD suite efluid > AFD efluid > REQ - Requêteur > AFD > Paramétrage requêteur V12.xls

 Correction d'une anomalie 

 Actions pour le développement 

En plus de toutes les actions pour le développement d'un écart, dans le cas d'une correction d'anomalie, il faut s'assurer que les colonnes et les critères impactés soient bien mis-à-jour dans la base. 

 Création des scripts de migration : 
 modification d'un attribut : il faut utiliser le script Requêteur - Scripts de migration requêteur#Modification d'un attribut pour mettre à jour toutes les tables impactées .
 modification d'un attribut avec changement de type : il faut utiliser le script Requêteur - Scripts de migration requêteur#Modification d'un attribut avec changement de type pour mettre à jour toutes les tables impactées

Category:requêteur<!!!>Category:guide
Le principe :

L’idée est de se connecter à une base source, extraire un  paramétrage donné et ensuite l’injecter dans une base destination. Pour ce faire, on utilise le fichier .bat  « ExtractionInjectionVerification.bat », qui posera une série de questions sur les bases des données source et  destination entre autres. 
Pour identifier les bases source et destination le système utilise des fichiers de type « jar » où sont stocké les fichiers framework2.properties. Il existe autant de jar que des bases des données sources et destination.

La procédure  : à suivre pour extraire, puis injecter un paramétrage donné est la suivante :

Etape 1 : Définition de la base cible et destination. Appliquer la procédure suivante :

1.a : Aller au répertoire « \efluid\scripts\extraction\files\lib »

1.b : Vérifier que les bons jar existent. S'ils n’existent pas on les crée. Ils servent juste à surcharger le framework2.properties de la cible et la destination

Etape 2 : Extraction. Appliquer la procédure suivante :

2.a : Vérifier que dans le répertoire « ..\efluid\scripts\extraction\files » le réperoire « logs » existe. Sinon le créer

2.b : Ouvrir la ligne de commande 

2.c : Lancer le fichier « ExtractionInjectionVerification.bat »

2.d : Répondre aux questions :

	Q : Quel type d’opération est à réaliser ? 

	R : 0 (extraction)

	Q : Avec ou sans suppression ?

	R : 0 (sans suppression). ATTENTION à cette réponse !!

	Q : Mode insert ou update ?

	R : 0 (update)

	Q : Choix du domaine ?

	R : saisir le numéro de votre domaine. Il apparait dans l’écran 

	Q : Choix du fichier de paramétrage ?

	R : 1 (paramétrage)

	Q : Choix du schéma à utiliser ?

	R : saisir le numéro de votre schéma. Il apparait dans l’écran

	Q : Choix de l’entité pour la confidentialité à utiliser ?

	R : le numéro de votre choix. Il apparait dans l’écran

Etape 3 : Injection. Dans la foulée le .bat continue à poser les questions suivantes :

	Q : Voulez-vous faire une injection ?

	R : 0 (oui)

	Q : Choix du schéma à utiliser ?

	R : saisir le numéro de votre schéma. Il apparait dans l’écran

	Q : Choix de l’entité pour la confidentialité à utiliser ?

	R : le numéro de votre choix. Il apparait dans l’écran

	Q : Choix Injection avec ou sans suppression ?

	R : 1 (avec suppression si l’on veut annuler et remplacer) 

Pour plus de détail, merci d'aller sur eRoom

http://wperoom1/eRoom/Production/QualiteDeveloppementEfluid/0_454c2

REMARQUES : 

•	Il ne faut pas extraire avec suppression de l’origine

•	Il faut injecter avec suppression de la cible

•	Quelques problèmes de compatibilité peuvent arriver suite à la différence de version entre la base source et 
la destination<!!!>category:guide
A partir du socle technique V14, une remodularisation Maven des projets a lieu (Cf evt 236600).
Celle-ci engendre plusieurs modifications que les développeurs vont devoir prendre en compte pour continuer à travailler.
Ce guide vise donc à lister et expliciter ces modifications. Certains seront spécifiques à un projet, d'autres sont génériques et concernant toutes les applications/briques.

 Re-import projet efluid-webapp dans eclipse 

Le module efluid-webapp a été déplacé de l'emplacement "app" vers l'emplacement "web", il est donc nécéssaire de ré-importer le projet dans eclipse. Pour cela il faut supprimer le projet efluid-webapp (idéalement supprimer aussi les fichiers .classpath, .factorypath, .project et le dossier .settings du répertoire app) , puis ré-importer le module efluid-webapp depuis l'emplacement web en suivant le guide traditionnel
Fichier:ReimportEfluidWebapp.PNG

Ce module efluid-webapp contient désormais uniquement les ressources web (JSP, JS, CSS, etc...), les classes et ressources se trouvent toujours dans le dossier app, qui représente désormais le module efluid-jar. Il est donc important d'importer également le module efluid-jar dans eclipse toujours en suivant le guide traditionnel.

Fichier:ImportEfluidJar.PNG

Attention à bien avoir la case "Resolve dependencies from Workspace projects" cochée, et le profil "dev-metz" actif sur tout vos projets efluid dans eclipse comme le montre le screen ci-dessous.
Fichier:EclipseMavenSettings.png

 Suppression du dossier de test integration 

Les tests d'intégration sont déplacés des dossiers src/test/integration vers les dossiers src/test/java. Cela permet de retomber dans une configuration plus classique de Maven, et ainsi éviter les problèmes de non prise en charge de ces répertoires par eclipse par exemple (Cf problèmes depuis eclipse Photon).

Tous les tests d'un module Maven sont donc présents dans src/test/java. Les ressources ne changent pas d'emplacement et sont toujours dans src/test/resources.

La différence entre un TU et un TI est donc uniquement son nommage. Pour rappel la norme est : 
 Test unitaire : TestXXX
 Test intégration : TestITXXX
 Si besoin de classe utilitaire dans les tests mais qui ne sont pas des tests l'utilisation de suffixe est possible : XXXTest

 Suppression de l'obligation d'utiliser les profils Maven test-unitaire et test-integration 

Les profils test-unitaire et test-integration ne seront plus nécessaires pour lancer des TU et/ou TI avec Maven localement par les développeurs. Les tests sont pas contre "skippé" par défaut, pour les exécuter il faut donc procéder comme suit

Si l'on veut lancer uniquement les TU on utilise la commande Maven suivante : mvn clean install -DskipTests=false
ou alors la commande : mvn clean install -Ptest-unitaire

Si l'on veut lancer les TU et les TI on utilise la commande Maven suivante : mvn clean install -DskipTests=false -DskipITs=false
ou alors la commande : mvn clean install -Ptest-integration

 Nouveau module de configuration 

 Suppression des jars de properties UL 

Les properties2 (framework2.properties, etc...) qui étaient injectés directement via des jars par l'UL au moment des lancements de tests dans Jenkins, sont désormais supprimés, afin que tous les properties soient présents dans le code source. Cela permettra aux développeurs de pouvoir lancer les tests sur leurs postes de façon la plus proche possible de ce que fait l'UL. Les seules différences seront : 
 La valorisation de la BDD, car les schémas sont créer automatiquement par les jobs jenkins
 La valorisation des emplacements filesystem, car l'UL est sous Linux et les développeurs sous Windows

Afin de pouvoir visualiser plus facilement les surcharges coté UL, le fichier framework2.properties effectif utilisé lors des builds UL sera directement archivé sur le build courant, comme le montre le screen suivant : 
Fichier:ArchivageFramework2.PNG

 Modules de configuration 

Il existe désormais plusieurs modules Maven déstinés à contenir la configuration technique (les fichiers properties).
 Le module de configuration (efluid-config) contient la configuration par défaut de efluid, et cela produit un jar qui est livré aussi bien en dev, qu'en prod, et que pour les tests. Un second jar est également produit par ce module mais ne contient que la configuration par défaut pour les tests, il n'est donc disponible qu'en test. Les développeurs peuvent le consulter afin de voir comment sont paramétrés techniquement les tests, et quelles sont les surcharges par rapport à la configuration par défaut.
 Le module de configuration de dev (efluid-config-dev) va permettre de centraliser les ressources properties spécifiques aux développeurs, c'est ce qui se trouvait donc dans le répertoire properties2 de tous les modules. Ce module étant désormais centralisé, il n'y aura plus besoin d'avoir des fichiers properties disséminés dans chaque module. Ce module de configuration contient également les bundles de connexions aux BDD (qui ne sont désormais plus rangés dans des répertoires depuisParis/depuisMetz mais directement dans un répertoire bdd).
 Il existe également un module de configuration de prod, mais il n'est pas utile aux développeurs c'est simplement un pont permettant de faire fonctionner les surcharges présentes dans l'installeur des applications par exemple.
 Il existe enfin un module de configuration de batch, qui contient les propriéts spécifiques en lancement des batchs. Ce module est donc utilisé pour les lancement de batch. Lorsque on utilise ce module, on utilise pas le module de configuration de prod (c'est l'un ou l'autre).

Afin d'avoir la meilleure prise en charge de ces properties par les IDE il est conseillé d'importer les modules efluid-config et efluid-config-dev dans les IDE.

 Nettoyage des properties2 
 
- Copier les fichiers framework2.properties, Hermes2.properties, EDK2.properties, Ecore2.properties, etc...  en les mettant respectivement dans le module config-dev/src/main/properties2
Fichier:Nettoyage properties2 copierfichiers.JPG

- Supprimer les fichiers framework2.properties, Hermes2.properties, EDK2.properties, Ecore2.properties, etc... qui se trouve dans tous les modules de efluid. (à l'exception du module config-dev)

Fichier:Deplacerproperties2.JPG

- Votre configuration est prête, si vous voulez tester vous pouvez lancer un simple test et vérifier que la JDBC_CONNECT_STRING a bien pour valeur ce que vous avez dans le framework2.properties qui se trouve dans le module config-dev/src/main/properties2

Fichier:ExempleBddConfigDev.JPG

On s'attend donc à avoir dans la log : JDBC_CONNECT_STRING=jdbc:oracle:thin:@ldbddedt3:2483/DE14EDT1

Fichier:Testproperties2.JPG

 Comment surcharger un paramètre dans les properties ? 

Cela ne change pas par rapport au fonctionnement d'avant, il faut toujours modifier votre framework2.properties qui se trouve maintenant dans l'unique point d'entrée : le module config-dev/src/main/properties2.
Si à l'exécution votre fichier Framework2.properties n'est pas trouvé, vérifier que le(s) projet(s) config-batch ou config-prod sont présents dans le workspace, sinon les importer.

 Comment ajouter/modifier des properties dans le framework suite à une demande 

Si vous avez une demande d'ajout/modification/suppression de paramètre comme par exemple modifier le POOL_MAX_CONNECTIONS, il faut aller modifier le framework.properties se trouvant dans le module config (efluid/config/src/main/resources)

Fichier:Ajouter modifier properties framework.JPG

 Paramétrage technique associés aux TU/TI 

Auparavant le paramétrage technique s'effectuait uniquement via les fichiers properties (fmk.properties, etc...). Ainsi on avait des fichiers de properties dédiés au TU, et d'autres dédiés au TI (voir même des fichiers spécifiques pour un module particulier).
Désormais tout cela n'est plus d'actualité, les fichiers de properties présents dans le dossier src/test/resources du module efluid-config sont pris par défaut pour les TU. Rien n'est ajouté de plus coté UL. Ainsi on a exactement le même lancement côté DEV et côté UL.
Pour les TI on n'utilise pas d'autres fichiers de properties, les mêmes fichiers que pour les TU sont pris. Afin de gérer la configuration spécifique TI nécessaire (jdbc connexion manager pour avoir une BDD, pas de Mock de paramètres entreprises etc...) on utilise une "Rule" Junit ContexteTestIntegrationRule qui va automatiquement modifier le paramétrage technique. Cela fait qu'on peut lancer facilement des TI sans devoir toucher aux fichiers de configurations, on a ainsi encore une fois un lancement TI qui est équivalent entre UL et dev, et même un lancement TI qui est proche entre les IDE et Maven.
Ainsi tous les TI actuels n'héritant pas de AbstractTestIT doivent être revus afin d'ajouter la "Rule" Junit ContexteTestIntegrationRule. (et on pourrait à terme se débarrasser de AbstractTestIT ou TestEfluid, etc.)
<p>
Exemple d'utilisation de la Rule Junit pour les TI test.efluid.arc.commun.rule.ContexteTestIntegrationRule

 @Rule
  public ContexteTestIntegrationRule contexte = ContexteTestIntegrationRule.newContexteAvecMockDesParametresTechniques();

  @Test
  public void devrait_tester_quelque_chose()
  // Given
    ModeleAffaireTravaux modele = contexte.enregistrerParEntityProcess(creerModeleAffaireTravaux().type().avecModeInitialisationDemandeur(RESPONSABLE_PROJET).build());

Le mock des paramètres techniques se fait à l'appel de "newContexteAvecMockDesParametresTechniques".

 Il est inutile d'utiliser la Rule ET d'hériter de "AbstractTestIT" ! (Il existe des cas dans efluid ! Ils sont censés faire la même chose.)
 Evitez de déclarer les classes testées comme "private static final" de la classe de test. En faisant ça vos différentes méthodes de test ne sont pas isolées les unes des autres, vous ne gagnez rien en terme de perf puisque l'instanciation est de l'ordre de la nano seconde, et il n'y a probablement aucune raison à ça.
par exemple : 
public class TestGenerationReferencePDSProcess {
  private static final GenerationReferencePDSProcess PROCESS = new GenerationReferencePDSProcess();
Si c'est vraiment justifié, dans ce cas là, 
 soit vous mockez les paramètres techniques avant d'initialiser vos autres resources statiques, puis instanciez la rule avec "newContexteSansMockDesParametresTechniques" :
public class TestITGenerationReferencePDSProcess {
  
  static {
    MockProperties.addMockProperties(ParametersConfig.CONFIG_TESTS_INTEGRATION);
  }

  private static final GenerationReferencePDSProcess PROCESS = new GenerationReferencePDSProcess();

  @Rule 
  public ContexteTestIntegrationRule contexte = ContexteTestIntegrationRule.newContexteSansMockDesParametresTechniques();
 
 soit vous passez la Rule en ClassRule, qui sera static du coup (les @Before du ExternalResource deviendront en fait des @BeforeClass), mais vos @Test ne seront plus isolés (la transaction sera ouverte avant tous les tests et fermés après tous les tests de la classe),

 Si ça ne fonctionne pas (erreur du genre NoDBAccess), c'est qu'un autre traitement charge les paramètres techniques avant le mock. Ils sont conservés tels que chargés ad-vitam (pour une JVM).
Pour rappel, pour exécuter une classe de test (mais pas seulement) en java, on initialise celle ci en jouant, dans l'ordre dans lequel ils sont déclarés, tous les appels "static", ensuite on appelle le @BeforeClass (en prenant celui de plus haut niveau d'abord en cas d'héritage),  ensuite la classe est instanciée, et finalement on joue les @Before. En utilisant une rule, ce sont d'abord les before de la rule qui sont joués puis les méthodes annotées (@Before, @BeforeClass) de la classe.

 Cas Particuliers 
 Changer la connexion d'un utilisateur : il est parfois nécessaire d'avoir un utilisateur avec des attributs particuliers connecté. En utilisant la rule, à chaque lancement de test, on ouvre une connexion puis on connecte l'utilisateur. Il s'agit de modifier la méthode de connexion de l'utilisateur or il n'y a pas de méthode (actuellement) qui permette à l'instanciation de la règle de le faire. Il faut donc agir entre l'instanciation de la règle et le lancement du test => dans le constructeur du test. On a un exemple ci-dessous :
  public AbstractContrat() {
    contexte.setProcedureConnexionUtilisateur(() -> getUtilisateurUtils().connecterUtilisateur());
  }

 Emplacement des tests 

 Rappel concept et historique 

L'emplacement de chaque test est un élément important à prendre en compte par un développeur lorsqu'il ajoute un test.
En effet en fonction de l'emplacement du test, l'accès aux différentes dépendances est différent. 
Un test placé dans le module efluid-jar ne pourra pas "voir" les classes du module efluid-interfaces car efluid-jar n'a pas de dépendance vers efluid-interfaces. L'inverse est vrai par contre, un test dans efluid-interfaces pourra accéder aux classes de efluid-jar.
Avec la re-modularisation, les choses changent un peu car auparavant la plupart des tests se trouvait dans le module efluid-webapp qui était en fait un genre de meta-module permettant une agrégation de tous les modules fonctionnels. Ainsi efluid-webapp pouvait accéder à efluid-jar, mais aussi à efluid-interfaces, reversement etc... Ainsi tous les tests de efluid-webapp accédaient à quasiment tous les modules fonctionnels de l'application.
Aujourd'hui ce n'est plus le cas, efluid-webapp est redevenu un simple module contenant les ressources web, et efluid-jar n'a pas de dépendance vers les autres modules fonctionnels, car sinon on aurait des dépendances cycliques. Ainsi les tests présents dans le module efluid-jar n'ont pas accès aux autres modules. Or actuellement, il y a beaucoup de tests d'intégration qui nécessitent les autres modules car ce sont des tests transverses à de nombreux modules de l'application (ça émule le fonctionnement réel de l'application en fait).

 Introduction du module integration 

La remodularisation ajoute un nouveau concepte : le module integration.
Ce module contient des tests qui auront accès à l'ensemble des modules fonctionnels de l'application.
C'est un module qui est le pendant du module efluid-webapp (qui permet de lancer l'application) mais pour lancer des tests transverses.
Ce module sera exporté (dans un jar) qui sera accessible à d'autres applications (migefluid, plugin travaux, etc...) mais ne sera pas accessible aux modules internes d'efluid (pour éviter les dépendances cycliques).
Ce module accèdera également aux modules qui ont exportés leurs tests (notamment les tests du module efluid-jar).
Le module efluid-webapp n'aura aucun lien avec le module integration et inversement.
Il est donc conseillé d'importer ce module dans eclipse.
Ce module sera également créé prochainement dans les briques (c'est le module qui s'appelait demo dans archi). Dans les briques ce module contiendra en plus des objets dédiés aux fonctionnement des tests, car les briques ont souvent besoin d'émuler le comportement d'une application, ce qui n'est pas le cas dans efluid car on a déjà les vrais objets à disposition.

 Bonnes pratiques 

Ayant un module supplémentaire, les possibilités d'emplacement des tests sont plus nombreuses. Il est donc nécessaire de se poser encore plus la question de "où dois-je placer mon test". Voici quelques bonnes pratiques permettant de guider le choix : 

 Un test unitaire doit se trouver au plus près du code qu'il teste. Par défaut il doit donc se trouver dans le module où se trouve la classe testée.
 Si ce n'est pas possible, alors on peut le placer dans un autre module fonctionnel qui aurait plus dépendance. Exemple : placer un test d’intégration dans le module efluid-interfaces même si la classe que l'on teste se trouve dans efluid-jar, car on veut avoir accès à efluid-interfaces pour étendre le périmètre du test ou pour des cas particuliers.
 Si ce n'est toujours pas possible alors on placera le test dans le module d’intégration. Mais cela doit vraiment rester une exception, car le module d’intégration ne doit contenir que le strict nécessaire, afin de ne pas être un gros fourre-tout. Il est important d'avoir le code le plus modulaire possible, et donc les tests associés également.

 Un test d'intégration doit également se trouver au plus près du code, mais c'est souvent plus difficile notamment pour les tests d'intégration métier. 
 Si ce n'est pas possible, les règles sont ensuite les mêmes que pour les tests unitaires, mais il est admis qu'il y aura beaucoup de tests d'intégration qui se devront dans le module d’intégration (du moins tant que l'application ne sera pas plus modulaire qu'aujourd'hui).

 Lancement des batchs 
Pour lancer un batch en dev, il existe désormais un nouveau module Maven pour cela : efluid-batch-launcher-dev. Ce module se situe dans efluid/batchs/launcher-dev. Il permet de prendre en compte le module de config spécifique aux batchs, et n'inclut pas le module de config de test mais inclut le module config dev. Il est le pendant du module efluid-webapp pour le lancement de l'application.

Il est possible de surcharger les propriétés de configuration de dev dans ce module en ajoutant des fichiers dans src/main/properties2. Exemple : ajout d'un fichier framework2.properties dans src/main/properties2. Attention, dans ce cas on prend le pas sur le fichier complet présent dans config-dev, et ainsi il faut tout redéfinir (notamment la BDD).

Ainsi les lanceurs de batchs coté eclipse doivent se baser sur ce module et avoir un classpath paramétré comme le montre les screen ci-dessous.
Fichier:LanceurBatchEclipseDev 2.PNG
Fichier:LanceurBatchEclipseDevPNG.PNG

 Suppression deploy.pom et nouvelle organisation applications/ecore/archi/pom parent 
 En socle V14 le projet EDK a été supprimé, et les artefacts ont été déplacés vers archi, ecore et les applications.
 Il ne reste donc que 2 briques : archi et ecore. A cela s'ajoute un élément important : efluid-parent. Ce pom est le père de tous les projets. Il contient la définition des profils, des plugins, et des versions de toutes les dépendances.
 Les briques logicielles en socle V13 contenant des fichiers deploy.pom qui servaient à créer un pom de consommation, c'est à dire le pom qui sera visible des applications qui veulent utiliser la brique (efluid par exemple). Le deploy.pom est donc différent du pom.xml qui lui est le pom de build. Le pom de build contient souvent des dépendances en scope compile, utilisées pour compiler la brique, alors que le pom de consommation deploy.pom contient lui plutôt des dépendances en scope runtime car non nécessaire à la compilation de l'application, mais qui doivent être présentes au runtime.
 Les fichiers deploy.pom résolvent donc le problème des consommateurs de projets au niveau Maven, mais contourne le système de base de Maven. C'est une fonctionnalité qui est discutée dans la mailing liste maven mais non implémentée Cf https://cwiki.apache.org/confluence/display/MAVEN/Build+vs+Consumer+POM. Les deploy.pom générent donc en problème au niveau développement, car ils viennent se substituer aux pom.xml, et lorsque l'on fait mvn clean install on place donc ces deploy.pom dans le repository maven local, ce qui fait que localement les consommateurs sont content, mais le projet lui même peut avoir des problèmes si l'on souhaite le compiler partiellement. Imaginons que l'on compile le projet archi. Si l'on compile tout d'un coup pas de problème, le réacteur maven voit bien les pom.xml de chaque module et le projet reste donc intègre. Par contre si on ne compile que le module archi-batch par exemple, alors ça ne marche pas, car maven va aller chercher le pom de archi-jar dans le repository maven local, et la en réalité il aura le contenu de deploy.pom de archi-batch or celui-ci à réduit les scope à runtime, donc il obtient des erreurs de compilation.
 Une autre façon de faire consiste à créer des modules d'import dans les briques, qui seront donc des modules utilisés par les consommateurs, et dans lesquels on pourra mettre les scope que l'on souhaite sur les dependances. C'est en fait des modules techniques qu'il faut ajouter dans les briques, et supprimer tous les fichiers deploy.pom. Ainsi dans le repository local maven on aura le meme contenu que les fichiers pom.xml, ce qui revient à un comportement standard Maven. Par contre les consommateurs des briques ne devront plus utiliser directement les artefacts des briques (archi-jar par exemple), mais les artefacts d'imports (archi-app-import). Les artefacts d'imports pourront englober plusieurs artefacts unitaires (exemple : archi-app-import contiendra archi-annotation, archi-log, archi-jar, etc...).
 Un autre problème en socle V13 est que tous les projets héritent de efluid-parent, or c'est efluid-parent qui décrit les versions des dépendances (via un dependencyManagement). Quand tout est synchronisé sur la même version cela ne pose pas de problème. Par contre dans l'intervalle de livraison des briques, des versions d'efluid-parent sont différentes, et donc les versions des dépendances également, ce qui cause des problèmes assez bizarres et difficilement détectables.
 Pour améliorer cette situation il faut qu'il y ait un lien fort entre applications->ecore->-archi->pom-parent, on doit donc tirer les informations du pom parent par l'archi uniquement, qui elle ne sera tirée que par ecore uniquement, et les applications ne tireront donc les versions que depuis ecore uniquement. Pour faire cela le pom efluid-parent a été divisé en deux : efluid-parent qui contient les plugins et les profiles, et efluid-bom (build of materiel) qui contient les versions de dépendances (via un dependencyManagementy). Ainsi on créé un nouveau module dans l'archi qui sera un bom au niveau archi, et qui tirera le bom efluid-bom. On fait de même dans ecore. Et dans efluid on tire le bom ecore-bom uniquement, et on ne donne plus aucune version directement, tout sera par le bom. La contrainte est que chaque modification de efluid-bom devra repasser par une release de l'archi, puis une release de ecore. Par contre toutes les briques/applications seront toujours cohérente entre elle.

 Guide de migration pour passer d'une archi V4 à V5 
 [Guide de migration]<!!!>category:guide
Liens vers le e-forum
Cette page wikefluid regroupe les diverses informations liées à la migration du projet efluid en UTF-8
Le fil de discussion principal sur ce sujet dans le forum :
Projet migration vers UTF-8

Gestion de l'encodage selon les versions
<p>
 branches des versions < 15.1 : se fait en ISO-8859-15
 branches des versions >= 15.1 : se fait en UTF-8
Un outil utf8helper a été créé afin de faciliter la conversion des fichiers ISO-8859-15 en UTF-8, notamment lors des reports de code. cf. paragraphe outil utf8helper<p>
Concernant les reports, un contrôle bloquant a été ajouté dans Gerrit pour s'assurer que les fichiers livrés soient toujours encodés en UTF-8.
Il y aura un -1 de l’UL en develop si jamais les fichiers ne sont pas bien encodés
⚠️️Attention⚠️: 
 Ce contrôle ne fonctionne que dans le sens branche maintenance → branche v15. 
 Aucun contrôle n'est ajouté pour vérifier l'encodage dans le sens branche v15 → branche de maintenance. Il convient de s'assurer que les fichiers reportés sur la branche de maintenance sont bien en ISO-8859-15

SqlMigrator et livraison de scripts
L'encodage des fichiers dépend de la branche / version :
 branches des versions < 15.1 : se fait en ISO-8859-15
 branches des versions >= 15.1 : se fait en UTF-8
 Il faut désormais éviter de spécifier dans les changeSet un encoding=windows-1252

livraison de scripts dans suivefluid : ISO-8859-15

Le fait de changer d'encodage modifie le checksum des fichiers qui se trouvent dans sql/database.
La problématique de compatibilité du checksum est prise en charge par l'UL.

Map4J et UTF-8
La lecture des squelettes dans Map4J peut planter suite à la migration UTF-8. En attendant la livraison de la correction dans Map4J, ajouter au fichier map4j/pom.xml l'argument suivant pour corriger le problème : 
  -Dfile.encoding=UTF-8

Intellij et .editorconfig
En raison du changement d'encodage à partir de la v15, il est vivement recommandé d'avoir au moins un workspace dédié pour les versions >= v15 
L'encodage automatique des fichiers créés sous IntelliJ est possible en paramétrant IntelliJ via le plugin editorconfig.
Il s'agit d'indiquer dans un fichier .editorconfig l'encodage utilisé :
  # Configuration par défaut de tous les fichiers
  [*]
  insert_final_newline = true
  indent_style = space
  indent_size = 2
  charset = utf8
Le fichier doit être créé à la racine du projet.
Il est également livré dans le repository efluid.
Pour le récupérer, il faut importer les modules qui nous intéressent et le projet racine. clea permet de récupérer la config .editorconfig.
Il faut également paramétrer chaque module importé dans Settings > Editor > File Encodings d'IntelliJ pour y indiquer l'encodage :
Fichier:FileEncoding-2.jpg

Eclipse et .editorconfig
Pour que le .editorConfig soit bien pris en compte sous Eclipse, il faut bien vérifier que les projets importés soit bien en "Inherited from container" et pas en "Other".
Fichier:EditorConfigEclipse.png
De plus, une version de Webby pour Eclipse compatible avec le passage en UTF-8 est disponible via l'update site :
https://github.com/roisoleil/m2eclipse-webby/raw/update-site/latest/ 
Informations complètes sur le forum : Eclipse pour JDK17

Outil utf8helper
Lors des reports ou de merges à partir d'anciens commits vers un référentiel UTF-8, on risque de récupérer des fichiers encodés en ISO-8859-15, et avoir un mix entre de l'UTF-8 et de l'ISO-8859-15 au sein même des fichiers.
Cela pose des problèmes dans les éditeurs et des problèmes pour livrer son code dans le référentiel UTF-8.
L'outil est à passer lors des cherry-picks, avant la résolution des conflits.
utf8helper a été créé pour convertir les fichiers en UTF-8 et résoudre ce genre de problème.
Il est disponible dans gerrit : utf8helper dans gerrit
Pour l'installer, dézipper simplement le jar et le fichier sh dans votre répertoire binaire (pour git bash) : C:\users\<nom>\bin
Vous pourrez le lancer sur des répertoires pour convertir automatiquement l'iso en utf8, tout en tentant de résoudre les soucis de double encodage utf8
Exemple d'utilisation : 
Dans cet exemple, la commande est exécutée dans le répertorie archi et convertira les fichiers .java et .properties uniquement.
  $ utf8helper.sh .
  Conversion automatique de fichiers en UTF-8.
  Usage :
     utf8helper <répertoire> [<répertoire> [...]]
  Liste des répertoires à prendre en compte :
     - D:\java\workspaces\1\archi\.
  Lecture du fichier de configuration : C:\Users\sackste\.utf8helper.conf
  Configuration :
     action=CONVERT_AND_CORRECT
     excludedDirectories=target, .git, .idea, properties2
     fileExtensions=java, properties
Plus de précisions sur le forum : aide à la résolution de conflits UTF-8/ISO

Adaptations concernant les batchs
Stratégie de portage / tests des batchs d'interface
 Quels batchs sont concernés ?
 les batchs traitant des données sans import / export de fichier n'ont à priori pas besoin d'être adaptés ni testés spécifiquement à la migration UTF-8
 les batchs créant et lisant des fichiers issus de systèmes tiers sont susceptibles d'être impactés par la migration UTF-8
 format json : normalement, les fichiers json sont nativement en UTF-8
 format XML : à priori, il faut laisser le parser traiter les fichiers. Malgré cela, il est peut-être nécessaire de tester / adapter le batch : 
 Quand l'encodage n'est pas défini dans la balise : dans ce cas, préciser l'encodage.
 Vérifier quel XML manager utilisé, notamment pour l'écriture du fichier XML qui pourrait être buggé (voir Guide migration UTF-8#Fichiers xml)
 tout autre format (.csv, .txt, etc.) : il est nécessaire d'adapter ces batchs pour forcer l'encodage ISO-8859-15

Comment les tester ?
 De manière générale :
 Faire en sorte que les fichiers contiennent des caractères accentués et le signe '€' (dans une zone commentaire par exemple, une adresse, etc...)
 Si besoin, pour des tests sur poste de dév, il est possible d'indiquer à la JVM l'encodage à utiliser pour lire/écrire des fichiers : (commande -DFileEncoding ='iso-8859-18' ou 'utf8' 
 Le batch devra être testé si on intervient sur l'écriture / lecture du fichier (forçage de l'encodage, ajout / suppression d'une balise définissant l'encodage, etc.)

 Travailler conjointement avec la recette : 
 Indiquer à la recette par le biais d'événements les batchs sur lesquels il faut mener une campagne de non-regression 
 Pour les batchs d'export :
 Idéalement, transmettre le fichier créé au système tiers pour vérifier qu'il est correctement lu. Cela nécessite de travailler avec la recette / chef de projet car le développement ne dispose pas forcément d'un SI Tiers, ni de l'environnement adéquat. 
 Pour les batchs d'import :
 Se procurer un fichier du SI Tiers et l'injecter pour voir s'il est lu correctement.
 Lorsque des TI métier existent sur le batch, il est fort probable qu'on puisse les utiliser pour tester. De plus, on dispose certainement de fichiers d'exemples.
 Dans tous les cas, s'assurer que l'encodage du fichier correspond bien à l'encodage du fichier fourni par le SI Tiers.

Batchs traitant des données sans import / export de fichier
Ces batchs n'ont à priori pas besoin d'être adaptés.

Batchs d'interfaces
Ces batchs échangent des données à partir de fichiers avec un système tiers.
Selon le format du fichier, il peut être nécessaire d'adapter ces batchs selon le format du fichier échangé,.
Fichiers json
Les fichiers json sont en principe encodés en UTF-8 par défaut.
Si un encodage est spécifié au sein du fichier, il est recommandé de le supprimer.

Fichiers xml
Il est recommandé de laisser faire le parser XML sans forcer l'encodage au niveau de la lecture / écriture des fichiers.
Généralement, les fichiers XML indiquent l'encodage du fichier dans l'entête, donc le parser devrait pouvoir se débrouiller seul.
Toutefois, certains fichiers peuvent être sans encodage dans l'entête. Dans ce cas :
 S'il s'agit de fichiers exportés (créés) : imposer un encodage UTF-8 et l'indiquer dans l'entête
 S'il s'agit de fichiers importés (lus) : la lecture se fait en UTF-8. Néanmoins, se renseigner vis-à-vis du client/système tiers si l'encodage ne peut pas être ajouté dans l'entête du fichier XML fourni.
 Les flux xml produits par efluid.net n'ont pas de balise définissant l'encodage : une adaptation est en cours côté échanges pour que les flux XML générés portent systématiquement l'encodage du fichier dans l'entête XML. Evenement 364430

⚠️ A surveiller :⚠️
 Pour l'import de fichiers XML
 Utilisation des AbstractXMLStreamMapper : l'encodage par défaut n'est pas forcément défini. Sans être précisé, ce devrait être l'encodage système (ou JVM) utilisé.
 L'encodage sera bien pris en compte lors de la lecture du fichier si la balise est présente dans le fichier.

 Pour l'export de fichiers XML
 Quand utilisation des SortedExportXMLStreamManager : se base sur du SortedExportFileManager qui est l'outil framework d'écriture XML (n'utilise pas le composant XML du jdk)
 l'encodage ne semble pas être défini par défaut. Si celui-ci n'est pas défini, on peut avoir un doute sur l'encodage du fichier exporté.
 bien vérifier que l'encodage est défini dans le gestionnaire d'export (que le setEncoding est utilisé)

 Il peut y avoir un bug en utilisant une classe héritant de AbstractXMLStreamMapper pour écrire le fichier XML : Le FileWriter instancié pour créer le fichier ne précise pas l'encodage du fichier, mais ajoute quand même la balise d'encodage lorsque l'encodage est défini.
Conséquence : le fichier xml contiendra la balise encodage 'iso-88-59-15' alors que le fichier sera écrit en UTF-8
Correctif en cours dans l'archi : https://gerrit.efluid.uem.lan/c/archi/+/207410 

Autres fichiers (fichiers textes divers, csv...)
Il est recommandé de forcer l'encodage des fichiers en ISO-8859-15 pour ne pas impacter les systèmes tiers.
cf. Forcer l'encodage pour savoir comment procéder pour forcer l'encodage

Forcer l'encodage
Généralement, ces batchs héritent de AbstractImportExportProgram.
 Pour l'import : 
 il suffit d'ajouter une valeur au properties du batch ENCODING_IMPORT = ISO-8859-15
 pour l'export :
 il suffit d'ajouter une valeur au properties du batch ENCODING_EXPORT = ISO-8859-15
 il faut ensuite paramétrer le FileManager en indiquant l'encodage forcé :
  SortedExportFichierPlatManager<String> mgr = new SortedExportFichierPlatManager<>(...)
  mgr.setEnforceEncoding(true);
  mgr.setEncoding(getEncodingFichierExporte());

NB  : il est toujours possible mais non recommandé de surcharger les triggers #getEncodingFichierImporte et #getEncodingFichierExporte

Si besoin (et pour tester), il est possible de lancer le batch en forçant l'encodage utilisé par la JVM pour lire/écrire les fichiers :
Au niveau du lanceur batch, ajouter dans les VM options l'option suivante : 
  "-Dfile.encoding=utf-8" (ou iso-8859-15 selon le cas souhaité).

Exemple de de forçage de l'encodage des fichiers pour les batchs d'import / export Viena : Change dans gerrit

Correction des tests en erreur
Le code source des tests a été encodé en UTF-8.
Il est nécessaire de convertir en UTF-8 les fichiers qui pourraient être en entrée de ces tests.

Dans le cas des tests les batchs d'interface : les fichiers en entrée / sortie des tests doivent suivre l'encodage précisé pour chaque batch (UTF-8 ou ISO-8859-15 selon les cas). 
Se référer au paragraphe Batchs d'interfaces<!!!>Category:groupe de développementCategory:workflow

Description du groupe

Le groupe Workflow est de plus en plus au cœur de l’application efluid et il a la responsabilité des domaines suivants :
 Workflow (WKF)
 Affaires Génériques (AFG)
 Maîtrise de l’Energie (MDE)
 ETinéraire (GRH)
 Liste de gestion (LDG)

Le domaine Workflow est un domaine regroupant une bibliothèque de composants techniques permettant de gérer des processus à étapes. Grâce à ces composants fortement réutilisables et fortement paramétrables, il est possible d'associer à un concept efluid (contrat, affaire...) un élément de population dont le cycle de vie est décrit par une suite d'étapes, ce que l'on appelle un workflow. L'élément de population évolue au sein de ce workflow au fur et à mesure de l'exécution des traitements que les utilisateurs auront programmés. Il passera donc par différentes étapes, avant d'atteindre une étape finale ou il n'évoluera plus (état "terminé" le plus souvent). Le workflow comporte un ensemble d'écrans opérationnels et de paramétrage ainsi qu'une partie batch (spécialisation de l'architecture batch classique) permettant d'exécuter des traitements en masse. Ces composants, fruit d'une conception avancée, sont très fortement paramétrable, ce qui leur offre une grande capacité à traiter de multiples cas fonctionnels. On retrouve des workflow plus particulièrement dans les domaines Contrat, Interventions, Contentieux et Agence en ligne, la tendance est à une réutilisation maximum de ce type d'approche.

Le domaine Affaires Génériques permet quand à lui de gérer des objets métier dont une grande partie de la structure (les attributs) est définie par paramétrage. Il pourrait en fait être nommé Modèle Objet métier, car il ne s'applique pas uniquement aux affaires. Grâce aux composants de ce domaine fonctionnel, il est possible d'utiliser les attributs statiques des objets java (attributs du code) et d'ajouter de nouveaux attributs dits "dynamiques" par paramétrage. Les IHM de gestion de ces objets paramétrables est elle aussi dynamique et il est possible de paramétrer le comportement de ces IHM générées dynamiquement. Un exemple pour illustrer cela est le marketing. On peut imaginer vouloir créer une affaire de marketing à un instant T, pour promouvoir une nouvelle offre de commercialisation. Pour cela il suffit de configurer les attributs nécessaires dans une affaire générique que l'on qualifiera d'affaire marketing et on pourra faire évoluer cette affaire au travers d'un workflow qui pourra par exemple envoyer des courriers aux clients, puis ensuite recevoir les réponses des clients au travers d'un questionnaire, les exploiter en offrant des possibilités de relance ou d'établissement de propositions commerciales / souscription de services, etc...

Le domaine MDE se base totalement sur l'utilisation du workflow et des affaires génériques. Il permet de gérer les certificats d'économie d'énergie et de traiter les processus relatifs à la Maîtrise De l'Energie. Une partie des traitements est disponible dans efluid, et l'autre se trouve dans le portail partenaire. Le portail permet aux partenaires (électriciens, chauffagiste...) de créer des affaires MDE en renseignant les informations des travaux qu'ils ont effectué (exemple : pose d'un chauffe-eau solaire). Ces affaires entrent ensuite dans un workflow qui permettra aux agents back-office de lancer des traitements, d'établir des rapports à destination des organismes de l'état, de suivre l'activité MDE, mais aussi également aux partenaires, qui pourront donc interagir avec le distributeur pour entre autres obtenir les aides de l'état incitant à la maîtrise de l'énergie.

Le domaine eTinéraire permet de gérer l'activité de recrutement (pour le compte de l'UEM exclusivement à ce jour). D'une part dans efluid ou l'on va retrouver les processus de gestion des annonces, des candidats, des candidatures ou des rendez-vous programmés dans un objectif de suivi, et d'autre part via le "portail recrutement" ou les candidats peuvent postuler aux annonces en fournissant CV, lettre de motivations et autres informations relatives etc. Ce domaine se base lui-aussi sur l'utilisation massive du workflow et des affaires génériques.

 Pilotage 

 Reste à faire 

Détails

Qualité

Événements supports

 WKF : 50257
 AFG : 68088
 GRH : 65604
 MDE : 52591
 LDG : 112717

Métriques du groupe

 Rapport sonar ecore workflow
 Rapport sonar efluid workflow

Règles qualité

 Pour la définition des classes, utiliser un cartouche du type :
/**
 * Écrire ici la description succincte qui sera indiquée dans la javadoc .
 * 
 * (Au besoin ici la description plus complète)
 */

Fichier:JavadocWorkflow.png

 Ne pas hésiter à faire un Ctrl+i pour remettre une indentation plus "propre"
 Idem avec Ctrl+Shift+o pour mieux organiser les imports et supprimer ceux qui sont inutilisés
 Ajouter l'annotation @Override au dessus de la déclaration d'une methode lorsqu'il s'agit d'une redéfinition de méthode de superclasse
 Utiliser au minimum les @SuppressWarnings et autre //$NON-NLS-1$, modifier plutôt la configuration des warnings Eclipse dans Windows/Preference puis Java/Compiler/Errors-Warnings
 Nettoyer les "// TODO Auto-generated method stub"
 Ne pas utiliser de code en dur : créer des constantes si nécessaire
 Ne laisser pas du code en commentaire
 Chaque méthode doit être documentée par une javadoc, soyez succint mais ne vous contentez pas de reprendre son nom ; n'y mettez pas les arguments @return et @param, sauf si cela apporte vraiment une information utile bien sûr. En général il est préférable de commenter ce que fait la méthode à ce niveau, plutôt que de mettre des commentaires dans le corps de la méthode.

Planification des revues de code

 Planification revue de code - nouveaux devs

 Évènement Libellé Développeur Relecteur Revue Correction{{#for_external_table:
-
 
 
 
 
 Revue de code:
  }}

 Planification revue de code - JPAR
 évènement libellé developpeur relecteur relecture correction 104678 GRH - création d'opération par copie JPAR CBO Revue de code:104678 

 Planification revue de code - Maintenance CBO

 évènement libellé developpeur relecteur relecture correction 66216 GRH - ajouter un filtre "demandeur" en recherche d'annonce CBO TCO Revue de code:66216 

 Planification revue de code - Maintenance MHA

 évènement libellé developpeur relecteur relecture correction 76578 G10-RC anomalie double voir + sur EDP reconduction contrat MHA TCO Revue de code:76578  67204 échec d'éxécution de batch WKF999 (problème sur condition d'execution sur critères d'attributs dynamiques) MHA JFL Revue de code:67204 

 Planification revue de code - Autres

 évènement libellé developpeur relecteur relecture correction 118129 Impossible de valoriser en masse des modèles d'attributs d'affaires génériques MMA JTA Revue de code:118129  134558 Dépendances entre campagne et workflow via le type de lot de workflow MMA PPA Revue de code:134558 135728 Le tri des anomalies sur les EDP workflow doit être fait sur le statut de celles-ci CPI PPA Revue de code:135728

 Revues de commit 

 Chaque commit sur la develop doit être validé par un membre de l’équipe (que ce soit pour efluid, AEL, ecore, efluidPub, edk etc…) et pour n’importe quel type d’évènement. C’est aussi valable pour les script ebuild.

 C'est au développeur de choisir la personne qui validera le commit, selon le type d’évènement, et les dispos (congé etc…) de chacun. Si vous ne savez pas à qui envoyer, envoyez à tout le groupe.

 Cela se fera sous la forme d'un mail envoyé depuis le tableau de récapitulation : Revue des commit du groupe Workflow
Il suffit de cliquer sur la petite enveloppe près du numéro de l’évenement listé dans le tableau.

 Un évènement considéré comme critique ne peux pas passer livrer-it sans cette validation préalable.

 Pour qu'un événement apparaisse le tableau Revue des commit du groupe Workflow, il faut se positionner sur l’onglet « analyse des problèmes » et noter dans le bloc « explication technique » : Validation WKF {nom du projet} {branche} : {id du commit}
Avec 

- {nom du projet} = au choix : efluid / AEL / efluidPub / ecore / edk ou autre

- {branche} = le nom de la branche : develop / HERMES_11 ..

- {id du commit} = l’id du commit, sachant que les 7 premiers chiffres suffisent

 Réunion de groupe 
 CR de la réunion du 28 novembre 2011
 CR de la réunion du 21 novembre 2011<!!!>Estimation des RAF
Category:groupe de développement

Règles de développement

 Docs 
Gestion des limitations dans le CRI

Webservices utilisés dans le domaine

Liste de tutos Intervention

Tests unitaires
Il est proposé de mettre en place les tests unitaires dès qu'on modifie/crée une API dans le cadre de la réalisation d'un événement (que ce soit un nouveau développement ou une anomalie à corriger).
Actuellement, nous n'avons aucun recul pour pouvoir estimer le temps passé sur les tests unitaires par rapport à un développemment.
Pour pouvoir avoir cette information, il faut créer un événement fils ayant les caractéristiques suivantes :
 type : qualité 
 libellé : 'tests unitaires' 
Lorsqu'on réalise le test unitaires, on impute son temps comme du développement sur l'évt 'tests unitaires'.
De cette manière, on peut distinguer le temps passé pour les tests unitaires du temps passé pour le développement lui-même.

Documentations et outils concernant les tests unitaires
Page Tests logiciels dans wikefluid
Section dédiée aux tests unitaires dans le Forum 
Outils disponibles
 Page des tests unitaires de l'usine logicielle

 Tests unitaires qui posent des difficultés 
Pour les tests unitaires ci-dessous, il est proposé d'en débattre afin de trouver la meilleure manière de les prendre en compte. Ils peuvent être ajoutés ici suite à une revue de code où des questions sur la manière de faire le test unitaire, ou tout simplement parce qu'on n'a pas réussi à réaliser le test unitaire.

 Tests intégrations 

 Tests JMeter  
 stage test JMeter JNE

 Trucs et astuces 

 Classe de test développeur pour les traitements workflow déporté en JMS 

Il existe une classe de test « développeur » qui permet d’appeler un traitement comme si on dépilait le message dans la file JMS suite au déport de l'exécution du traitement workflow.
La classe de test : TestITTraitementWorkflowDeporteJMS
Pour la lancer, il est nécessaire de modifier les 3 méthodes en début de classe, qui permettent d’indiquer l’intervention, la campagne, et le traitement qui sera exécuté.

 Requêtes utiles 
 Changement de statut d'une affaire et insertion dans taction 
UPDATE TAFFAIRE
SET STATUT          = 1,
  ACTEURMODIFICATION='evt 155981',
  DATEFIN           = CURRENT_DATE,
  DATEMODIFICATION  = CURRENT_DATE
WHERE REFERENCE     ='2086383';

INSERT
INTO TACTION
  (
    ID,
    ETATOBJET,
    ACTEURCREATION,
    ACTEURMODIFICATION,
    DATECREATION,
    DATEMODIFICATION,
    CODEFOURNISSEUR,
    CODEGRD,
    STATUT,
    REFERENCE,
    DATERDVDEBUT,
    DATERDVFIN,
    DATEOBJECTIF,
    OBJETMAITRE_ID,
    OBJETMAITRE_ROLE,
    RESPONSABLE_ID,
    RESPONSABLE_ROLE,
    DESTINATAIRERDV_ID,
    DESTINATAIRERDV_ROLE,
    OBJET,
    OBSERVATIONS,
    ACTEURSUPPRESSION,
    DATESUPPRESSION,
    AGENCE,
    DEMANDEDEPRISEENCOMPTE,
    ACTIONPUBLIEE,
    PUBLIEEPARFOURNISSEUR,
    REPONSEINTEGREEPARWORKFLOW,
    PARAMETRAGEACTION_ID,
    PARAMETRAGEACTION_ROLE
  )
  VALUES
  (
    uniqueid.nextval,
    '0',
    'statut|evt|155981',
    NULL,
    CURRENT_DATE,
    NULL,
    'F-ES',
    'G-ES',
    '2',
    seq_ref_action.nextval ,
    NULL,
    NULL,
    NULL,
    (SELECT ID FROM TAFFAIRE WHERE REFERENCE = '2086383'),
    (SELECT ROLE FROM TAFFAIRE WHERE REFERENCE = '2086383'),
    NULL,
    NULL,
    NULL,
    NULL,
    'affaire terminée',
    NULL,
    NULL,
    NULL,
    NULL,
    '0',
    NULL,
    '0',
    '0',
    'MDLACTINFO',
    'com.hermes.ref.affaireaction.businessobject.ModeleAction'
  );

Qualité

Métriques du groupe

 Rapport sonar intervention

Différents modes de planification et ordre des mises à jour des autres objets
Dans le tableau ci dessous, les numéros représentant l'ordre naturel des mises à jour selon l'objet pivot (numéro 1) de la mise à jour
 modePlanification Intervention  Tentatives activées    Tentatives non activées  Réservation      type Moteur Planification planification par défaut 1 dateDebut  2 dateDebut recopiée    N/A  N/A      à l'ancienne, fichier excel planification par créneau 2 dateDebut recopiée  3 dateDebut recopiée    N/A  1 dateDebut      planification interne avec réservations displanisWeb 2 dateDebut recopiée  1 dateDebut    N/A  N/A      planification externe avec displanis divagamWeb 2 dateDebut recopiée  1 dateDebut    N/A  N/A      planification externe avec divagam<!!!>Category:groupe de développement

Qualité/développement
 Guides et conventions
Lien vers le guide du nouveau développeur
Lien vers les conventions de code efluid
Lien vers la page de qualité générale
Lien vers la page dédiée aux tests logiciels
Lien vers le guide ARC300MT
Lien vers la doc archi
Lien vers le guide de modularisation batch

Trucs et astuces
 Créer des énumérés persistants
 Ajouter un attribut sur un business object (à actualiser)
 Modifier des attributs sur un business object (à actualiser)
 Supprimer des getters sur un business object (à actualiser)
 Créer un paramètre entreprise avec critères
 Créer un paramètre entreprise sans critères avec énuméré statique
 Lancer un batch en mode deployé

Git/Gerrit
 Convention de nommage des commit

SQL et outils associés
Tableau de scripts réutilisables
Fragments de code sql

Tests/Performances
Scénarios JMeter Facturation

F.A.Q.
 Foire aux questions expertise/dév facturation<!!!>Category:groupe de développement

Trucs et astuces
 Requêtes utiles 
 Requêtes SQL utiles domaine consommation
 Requêtes SQL utiles domaine BGE
 Requêtes SQL utiles domaine matériel
 Requêtes SQL utiles domaine relève
 Requêtes SQL utiles domaine reconstitution des flux
 Requêtes SQL utiles pour des informations de suivi dans Suivefluid

 Requête pour suivre l'avancement d'un batch v3 

table BATCH_RELEVE
SELECT code_batch, COUNT(*) "nb éléments", ROUND(100 * COUNT(*) / sum(count(*)) over(partition by code_batch) ,2) "%", CASE WHEN TECH_STATUT=1 THEN 'à traiter' ELSE 'traité' END "statut" 
FROM BATCH_RELEVE tmp GROUP BY code_batch, TECH_STATUT 
ORDER BY code_batch, TECH_STATUT;

table BATCH_PDS 
SELECT code_batch, COUNT(*) "nb éléments", ROUND(100 * COUNT(*) / sum(count(*)) over(partition by code_batch) ,2) "%", CASE WHEN TECH_STATUT=1 THEN 'à traiter' ELSE 'traité' END "statut" 
FROM BATCH_PDS tmp GROUP BY code_batch, TECH_STATUT 
ORDER BY code_batch, TECH_STATUT;

table BATCH_CONTRAT
SELECT code_batch, COUNT(*) "nb éléments", ROUND(100 * COUNT(*) / sum(count(*)) over(partition by code_batch) ,2) "%", CASE WHEN TECH_STATUT=1 THEN 'à traiter' ELSE 'traité' END "statut" 
FROM BATCH_CONTRAT tmp GROUP BY code_batch, TECH_STATUT 
ORDER BY code_batch, TECH_STATUT;

table BATCH_ENERGIE_NON_FACTURE
SELECT code_batch, COUNT(*) "nb éléments", 	ROUND(100   * COUNT(*) / sum(count(*)) over(partition by code_batch) ,2) "%", CASE WHEN TECH_STATUT=1 THEN 'à traiter' ELSE 'traité' END "statut" 
FROM BATCH_ENERGIE_NON_FACTURE tmp GROUP BY code_batch, TECH_STATUT 
ORDER BY code_batch, TECH_STATUT;

table BATCH_PROJECTION_COURBE
SELECT code_batch, COUNT(*) "nb éléments", 	ROUND(100   * COUNT(*) / sum(count(*)) over(partition by code_batch) ,2) "%", 	CASE 		WHEN TECH_STATUT=1 		THEN 'à traiter' 		ELSE 'traité' 	END "statut" 
FROM BATCH_PROJECTION_COURBE tmp GROUP BY code_batch, TECH_STATUT 
ORDER BY code_batch, TECH_STATUT;

 Liens vers les Modèles Physiques de Données 
 CONSOMMATION :  > efluid - Documentation Technique > DCT suite efluid > DCT efluid > Domaine consommation > visio

http://wperoom3.uem.lan/eRoomReq/Files/Production/DocTechniqueEfluid/0_18f4c3/MPDconso.vsd
 RELEVE :  > efluid - Documentation Technique > DCT suite efluid > DCT efluid > Domaine relève > conception

http://wperoom3.uem.lan/eRoomReq/Files/Production/DocTechniqueEfluid/0_1ab3d8/MPDreleve.vsd
 MATERIEL :  > efluid - Documentation Technique > DCT suite efluid > DCT efluid > Domaine matériel > conception > visio

http://wperoom3.uem.lan/eRoomReq/Files/Production/DocTechniqueEfluid/0_1ab2b2/MPDmateriel.vsd

Règles de développement

 Suppression batchs obsolètes 
Liste des batchs à retrouver en recherchant "Suppression batch*" dans suivefluid
Suppression batch conso obsolète

 Sécurisation des chargements dans les batchs 
Sécurisation des chargements dans les batchs

 Liste des rôles hashcodés 
On peut rapidement voir la liste des colonnes hashcodées pour toutes les tables du goupe dans la classe ConsommationRoleDAOUtils.
Ci-dessous un rappel du contenu de la classe (à maintenir donc...)
 Table TRELEVE 
 PACALENDRIER_ROLE
 ACTEUR_ROLE

 Table TGRANDEURPHYSIQUEGENERALE 
 ROLE

 Table TINFORELEVE 
 PACALENDRIER_ROLE

 Table TGRANDEURPHYSIQUEGENERALEINFOR 
 ROLE

 Table TCONSOMMATIONMENSUELLE
 PACALENDRIER_ROLE_ROLE

 Table TELEMENTDEPOPULATIONRELEVE 
 POINTDESERVICE_ROLE

 Table TPACALENDRIER
 ROLE

Documentations et outils concernant les tests unitaires
Page Tests logiciels dans wikefluid
Section dédiée aux tests unitaires dans le Forum 
Outils disponibles
 Page des tests unitaires de l'usine logicielle 
 couverture des tests unitaires dans Sonar 

 Tests de performance TP 
Les tests de performances TP sont rélaisés à l'aide de scénarios JMeter. Ceux-ci peuvent également servir à effectuer des tests de non-regression.
La liste des scénarios JMeter réalisé est disponible sur la page des scénarios CNS

 Sujet de stage / alternance 
 enrichir les process de chargement (avec méthode des Loader)
 refonte du chargement de l'historique de relève pour éviter de recharger l'historique entier à la validation d'une relève)
 revoir le chargement de l'historique de relève en TP<!!!>Category:groupe de développement

 Pilotage de la section 
 comment ça marche ?
 réunion mensuelle section RLC_MNS

Scripts SQL
Fragments de code sql des groupes

 Qualité 
 taches de fond

 Evénements supports 
 RLC : 31374
 CTX : 31372

Métriques de la section
 sonar du groupe

 Règles qualité 
 Lire Coder proprement
 Lire les conventions de codage du groupe
 Lire les bonnes pratiques du groupe
 Optimisation de code

 Revues de code 

 Configuration et procédure des revues
 Planification des revues de code du groupe Mensu Recouvrement

 Reste à faire <!!!>Category:groupe de développement

 Présentation du groupe 
Organisation de la division Comptabilité
Membres du groupe

 Pilotage du groupe 
 Description  
 Pilotage Maintenance
 Pilotage nouveaux développements
 Pilotage qualité
 Pilotage Projets

 Résumé des outils de pilotage utilisés dans la division 
 Requêtes suivefluid
 Boite mail partagée
 Chaîne slack "coordianaltion"
 Chaîne slack "anomalies-compta"
 Chaîne slack "prestations-compta"
 Fichier Excel prévisionel

 Trucs et astuces 

 Règles de développement 
 Règles et bonnes paratiques de codage|Règles et bonnes pratiques de codage
 Dès le début d’un nouveau dév : confronter son idée de conception à un autre développeur : même si on n'est pas bloqué
 En cours de dév :  faire valider l'avancement du dev, les changements éventuels de conception, etc.
 En fin de dev : faire relire son code par un autre développeur. Planification des revues de code

 Trucs et astuces sql 
 Scripts sql récurrents
 Liste des scripts génériques cac
 scripts sql permettant de faire le lien entre les types de ligne et les types de transaction
 utilisation des templates dans SQL developer

 Trucs et astuces test 
 Point à verifier lors de la recette du Batch REG004MT
 Lancer un batch en mode deployé

 Trucs et astuces batch 
 Recommandations sur la conception d'un batch MT
 Check liste livraison nouveau batch
 Check liste livraison maintenance

 Check liste ajout d'un nouveau type de transaction
 Ajouter une nouvelle valeur dans l'énuméré persistant correspondant
 Définir avec l'analyste les critères dont on aura besoin pour traiter ce type de transaction
 Modifier le batch IC0001MT pour intégrer les nouvelles requetes
 Modifier le paramétrage entreprise des journaux
 Modifier le paramétrage entreprise des comptes tiers 
 Faire une notes de paramétrage pour traiter ce nouveau type de transaction (recette) 

 Trucs et astuces : partage de connaissances 
 Wiki
 eForum
 eRoom
 Boite mail partagée, sous répertoire "partage de connaissances"
 OneNote (de la division)<!!!>Category:groupe de développement

Qualité

Règles qualité

 Pour la définition des classes, utiliser un cartouche du type :
/**
 * <p>Title: {nom de la classe}</p>
 * <p>Description: {description}</p>
 */

 Ne pas hésiter à faire un Ctrl+i pour remettre une indentation plus "propre"
 A l'inverse éviter le formatage avec Ctrl+Maj+f car cela fait des sauts de ligne inutile dans le code
 Idem avec Ctrl+Shift+o pour mieux organiser les imports et supprimer ceux qui sont inutilisés
 Ajouter l'annotation @Override au dessus de la déclaration d'une methode lorsqu'il s'agit d'une redéfinition de méthode de superclasse
 Utiliser au minimum les @SuppressWarnings et autre //$NON-NLS-1$, modifier plutôt la configuration des warnings Eclipse dans Windows/Preference puis Java/Compiler/Errors-Warnings
 Nettoyer les "// TODO Auto-generated method stub"
 Le code « en dur » est à bannir, utiliser plutôt des constantes

Planification des revues de code

 Planification revue de code - ESI
AFD_écart_evt74436.doc

 évènement libellé developpeur relecteur relecture correction 77382 74436 - 3 - Création EPTypeEnvironnement ESI CBO   77384 74436 - 4 - Modèles d'environnement ESI CBO Revue de code:77384_77386  77386 74436 - 5 - Création EEtatEnvironnement ESI CBO   77386 74436 - 6 - Environnement (sans traitements récurrents ni mail) ESI CBO Revue de code:77384_77386  77387 74436 - 7 - Page d'accès aux environnements ESI CBO Revue de code:74436  77388 74436 - 8 - Modèles de fiche environnement ESI CBO Revue de code:77388  77389 74436 - 9 - Consultation fiche environnement depuis la page d'accès aux environnement ESI CBO Revue de code:77389  77392 74436 - 11 - Modes de traitement ESI CBO   77393 74436 - 12 - Traitements récurrents ESI CBO   77394 74436 - 13 - Création ETypeOrdonnanceur ESI CBO   77395 74436 - 14 - Ordonnanceur ESI CBO   77396 74436 - 15 - Performances Ordonnanceur ESI CBO   77399 74436 - 16 - Listes de diffusion ESI CBO   77707 74436 - 20 - Habilitations ESI CBO  <!!!>Category:groupe de développement

 Mise à jour de la documentation  
Il est indispensable de mettre à jour la documentation après un développement nouvelle fonction voire après une correction de code.
Celle-ci doit être vérifiée avec la revue de code de chaque développement (découpage technique présent pour chaque développement).

Les principaux documents à mettre à jour lors d’un développement sont :
Les afd produit :
afd offre produit (efluid) 
afd moteur de facturation (efluid) 
afd valorisation (ecore) 
afd tarif (ecore) 
afd algorithme tarification (ecore) 
Le guide utilisateur offre 
Eventuels DCT (efluid et ecore) 
L’afd de l’écart s’il y a des différences entre le développement et l’analyse

 Estimation des RAF  

 Estimation des RAF par développeur 

 Estimation des RAF lot 12 

Détails

Suivi groupe offre moteur
SEFBot:Suivi OFFRE MOT

Qualité
Scripts sql récurrents du domaine offre
 Connexions aux bases clients avec SQL+
 Scripts sql récurrents du domaine offre
Métriques du groupe
 Rapport sonar offre-moteur
 Rapport sonar valorisation

Règles de gestion du paramétrage offre
 Règles de gestion du paramètre offre<!!!>Category:groupe de développement

 Mise à jour de la documentation  
Il est indispensable de mettre à jour la documentation après un développement nouvelle fonction voire après une correction de code.
Celle-ci doit être vérifiée avec la revue de code de chaque développement (découpage technique présent pour chaque développement).

Les principaux documents à mettre à jour lors d’un développement sont :
L'afd éditique ]
Les guides utilisateur édition 
Les DCT 
Le référentiel des éditions 
Les modèles Rose dans ecore et efluid (disponible dans le reposetory git documentation)
L’afd de l’écart s’il y a des différences entre le développement et l’analyse

Qualité
Procédure de revue de code
 Procédure de revue de code

Métriques du groupe
 Rapport sonar éditique

 Outils techniques 

 Pour tester un flux depuis son poste 

curl -i --request POST --data @chemin/local/vers/mon/fichier_flux.xml http://lrstredt3.uem.lan:49080/hermes > mon_doc.pdf

 Demander l'accès un serveur de développement Exstream 

Suite à la mise en place d'un nouveau serveur de développement Exstream, il convient de demander les droits d'accès à ce serveur pour pouvoir l'utiliser avec Control Center, en accès via l'explorateur de fichier et en accès aux partages de fichier.
Pour cela, il faut :
 Ouvrir une demande FootPrints :
 Depuis le catalogue des services, suivre "Demandes de services", puis "Demande d'accès", puis "Ouverture de flux réseau".
 Dans l'onglet description de la demande qui s'ouvre, un modèle de matrice de flux est proposé. Il convient de créer une nouvelle matrice à partir de ce modèle pour répindre au nouveau besoin avec les lignes suivantes :
 Version  Identifiant  Site SOURCE  Zone réseau SOURCE  FQDN ou nom SOURCE  Site CIBLE  Zone réseau CIBLE  FQDN ou nom serveur logique CIBLE  Protocole  Port V1.0  U1  CGI (Paris et Toulouse)  L2L_CGI_STDENIS  Poste de travail  UEM  inside  ldsrvedt2.uem.lan  SMB  445 V1.0  U2  CGI (Paris et Toulouse)  L2L_CGI_STDENIS  Poste de travail  UEM  inside  ldsrvedt2.uem.lan  SSH  22 V1.0  U3  CGI (Paris et Toulouse)  L2L_CGI_STDENIS  Poste de travail  UEM  inside  ldsrvedt2.uem.lan  TCP  28800

 e2c : composant d'édition efluid 

Cette page est dédié aux informations relatives à e2c, le  module d'édition efluid introduit en v15, destiné à remplacer ExStream à terme.

 e2c : Lancement des test jest (IHM JS) avec VSCode  

Configuré l'environnement en suivant la doc archi-front-JS

vignette
Avec les extensions les tests jest se lance automatiquement à la sauvegarde du fichier. Il est possible de les relancer séparément ou en débug a l'aide des boutons situé au dessus du nom de la fonction ou dans la marge (voir image ci-contre)

/!\ Pour que les extension jest fonctionne correctement, le dossier ouvert dans VSCode doit être celui contenant le fichier "package.json" /!\

 edoc : validation technique d'une nouvelle version de SolR 

La procédure de validation technique d'une nouvelle version de SolR est disponible ici : https://gerrit.efluid.uem.lan/plugins/gitiles/edoc/+/refs/heads/develop/documentation/README.md<!!!>Category:groupe de développement échanges
Category:groupe de développement
Category:échanges

L'objectif de cette page est de regrouper l'ensemble des informations utiles aux développeurs du groupe échange.

 Guide du développeur échanges 
 Le nouveau développeur échanges 
Si tu viens d'intégrer le groupe échange, ce chapitre est fait pour toi. Tout d'abord bienvenue, bravo et surtout bon courage ! :)

 Description du domaine "échanges" 
Avant de plonger dans ce nouveau monde, qui je le sais, te passionnes déjà, voyons tout d'abord ce que c'est que "le domaine échanges".

Le domaine Échanges traite de toute la problématique d'interactions entre efluid (ou une application de la suite efluid) et une autre application (externe ou une application de la suite efluid).

Ces interactions se font sous différentes formes :
 Des échanges de flux SOAP (efluidPub <-> efluid.net) ;
 Des échanges de fichiers (XML, CSV) via mail, FTP, ou autre (publication ou consommation d'information vers ou en provenance d'un système externe) ;
 Des appels ou des mises à disposition de services web.

Ça ne semble pas bien compliqué comme ça, mais avant de commencer à coder avec tes petits doigts tout neufs il est nécessaire de comprendre un peu plus en détail ce que font tes collègues. C'est pas ce qu'il y a de plus marrant, mais tout le monde y est passé, il y a pas de raison que t'y échappes.

Pour cela, il t'es demandé de parcourir les différents chapitres ci-dessous et d'en maîtriser le contenu.
 Lire les documents d'Analyse Fonctionnelle Détaillée (AFD) 
Il est demandé et nécessaire de maîtriser et de bien comprendre les concepts exposés dans les deux AFD suivantes :
RoomNom du documentCommentairesefluid.netAFD - Analyse globale du système d'échanges.docxefluid.netAFD - Demande de publication.docxefluid.netCartographie des flux efluid.net.xslxListe l'ensemble des flux du domaine échanges
En complément, les AFD présentes dans la room suivante sont également à lire : room efluid.net

 Lire les Documents de Conception Technique (DCT) 
Il est demandé et nécessaire de maîtriser et de bien comprendre les concepts exposés dans les DCT suivants :
RoomNom du documentCommentairesedkDCT - EDK - système d'échanges- concept.docedkDCT - EDK - Echaînement - echanger.docxedkDCT - EDK - Echaînement - Gérer abonnement modèle de service.docedkDCT - EDK - Echaînement - Gérer lots échanges.docedkDCT - EDK - Echaînement - Gérer modèle d'échange.docedkDCT - EDK - Echaînement - suivi abonnement.docedkDCT - EDK - Echaînement - Transfomation XSLT.docedkDCT - EDK - Echaînement - Echanges de masse.docxTrès important !
En complément, les DCT présents dans les rooms suivantes sont également à lire : 
 room conception edk
 room conception efluid.net

 Comprendre les flux d'échanges 
Lire et comprendre la page suivante : Comprendre les flux d'échanges

 Installer son environnement de développement 
Maintenant que tu au niveau littéraire, tu peux passer à l'étape de configuration de ton nouvel environnement de travail.

Il y a une page dédiée pour ça, c'est par ici : Installation du poste de développeur échange

 Exécuter la campagne de non-régression d'efluid.net 
Afin de découvrir l'application efluid.net et la mise en œuvre d'un certain nombre de concepts vu au cours des phases de lecture d'AFD et DCT, il est demandé d'exécuter la campagne de non-régression de l'application.

 Suivre le programme d'exercices du groupe échanges 
Il ne reste plus qu'à se lancer et s'exercer sur les sujets suivants :
 Exercice de création d'un flux de masse.docx
 Exercice de création d'un flux de masse.docx

 Le développeur échanges confirmé 
 Feuille de route du développeur 
Entant que développeur confirmé sur le projet efluid et donc du groupe échanges, je suis responsable des événements qui me sont confiés.
On entant par confié le fait d'être positionné comme développeur sur le dit événement.
Cela signifie que :
 Pour un nouveau développement :
 Je suis responsable de réaliser le chiffrage définitif à faire valider par TMAN en proposant un découpage en élément de chiffrage conformément au chapitre "Chiffrage et découpage en éléments de chiffrage" ci-dessous ;
 Je suis responsable de réaliser le développement conformément à l'AFD et conformément aux normes de conception et développement vues en formation et partagées au sein du projet efluid et de l'équipe de développement échanges ;
 Je suis responsable de proposer un DCT si le développement le nécessite : à présenter et faire valider en CT au besoin ;
 [Nouveau] Je suis responsable de mettre à jour les AFD impactées par mon développement à la fin de celui-ci ;
 [Nouveau] Je suis responsable de demander à l'analyste de mettre à jour l'AFD de l'écart que je suis entrain de développer si celle-ci est incomplète pendant la phase de développement ;
 Je suis responsable d'analyser les risques en termes de performance de mon développement et si cela est nécessaire, de mobiliser l'équipe performances pour organiser la réalisation de tests avec eux ;
 Je suis responsable de remonter les alertes quant à un possible dépassement de charge ou non livraison dans la version ciblée : au chef de projet, à l'expertise, à la coordination si besoin, etc... Remonter une alerte ça signifie anticiper une situation qui dérive : alerter le jour du ramassage n'est que peu productif ;
 Je suis responsable de la communication, tout au long de la phase de développement, avec l'expertise : pour obtenir des contextes de test, faire une démo, demander des compléments d'analyse, etc...
 Pour une correction d'anomalie :
 Je suis responsable de réaliser le développement conformément à l'AFD et conformément aux normes de conception et développement vues en formation et partagées au sein du projet efluid et de l'équipe de développement échanges ;
 Je suis responsable de l'ajout, lorsque cela est nécessaire, de TU ou TI permettant de couvrir le cas générateur de l'anomalie à l'avenir ;
 Je suis responsable de la bonne livraison de l'événement : dans toutes les versions attendues ;
 [Nouveau] Je suis responsable de mettre à jour les AFD impactées par mon développement à la fin de celui-ci ;
 Je suis responsable de remonter les alertes quant à une possible non livraison dans la version ciblée : au chef de projet, à l'expertise, à la coordination si besoin, etc... Remonter une alerte ça signifie anticiper une situation qui dérive : alerter le jour du ramassage n'est que peu productif ;
 Je suis responsable de la communication, tout au long de la phase de développement, avec l'expertise : pour obtenir des contextes de test, faire une démo, demander des compléments d'analyse et relotir au besoin. Là aussi, une situation menant à un relotissement s'anticipe : se rendre compte après le ramassage qu'on a pas pu livrer un événement n'est que peu productif.
 Pour le suivi de projet :
 Je participe aux réunions de suivi hebdomadaire du groupe de développement et me responsabilise, régulièrement, sur la rédaction de l'ODJ et du CR de la réunion ;
 Je participe régulièrement au turnover hebdomadaire du support des équipes d'expertise échanges.

 Norme de qualité de développement 
Les normes de qualité de développement sont celles en oeuvre pour l'ensemble de la filière "Applications et Flux".

On en trouvera une description ici :

http://wikefluid/index.php/Revues_de_code_de_la_fili%C3%A8re_%22Applications%22

Les règles de nommage spécifiques du groupe échanges sont proposées ici :

http://wikefluid.uem.lan/index.php/R%C3%A8gles_nommage_domaine_%C3%A9change#Autres

 Revue de code sous Gerrit 
Les revues de code sous Gerrit devront être menées afin de faire respecter les principes exposés dans les deux pages listées précédemment.

Il est demandé à chaque relecteur de porter une attention particulière à ce que les règles suivantes soient, à minima, bien respectées lors d'une revue de code sur tout change de branche develop (Sur une branche de maintenance, la priorité est souvent donné à la stabilité du code et une livraison rapide) :
 Le formatage imposé par le formateur Efluid (cf. Formateur efluid pour Eclipse) est bien respecté, sinon demander un CTRL+SHIFT+F sur la classe ;
 L'organisation des imports de classe est correcte, sinon demander un CTRL+SHIFT+O ;
 Les attributs, méthodes, paramètres de méthodes, etc... sont écrits en français ;
 Les messages applicatifs sont bien traités via le système d'internationalisation ;
 Les méthodes sont bien découpées et ne font, sauf exception, pas plus de 10 lignes ;
 Des tests (TU et/ou TI) couvrent le maximum de code et de cas possible.

 Chiffrage et découpage en éléments de chiffrage 
Cette a pour objectif de décrire les étapes attendus lors d'une demande de proposition de chiffrage en éléments de chiffrage suite à affectation d'un développement nouvelle fonction.

Pour rappel, lorsqu’un besoin est exprimé (interne ou client), un événement de développement nouvelle fonction est créé et un analyste travail sur un document d’expression de besoin pour lequel il demande à un développeur une estimation du coût : c’est le chiffrage prévisionnel.
Une fois le sujet analysé un document d’AFD ou d’analyse d’écart rédigé, l’analyste propose le sujet en comité d’analyse sur la base de ce chiffrage prévisionnel.
Une fois le sujet validé par ce comité et l’AFD présentée à l’équipe de développement, l’événement support du sujet est « affecté à l’équipe de développement ».

A ce moment-là, un développeur est proposé pour prendre en charge le sujet et je demande alors de « proposer un chiffrage avec découpage en éléments de chiffrage ».
Cela signifie que le développeur doit affiner le chiffrage prévisionnel suite à prise de connaissance dans le détail du sujet et de l’AFD et se demander : quelles sont les étapes de mon développement et quel temps j’estime nécessaire au développement de chaque étape ?
Ces étapes sont les « éléments de chiffrage » mentionnés ci-dessus.

Pour illustrer tout cela, prenons l’exemple de l’événement 230401 - BGE Capa : Publication externe Hx1. Ce DNF devait avoir, de mémoire, un chiffrage prévisionnel à 25 jours car l’idée était d’en faire le support du développement du batch de publication de BGE tous formats confondus en plus du développement du format Hx1.
Donc, les étapes de développements identifiées ont été les suivantes :

 Développement de traitements d’action de publication (DEV+TU+TI) : estimée à 3j ;
 Développement d’un batch d’export XML (DCT+DEV+TU+TI) : estimée à 15 ;
 Développement d’un ME de consommation interne de masse de BGE (DEV) : estimé à 1j ;
 Développement d’un ME de publication externe de masse de BGE au format Hx1 (DEV+TU+TI) : estimé à 10j ;
 Réalisation de tests de bout en bout : estimée à 2j.

Soit un total de 31 jours. C’est un peu plus que ce qui a été chiffré en prévision mais dans une proportion acceptable et surtout justifié par le découpage en différentes étapes ci-dessus.

Me reste donc à renseigner tout cela dans suivefluid.
Attention, l’événement doit être affecté à l’équipe de développement pour cela.
Il faut donc se rendre dans l’onglet « analyse financière » de l’événement qui au départ est donc renseigné ainsi :
Onglet chiffrage prévisionnel

Je renseigne ensuite l’ensemble de mes éléments de chiffrage avec leur charge en conservant la ligne initiale que je vide au fur et à mesure que je charge les différentes éléments :
Onglet chiffrage définitif

Il est important de noter que :

 Un nom significatif précisant le contexte pour chaque élément est nécessaire car les événements de découpage technique qui vont être associés à chacun de ces éléments seront noyés dans la masse. Donc « Développement ME de publication externe » ne sera pas parlant ;
 Le domaine fonctionnel doit être renseigné en cohérence avec le développement à réaliser :
 Action de publication : c’est du mécanisme publication ;
 Batch d’export : c’est du mécanisme d’export batch ;
 ME de consommation/publication : c’est du mécanisme efluid.net.
 Etc…
 Pour rappel, l’ensemble des domaines du groupe échanges sont répertoriés ici : http://eforum.uem.lan/viewtopic.php?f=18&t=2760 ;

Une fois ce travail réalisé, il convient d’enregistrer l’événement et de prévenir le responsable projet par mail de la réalisation du chiffrage et de l’attente de validation.
C’est suite à validation que les événements de découpage technique seront créés et affectés au développeur par le responsable projet.

Ce travail est à réaliser au sortir de chaque réunion d’organisation ou de lancement d’une version de développement comme celle que nous faisons depuis la v14 et au cours desquelles vous prenez connaissance des événements qui vous serons affectés dans la version concernée.

 Procédures et tutoriels 
Cette section a pour objectif de regrouper un ensemble de procédures et tutoriels utiles aux développeurs échanges.
 Procédures 
 Room regroupant l'ensemble des procédures d'installations techniques : Procédures d'installations techniques ;
 Procédure de paramétrage des livrables génériques (installation des composants efluid.net pour les clients) : efluid - Paramétrage des livrables génériques.doc ;
 ! Procédure de mise à jour du modèle EDK : Procédure - Mise à jour modèle EDK.doc
 Procédure de développement de WS :
  Développer un client
  Développer un serveur
  Tester
 Procédure pour la création de codification :  Procédure pour la création de codification
 Procédure pour le développement de factory de test : Procédure - Développement d'une factory de test pour efluid.net.doc
 Procédure pour l'utilisation d'abonnements avec envoi de mails sécurisés : Procédure - Instructions pour l’exécution d’un abonnement comportant un mail crypté sur efluid.net.docx

 Tutoriels techniques 
 Comprendre les flux d'échanges : Comprendre les flux d'échanges
 Installer un serveur FTP de développement sur son poste : Guide d'installation d'un FTP local
 Accéder au bureau à distance d'ES : Accès au bureau à distance d'ES

 Référentiel des évènements suivefluid récurrents du domaine "échanges" 
RéférenceLibelléCommentaires30986congés - contrat cgiCP, RTT, maladie, exceptionnel, ...30988congés - efluid SASCP, RTT, maladie, exceptionnel, ...102156pilotage groupe échange - contrat efluid sasÉvénement pour imputation de tâches de pilotage pour les agents efluid (réunion groupe dév, ...)129256pilotage groupe échange - contrat CGIÉvénement pour imputation de tâches de pilotage pour les agents CGI(réunion groupe dév, ...)30994pilotage activité efluid - contrat uemRéunion de service, comité technique, etc...30859UEM - support technique efluidPour la livraison EDK, type de tâche: assistance interne246750Revues de codeImputations des tâches de revue de code246749Correction des TU et TIImputations des tâches de correction de TU et TI liés au groupe "échanges", toute application confondue246751Correction sonarImputations des tâches de correction des problèmes remontés par sonar liés au groupe "échanges", toute application confondue

 Bibliothèque des pages relatives au domaine échange 
Sont répertoriées dans ce chapitre les pages relatives au domaine échange mais n'ont pas vocation à être particulièrement rangées dans un chapitre dédiée :
 Projet ERDF - Domaine Echanges (Il y a t'il encore un intérêt à garder cette page ?) ;
 Proto performance efluidNet ;
 Divers topics liés au domaine échange ;
 Envoie de mail en masse pour le format SRD ;
 Script correction fichier pub ;
 Domaine Echanges - surveillance des tests aléatoires ;
 Transformation XSLT ;<!!!>Category:groupe de développement CRM
Category:groupe de développement

 Règles de développement 

 Lire les 10 commandements de la programmation sans égo
 Lire les conventions de codage de la filière Client et Portail
 Lire les bonnes pratiques de la filière Client et Portail
 Dès le début d’un nouveau dév : confronter son idée de conception à un autre développeur : même si on n'est pas bloqué
 En cours de dév :  faire valider l'avancement du dev, les changements éventuels de conception, etc.
 En fin de dev : faire relire son code par un autre développeur. 
 Livraison partielle sous ecore
 Convention pour rédaction AFD

Trucs et astuces
 Coder une évolution
 Configurer ses saves actions
 Configurer une action "Clean Up"
 Popup
 Scripts sql récurrents
 Trucs et astuces SQL
 Trucs et astuces Framework
 Trucs et astuces JavaScript / JQuery
 Rajouter un overlay à efluid dev
 Tutorial de création d'une page profil
 Liste des scripts génériques crm
 passer un stash d'un clone à un autre
 Aide-mémoire pour le chiffrage des écarts
 Guides et procédures suivefluid par la coordination
 Générateur référence PDS Enedis

Pilotage
 lien vers la room de la filière

Intégration nouvelles ressources
 Liens utiles 
 Plan de progression: plan format xls
 CheckList checklist
 Entretien technique

 Liste des arrivants 
Une formation est prévue à l'arrivée de chaque nouveau développeur dans le groupe CRM en suivant les supports de présentation : Intégration nouveaux
Arrivées prévues :
 juin 2009 (dans contrat) Vincent Bouthinon
 août 2011 : Florence Demoulin
 septembre 2011 : Kilian Stein /  : accompagnement nouveau développeur - KST
 octobre 2011 : Sami Yacoub
 mars 2012 : Thomas Boudin /  : accompagnement nouveau développeur - TBO
 aout 2012 : Karim Tahri /  : accompagnement nouveau développeur - KTA
 février 2013 : Guillaume Laas /  : accompagnement nouveau développeur - GLA
 mai 2013 : Xavier Schmitt
 octobre 2013 : Anthony Bainville /  : accompagnement nouveau développeur - ABA
 novembre 2013 : Toni Dias /  : accompagnement nouveau développeur - TDI
 décembre 2014 : Brice Castagna /  : DEV accompagnement nouveau développeur - BCA (Brice Castagna) 
 juillet 2015 : Jérôme Koenig /  : DEV accompagnement nouveau développeur - JKOE (Jérôme KOENIG)
 juillet 2015 : Eric Finickel /  : DEV accompagnement nouveau développeur - EFI (Eric Finickel) 
 juillet 2015 : Alexandre Wehbe /  : DEV accompagnement nouveau développeur - AWE (Alexandre Wehbe)
 octobre 2015 : Damien Lienhardt /  : DEV accompagnement nouveau développeur - DLI (Damien LIENHARDT)
 mars 2016 : Romain Terracina /  : DEV accompagnement nouveau développeur - RTE (Romain Terracina)
 mars 2016 : Thomas Kedziora /  : DEV accompagnement nouveau développeur - TKE (Thomas Kedziora)
 mars 2016 : Cuma NEHIR /  : DEV accompagnement nouveau développeur - CNE (Cuma Nehir)
 octobre 2016 : Tom Besnard /  : DEV accompagnement nouveau développeur - TBE (Tom BESNARD)
 octobre 2016 : Nicolas Prandi /  : DEV accompagnement nouvel alternant - NPR (Nicolas PRANDI)
 décembre 2016 : Hazdine Messaoud /  : DEV accompagnement nouveau développeur - HME (Hazdine MESSAOUD)
 décembre 2016 : Hani Slaim /  : DEV accompagnement nouveau développeur - HSL (Hani SLAIM)
 janvier 2017 : Benoît Thevenin /  : DEV accompagnement nouveau développeur - BTE (Benoît THEVENIN)
 avril 2017 : Anthony Jungmann /  : DEV accompagnement nouveau développeur - AJU (Anthony Jungmann)
 juillet 2017 : Mathieu Buzon /  : DEV accompagnement nouveau développeur - NCL (Nathan CLAUDOT) (Metz)
 juillet 2017 : Nathan Claudot /  : DEV accompagnement nouveau développeur - MBU (Mathieu Buzon)
 septembre 2017 : Benjamin Dansokho /  : DEV accompagnement nouveau développeur -  BDAN (Benjamin DANSOKHO)  (Metz)
 septembre 2017 : Daniel Payet /  : DEV accompagnement nouveau développeur - DPA (Daniel PAYET)
 octobre 2017 : Daniel Clifford /  : DEV accompagnement nouveau développeur - DCLI (Daniel Clifford)
 novembre 2017 : Stéphane Joyeux /  : DEV accompagnement nouveau développeur - SJOY (Stéphane JOYEUX) (Metz)
 février 2018 : Pierre Leriche /  : DEV accompagnement nouveau développeur -  PLER (Pierre Leriche)  (Metz)(Metz)
 mai 2018 : Rémi Magnin /  : DEV accompagnement nouveau développeur - RMA (Rémi MAGNIN) (Metz)
 juillet 2018 : Thomas Legrand /  : DEV accompagnement nouveau développeur - TLE (Thomas LEGRAND) (Metz)
 aout 2018 : Florian Iggiotti /  : DEV accompagnement nouveau développeur - FIG (Florian IGGIOTTI) (Metz)
 septembre 2018 : Loic Szymanski /  : DEV accompagnement nouveau développeur - LSZ (Loic Szymanski)
 septembre 2018 : Quentin Morizot /  : DEV accompagnement nouveau développeur - QMO (Quentin MORIZOT)
 janvier 2020 : Matthieu Marin /  : DEV accompagnement nouveau développeur - Mathieu MARIN
 août 2021 : Guylan Dieu /  : DEV accompagnement nouveau développeur - GDI (Guylan DIEU)
 septembre 2021 : Geoffroy Bernard /  : DEV accompagnement nouveau développeur - GBE (Geoffroy BERNARD)
 septembre 2022 : Loïc Grispino /  : DEV accompagnement nouveau développeur - LGI (Loïc GRISPINO)
 janvier2023 : William Bobo /  : DEV accompagnement nouveau développeur - WBO (William BOBO)

Entretien de suivi
 responsable nouveau 1 2 3 4 5 6 9 12 15 18VBOJKOaout 2015septembre 2015octobre 2015 VBOAWEaout 2015septembre 2015octobre 2015novembre 2015décembre 2015janvier 2016entretien 3ème trimestrejuillet 2016octobre 2016janvier 2017RLEEFIaout 2015septembre 2015octobre 2015novembre 2015décembre 2015janvier 2016entretien 3ème trimestrejuillet 2016octobre 2016janvier 2017GLADLInovembre2015décembre2015entretien 1er trimestrefévrier 2016mars 2016entretien 2ème trimestreentretien 3ème trimestreoctobre 2016janvier 2017avril 2017TBORTEavril 2016non réalisé entretien 1er trimestrejuillet 2016août 2016entretien 2ème trimestrenon-réalisénon-réalisénon-réalisénon-réaliséBCACNEavril 2016 mai 2016juin 2015CNE absentaout 2016 - - - - -AWETBEnovembre 2016décembre 2016janvier 2017février 2017mars 2017avril 2017juillet 2017octobre 2017janvier 2018avril 2018VBOHMEjanvier 2017février 2017mars 2017avril 2017mai 2017 - - - - -CLAHSLjanvier 2017février 2017mars 2017avril 2017mai 2017juin 2017octobre 2017janvier 2018avril 2018juillet 2018TBOBTEfévrier 2017mars 2017avril 2017mai 2017 - - - - - -EFINPRfévrier 2017non réalisé (à l'école)non réalisé (à l'école)mai 2017juin 2017juillet 2017aout 2017septembre 2017 - -TBOMBUaoût 2017septembre 2017entretien 1er trimestrenovembre 2017décembre 2017entretien 2ème trimestreentretien 3ème trimestreentretien 4ème trimestrenon réalisénon réaliséDLINCLaoût 2017septembre 2017entretien 1er trimestrenovembre 2017décembre 2017entretien 2ème trimestreentretien 3ème trimestreentretien 4ème trimestrenon réalisénon réaliséVBOBDANoctobre 2017novembre 2017décembre 2017 - - - - - - -RTESJOYdécembre 2017non réalisé (formations + congés)février 2018 - - - - - - -TBEPLERmars 2018avril 2018mai 2018juin2018   juillet 2018aout 2018novembre 2018février 2019mai 2019aout 2019TBORMAjuin 2018juillet 2018entretien 1er trimestreseptembre 2018octobre 2018novembre 2018février 2019mai 2019août 2019novembre 2019RTETLEGjuillet 2018août 2018non réalisédépart planifiédépart planifiédépart planifiéplus sur le projetplus sur le projetplus sur le projetplus sur le projetBCAFIGseptembre 2018octobre 2018novembre 2018FIG absent + congésjanvier 2019février 2019mai 2019aout 2019novembre 2019mars 2020LRAQMOseptembre 2018 non réalisé (en cours)octobre 2018novembre 2018décembre 2018janvier 2019février 2019avril 2019juillet 2019septembre 2019EFILSZoctobre 2018 non réalisé (formations)novembre 2018décembre 2018janvier 2018février 2019mars 2019juin 2019septembre 2019décembre 2019mars 2020MBU[MMAR]février 2020mars 2020entretien 1er trimestre avril 2020mai 2020juin 2020entretien 2em trimestre (début juillet 2020)entretien 3em trimestre (début octobre 2020)entretien 4em trimestre (début décembre 2021)

Stagiaires
Sujets de stage

 Reste à faire 
Reste à faire CRM

Suivi ERDF
 Suivi développements ERDF

Réunions de groupe
 Compte rendu des réunions de dev
 Réunions dev/expertise CRM
 Analyses des 5 pourquoi

Congés
Note de congés

Qualité

Métriques du groupe

 Rapport sonar contrat
 Rapport sonar référentiel
 Rapport sonar simulation tarifaire

Planification des revues de code
Voir la page de revue de code du groupe CRM ;

Expériences
La revue de code en groupe
Le pair programming

Etudes
 Etude des outils de revue de code

Coding Dojo
 Coding dojo

Tests

 Règles à suivre pour réaliser des tests au sein du groupe CRM : convention CRM test
 Suivi du groupe de travail sur eRoom
 Suivi des revues de code sur les tests : revue de code sur les tests unitaires dans le domaine CRM
 Aide pour la réalisation de tests : Quand réaliser un test ?

Performances
CRM:Anomalies de perf lot 11<!!!>Category:recette
Category:groupe de développement

 généralité 
Le paramétrage est réalisé 
 soit dans une base de paramétrage appellé PARAM
 soit via la réalisation d'une Note de paramétrage

Il existe des guides de paramétrage dans eroom

 dossiers en cours 
 erdf 

 vialis 

 multi-eld <!!!>Category:groupe de développement

 Règles de développement 
 Lire les conventions de codage du la division CRM
 Lire les bonnes pratiques de la division CRM
 Dès le début d’un nouveau dév : confronter son idée de conception à un autre développeur : même si on n'est pas bloqué
 En cours de dév :  faire valider l'avancement du dev, les changements éventuels de conception, etc.
 En fin de dev : faire relire son code par un autre développeur. Planification des revues de code
 Lire la façon de réaliser les revues de code

 Aide-mémoire 
 Consultation d'un acteur dans efluid ;

 Pilotage 
 RAF 12.5 
 CLA

 KST

 RAF 12.6 
 CLA

 KST

Synthèse

Détail

Suivi ERDF
 Suivi développements ERDF

Planification des revues de code

Planification revue de code - Lot 12

 Évènement Libellé Développeur Relecteur Revue Correction{{#for_external_table:
-
 
 
 
 
 Revue de code:
  }}<!!!>Category:groupe de développement

 Actualités  

 Création des contextes de tests pour les batchs

 Planning d'utilisation des plateformes 

Le planning d'utilisation des plateformes est géré à travers le trello dédié.

 Consignes 

 Travaux sur les partages de contextes 

Avec le projet d'intégration d'efluid chez Enedis, certains batchs ont été portés en mode partage de contexte. Ce projet de portage est désormais fermé.
Depuis, les nouveaux batchs créés dans efluid sont supposés être développés en mode partage de contexte actif. Mais il peut rester des batchs à porter : ils sont traités au cas par cas.

Un portage de batch en mode partage de contexte actif est traité par un évènement de type Paramétrage Nouvelle Fonction. Un évènement ne gère qu'un batch et un seul.

En fils de cet évènement de portage sont créés les différents travaux : 
 des tests de validation de partage de contexte
 des adaptations du batch 

Les tests de validation de partage sont géré à travers des évènements de type test de performance. Chaque portage doit comporter au minimum un évènement de ce type.

Si un rapport de test de validation de partage indique un conflit, ce dernier est traité par un évènement de type découpage technique. Un découpage technique introduit des adaptations de code, livrées via git/gerrit. 
Un découpage technique doit avoir une double validation : 
 technique (validation du partage de contexte) : elle est réalisée à travers les tests de validation dédiés au portage. Le responsable du portage décide de réaliser ces tests au cas par cas ou globalement, une fois tous les découpages techniques livrés
 fonctionnelle (non régression) : elle est réalisée à travers un évènement fils au découpage technique de type recette. Cet évènement est affecté à un recetteur fonctionnel du domaine impacté par le découpage technique

Une fois tous les travaux terminés et recettés, le paramétrage nouvelle fonction qui gère le projet de portage est validé par l'équipe performances efluid et le paramétrage est livré dans git/gerrit.

 Anomalies de performances 

En développeur : le responsable de la correction.
En recetteur : un expert performances qui doit valider la correction de perfs.
Recette fonctionnelle : effectuée via un évènement fils de type "recette" avec en recetteur l'expert fonctionnel en charge 
de la recette fonctionnelle. Cet évènement de recette passe au statut "livré recette" quand l'anomalie de performance est livrée.
Dans certains cas, l'évènement de recette peut être inutile (livraison d'un index par exemple).
Test de performance : il est possible de créer un ou plusieurs tests de performances en élément fils de l'anomalie de performance pour valider cette dernière.
Mais le tests peut également être directement intégré à l'anomalie : c'est au choix du développeur. Dans tous les cas, le lien vers le rapport est 
fourni dans un commentaire associé à l'anomalie de performance.<!!!>Category:domaine
Category:Affaire Générique
Category:pole composants transverses
 Documentation générale 
 AFD du composant
 AFD de paramétrage des modèles d'affaires
 Guide utilisateur du paramétrage affaire génériques
 DCT "User guide" des modèles objets métiers
 Plan de progression

 Activation des affaires générique 
Pour avoir accès au paramétrage d'un modèle objet métier, il faut mettre à true le paramètre entreprise activeModeleObjetMetier.

 Type d'affaire générique 
 Affaire MDE
 Affaire Marketing
 Affaire Eclairage Public
 Affaire de Recrutement
 Affaire Correction de facture
 Affaire Vente d'Energie

 Technique 
 Créer une nouvelle sous classe d'affaire générique
 Créer un nouvel onglet statique

 Contraintes de paramétrage 
 le type de l'objet maître du modèle d'affaire doit correspondre exactement au type d'affaire du workflow, lui même défini sur le modèle d'affaire.<!!!>Category:domaine
Documentation générale
 AFD du composant
 AFD des courriers de relance

Description du domaine
A compléter

Modèle simplifié
A compléter

Remarques
A compléter<!!!>Category:domaine
 Documentation générale 
 AFD du composant
 AFD Paramétrage campagne de contentieux

Description du domaine
A compléter

Modèle simplifié
A compléter

Remarques
A compléter<!!!>Category:domaine
Documentation générale
 AFD du domaine
 AFD Edition
 AFD generer ligne de compte
 AFD passage en non valeur
 AFD bordereaux des cas particuliers
 Etat bordereaux des ASC non rembourses
 Etat des décomptes pour solde

Description du domaine
A compléter

Modèle simplifié
A compléter

Sujet de discution
Comment charger les lignes de comptes lorsque nous sommes en train de réaliser une opération financière sur un compte bordereau ?

Idées

Une correction en lot 11 propose de charger les lignes de compte qui remplissent les conditions suivantes :
Ligne de compte à la racine du compte

Et 

(
Le solde est <> zéro OU 

c'est une ligne ATTE OU 

c'est une ligne appartenant à un compte PARTENAIRE OU 

la date de solde est null
)

Réactions ?

Remarques
A complé<!!!>Category:domaine
Documentation générale
 AFD du domaine
 Etat bordereaux de remise des cheques
 Etat bordereaux des opérations financieres
 Etat bordereaux des opérations diverses
 Interface traiter retour banque
 Interface traiter retour TIP
 Interface traiter retour machine encaissement cheque
 Interface paiement internet
 Interface encaissement générique
 Interface cash compte
 Interface BIP
 Interface encaissement téléfact
 Interface impayes téléfact
 Generer fichier de decaissement
 Generer fichier d'encaissement
 Generer fichier retour banque SEPA
 Generer encaissement

Règle de développement 
GESTION EN MEMOIRE D'UN OBJET :

Mise en place d'une solution pour la gestion du partage en mémoire d’un objet déjà chargé :

Etape 1 : Appel du service. 
 private CompteClient chargeCompteClient(HttpServletRequest req, CompteClient cc) {
  Object arg[] = { cc };
  gestionMemoireObjetCreationOperationFinanciere(req);
  Object ret[] = (Object[]) sendLogicEvent("comptabilite", "GererCompteClient",     ComptabiliteConstantes.CHARGER_CONTEXTE_COMPTE_CLIENT, arg);
  return cc;
 }     

Etape 2 : Récupération de l'objet de type "Opération financiere" en cours de creation. 
 private OperationFinanciere getOperationFinanciereEnCours(HttpServletRequest req) {
  OperationFinanciere of = null;
  if (ReglementHTTPContext.getEncaissementEnCreation(req) != null) {
    of = ReglementHTTPContext.getEncaissementEnCreation(req);
  } else if (ReglementHTTPContext.getDecaissementEnCreation(req) != null) {
    of = ReglementHTTPContext.getDecaissementEnCreation(req);
  } else if (ReglementHTTPContext.getOperationDiverseEnCreation(req) != null) {
    of = ReglementHTTPContext.getOperationDiverseEnCreation(req);
  }
  return of;
 }

Etape 3 : Valorisation en memoire de l'objet Payeur. 
  private void gestionMemoireObjetCreationOperationFinanciere(HttpServletRequest req) {
  OperationFinanciere of = getOperationFinanciereEnCours(req);
  // -> Si null alors nous ne sommes pas dans le cadre de la creation
  if (of != null) {
    List<Payeur> listePayeur = of.getPayeur();
    for (Payeur payeur : listePayeur) {
      EventThreadContextUtil.addToThreadBOContextNextEvent(false, payeur);
   }
  }
 }

Remarques
A complé<!!!>Category:domaine
Category:échanges

 Aide au diagnostic 
Aide au diagnostic

Tutoriel de recherche et correction d'un caractère interdit

 Serveur FTP, SFTP, FTPS de test 

En vu de tester des accès FTP simples et sécurisés, un serveur est mis à disposition par l'équipe sécurité (Thibaut AJDONIK).
Voici les infos nécessaires à la connexion selon les différents protocoles FTP et dérivés :
 FTP : port 21 
 FTPS : port 30000
 FTPS (2way) : port 30001
 SFTP login/pass : port 22
 SFTP avec clé ssh: port 22 

Pour toutes les connexions utiliser les identifiants vsftptest/P&ss_Op

La clef privée pour le SFTP est en pièce-jointe avec comme passphrase P&ss_Op :
sftptest (dans Mes eRooms > efluid - Documentation Technique > DCT suite efluid > DCT efluid.net > Tests > Serveur FTP de test)

Le keystore est en pièce-jointe ci-dessous (avec comme passphrase P&ss_Op) :
yourkeystore.jks (dans Mes eRooms > efluid - Documentation Technique > DCT suite efluid > DCT efluid.net > Tests > Serveur FTP de test)

Les flux sont ouverts depuis ironwhale (qui héberge les environnement efluid.net de recette)

 Liens utiles  
Ce paragraphe a pour but de répertorier les liens vers les différents documents intéressant pour le domaine Échanges.

Important

Pour modifier un fichier d'eRoom, préférer le lien vers la room plutôt que le lien direct vers le fichier.

 Gestion du domaine 
Nom de la roomCommentairesGroupe échangesRegroupe l'ensemble des documents de gestion du domaine (BL, revue de code, CR de réunion...)

Suivi de la charge du domaine Echanges

 Composant 
NomCommentairesAFD - Analyse globale du système d'échanges.docDécrit l'ensemble du composant d'échangesAFD - Architecture des services en ligne.docComposant d'échangesRegroupe l'ensemble des AFDs du composant d'échanges

 Guide de développement / Guide utilisateur 
ThèmeRoom/wikiNomCommentairesEDKprocédure<del>procedure livraison EDK.doc</del>Ancienne procédure de livraison de l'EDK (CVS + ant)EdkProcédure de livraisonProcédure de livraison de l'EDK (usine logicielle)procédureProcédure - Mise à jour modèle EDK.docProcédure de mise à jour du modèle EDKService webguide environnement de développementService Web - Réalisation de tests avec SOAP UI.docDocument d'utilisation de SOAP UI.guides framework Hermes01 - Présentation rapide des services web.docPrésentation générale et assez générique de ce qu'est un service webguides framework HermesService Web - Profil Maven pour la génération de stub Axis2.docL’objet de ce document est de présenter la mise en place d’un profil de génération de code Maven et de donner un exemple d’utilisation.guides framework Hermes02 - Etude sur les services web.docRésultat de l'étude ayant mené à l'utilisation d'Axis 2 (en version 1.3)guides framework Hermes03 - Guide du développeur efluid pour les services web.docGuide de développement des services web exposés par efluidwebservicesDescription des WebServices efluid et AEL.docRésumé fonctionnel des services web déployés dans la suite efluidwebservicesliste des services web.xlsDocument excel listant les différents services web exposés dans la suite efluid et fournissant le lien vers le service déployé sur le nightly build

 Documents de référence 
NomCommentairesRoom des documents de référenceRegroupe l'ensemble des document de références du domaine échanges. C'est-à-dire les documents fournis par les organismes extérieurs exposant des services web [ERDF, GRDF, ...]). On y retrouve la description:  des services web d'information sur les PDS, de changement de fournisseur...les documents de description des flux produits par les organismes externes (R04, RE6M, REMM...)

 Modèle EDK 
RoomNomCommentaires1 - edkAFD - EDK - modélisation affaire générique.docDescription du modèle EDK des affaires génériques1 - edkAFD - EDK - modélisation Affaire.doc Description du modèle EDK des affaires1 - edkAFD - EDK - modélisation EDL.docDescription du modèle EDK des EDL1 - edkAFD - EDK - modélisation Matériel.docDescription du modèle EDK du package Matériel1 - edkAFD - EDK - modélisation RDF gaz.docDescription du modèle EDK du package RDF Gaz1 - edkAFD - EDK - modélisation RDF.docDescription du modèle EDK du package RDF1 - edkAFD - EDK - modélisation Relève.docDescription du modèle EDK du package Relève1 - edkAFD - EDK - modélisation Acteur.docDescription du modèle EDK du package Acteur1 - edkAFD - EDK - modélisation Contrat.docDescription du modèle EDK du package Contrat1 - edkAFD - EDK - modélisation Facture.docDescription du modèle EDK du package Facture

 Flux 
Remarque : c'est toujours efluid.net qui initie un échange.

Remarque : le point de vue pris pour les termes consommation et publication est celui d'efluid.net.

NomCommentaires3 - AFD fluxRegroupe l'ensemble des AFD des développements concernant les flux

A mettre en place

Ce qui serait intéressant c'est de mettre à disposition, pour chacun des flux publiés ou consommés par efluid.net, un certain nombre d'information, notamment:
 les clients chez lesquels le flux est en production
 un document permettant de lister les différentes étapes permettant de publier/consommer le flux (action à réaliser dans efluid, dans le portail, exemple de fichier d'entrée à consommer [et les informations à modifier dans le fichier pour réaliser la consommation correctement])
 le DCT
 l'AFD

Modèle d'échangeAFD / DCTCahiers de recetteClients concernésREL Cns int relèves GRD (soap EDK)<ul>AFD - EDK - modélisation Relève.docDCT : <ul>

Points d'arrêt stratégiques quand on debug un flux
efluid.net
Consommation :
Flux historique :
ConsommationProcess.consommerAbonnement
cas transmission SOAP : ne pas oublier de mettre un point d'arrêt juste après le sendEventToPublicateur pour attraper le LogicEvent sur le retour.
Flux de masse :
ConsommateurXXXDeMasseProcess.getFichierDeMasseInduction

Publication :
Flux historique :
PublicationProcess.publierAbonnement
cas transmission SOAP : ne pas oublier de mettre un point d'arrêt juste après le sendEventToConsommateur pour attraper le LogicEvent sur le retour.
Flux de masse : 
TraducteurEchangeDeMasse.doCreateFiles : lance le business split, puis en multi-threads : technical split etc.
TraducteurEchangeDeMasse.processBusinessGroups : lance le technical split, etc en multi-thread. Ne pas oublier de mettre un point d'arrêt juste après pour rattraper la sortie du multi-thread.
TraducteurDeGroupeEchangeDeMasse.call : lance le technical split, transformation XSLT, merge et zip.
XmlSplitter.split : instancie readers et writer, lance l'éclatement du fichier en entrée en "Groupes"
XmlSplitterReader.doCall : lit le fichier en entrée, en extrait les échanges ainsi que les critères métier sur lesquels splitter, en déduit le groupe à associer à l'échange, envoie l'(échange, Groupe) à écrire dans la Queue du Writer associé au bon Groupe.
ContexteXXX.equals : détermine à quel Groupe appartient l'échange (comparaison sur le hashcode).

efluidPub
Cas transmission SOAP : AbstractSOAPServlet.processPostRequest ou BCClientSOAPServlet.traiterMessage.
Ne pas oublier de mettre un point d'arrêt juste après traiterEvenement (recueil de la réponse).

efluid
Cas transmission SOAP  : ConsommationEventMgr.handleEvent.

Fonctionnement des XMLMgr
Ce fonctionnement est explicité la page Fonctionnement des XMLMgr.

 Services Web 
 Serveur 
 AFD spécifiques 
Les AFD des services web doivent être classées sous eRoom de la manière suivante:
Mes eRooms > efluid - Documentation Fonctionnelle Suite efluid > AFD suite efluid > AFD efluid > Web services
Domaine fonctionnelLienCommentairesActeurAFD - service web - creer contact.docAFD du service web "CreerContact"ActeurAFD - service web - gerer client.docAFD du service web "GererClient"ActeurAFD - service web - rechercher client.docAFD du service web "RechercherClient"Affaires - ActionAFD - service web - création action.docAFD du service web "CréationAction"Affaires - ActionAFD - service web - création pièce jointe sur action.docAFD du service web "CreerPieceJointe"ContratAFD - service web - facturer contrat.docAFD du service web "FacturerContrat"ContratAFD - service web - fin d'un service optionnel.docAFD du service web "TerminierServiceOptionnel"ContratAFD - service web - modifier contrat.docAFD du service web "ModifierContrat"ContratAFD - service web - rechercher contrat.docAFD du service web "RechercherContrat"ContratAFD - service web - souscrire service optionnel.doc AFD du service web "SouscrireServiceOptionnel"Adresse et PDSAFD - service web - modifier edl.docAFD du service web "ModifierEDL"Adresse et PDSAFD - service web - rechercher edl.docAFD du service web "RechercherEDL"RelèveAFD - service web - créer relève.docAFD du service web "CreerReleve"RelèveAFD - service web - rechercher relève PDS.doc AFD du service web "RechercherReleve"

 DCT Spécifiques 

RoomCommentairesservice webCette room regroupe l'ensemble des DCT spécifiques des services web exposés par efluid (efluidpub)

 DCT Composant 
RoomLienCommentairesconceptionDCT - EDK - Composant web services.doc DCT du composant d'exposition des services web (efluidPub principalement)conceptionEDK - Utilisation Client Services Web.docSchéma d'architecture de la configuration de déploiement pour le développement de service web

 Client 
 AFD spécifiques 
Les AFD des services web doivent être classées sous eRoom de la manière suivante:
Mes eRooms > efluid - Documentation Fonctionnelle > AFD suite efluid > AFD efluid > XXX - DOMAINE > AFD - Interfaces
Domaine fonctionnelNom du documentCommentairesEdl - Pds - DGDCT – modèle de service web – ERDF – information PDS.docEdl - Pds - DGDCT – modèle de service web – GRDF – information PDS.docContratAFD – service web – ERDF – Changement de fournisseur.docAFD du service web client "ChangementDeFournisseur"
 DCT Composant 
RoomLienCommentairesconceptionDCT - EDK - Client Services Web.docDCT du composant d'appel de service web (fonction externe, stub axis2...)
 DCT Spécifiques 
Les DCT des services web doivent être classées sous eRoom de la manière suivante:
Mes eRooms > efluid - Documentation Technique > DCT suite efluid > DCT efluid > DOMAINE XXX > services web
Domaine fonctionnelNom du documentCommentairesPdsDCT – service web – ERDF – information PDS.docAFD du service web client "InformationsPds" pour ErDFPdsAFD – service web – GRDF – information PDS.docAFD du service web client "InformationsPds" pour GrDFContratDCT – modèle de service web – ERDF – Changement de fournisseur.doc<!!!>Category:domaine
 AFDs 
eRoom
Les AFDs "archivées" ne sont pas représentées ci-dessous.

 AFD - Comp - complémentaire conso et contrat_SJ.doc
 Cas d'utilisation "Créer configurations matérielles"
 Cas d'utilisation "Mettre à jour et éditer provisoirement des contrats"
 Cas d'utilisation "Gérer P.A.M."

 AFD - MAT - Matériel.doc
 Concepts
 Cas d'utilisation "Créer les matériels"
 Cas d'utilisation "gérer les enregistrements de matériels"
 Cas d'utilisation "Gérer les opérations effectuées en laboratoire"
 Cas d'utilisation "Affecter un matériel à Agent ou a une entité extérieure"
 Cas d'utilisation "Gérer les P.A.M."
 Cas d'utilisation "Administrer une P.A.M."
 Cas d'utilisation "Gérer les disjoncteurs"
 Cas d'utilisation "Contrôler le stock"
 Cas d'utilisation "Contrôler le matériel affecté à Agent"
 Cas d'utilisation "Gérer les types de matériels"
 Cas d'utilisation "Gérer les désignations matérielles"
 Cas d'utilisation "Gérer les types de matériels"
 Indicateurs
 Requêtes utiles 
 Renuméroter les constructeurs par ordre alphabétique 
Requête utile à mettre à la fin des scripts qui ajoutent de nouveaux constructeurs de matériels.

create table TMP_ENUM  (code varchar2(10), type varchar2(10), lgcode number(2), ordre NUMBER(5,0) );

insert into TMP_ENUM 
SELECT code, type, lgcode, to_number(row_number() over (order by shortlabel))
FROM TENUM WHERE TYPE = 'MATCONSTRU' and etatobjet = 0 ORDER BY SHORTLABEL;

update tenum a set itemorder = (select ordre from TMP_ENUM b where a.code=b.code and a.type=b.type and a.type = 'MATCONSTRU' and a.etatobjet =0 and a.lgcode = b.lgcode)
where exists(select ordre from TMP_ENUM b where a.code=b.code and a.type=b.type and a.type = 'MATCONSTRU' and a.etatobjet=0 );

drop table TMP_ENUM ;

 Contrôles de cohérence des matériels 
Les requêtes suivantes servent à vérifier qu'il n'y a pas d'incohérence dans les matériel et le paramétrage associé.

 Matériels sans type de matériel cohérent 
select mat.id, mat.reference, mat.role, tm.id, tm.libelle, tm.etatobjet
from tmateriel mat 
	left join ttypemateriel tm on (mat.typemateriel_id = tm.id)
where mod(mat.etatobjet,2) = 0 
	and (tm.id is null or mod(tm.etatobjet,2) = 1)
order by  mat.role, mat.typemateriel_id nulls last, mat.reference;

 Matériels utilisant une désignation ayant un type de matériel supprimé 
select mat.id, mat.reference, mat.role, mat.designation_id
from tmateriel mat 
where mat.designation_id in 
	(SELECT d.id
	FROM tdesignation d
		join ttypemateriel tm on (d.typemateriel_id    = tm.id)
	where mod(tm.etatobjet,2)  = 1
		and mod(d.etatobjet,2) = 0
	)              
	and mod(mat.etatobjet,2) = 0
order by mat.role, mat.reference ;

 Matériels ayant une type de matériel différent de celui porté par sa désignation 
select mat.id, mat.reference, mat.role, mat.typemateriel_id, d.typemateriel_id
from tmateriel mat
	join tdesignation d on (mat.designation_id = d.id)
where mod(mat.etatobjet,2) = 0
	and mat.typemateriel_id != d.typemateriel_id 
order by mat.role, mat.reference;<!!!>Category:domaine
Category:Workflow
Category:pole composants transverses
 Documentation générale 
 AFD du composant
 AFD des traitements génériques
 AFD des traitements spécifiques efluid
 AFD de paramétrage des campagnes
 Guide utilisateur du paramétrage workflow
 DCT Workflow
 DCT Batch Workflow V3
 DCT des campagnes
 Plan de progression
 Chiffrages Workflow
 FAQ Workflow
 Évènement permanent 50257 "Action qualité - Domaine Workflow"
 Évènement 132245 homogénéisation des risques des campagnes workflow
 Améliorations/qualité de code Domaine workflow
 Répertoire partagé pour le DEV (eRoom) : efluid - Gestion de Projet > Gestion des domaines > domaine Listes de gestion / Workflow  > DEV_Repertoire_de_partage
 Batman

 Campagnes Workflow 

 ecore 
 Campagne de gestion des objets génériques - type de campagne 500
 Campagne de construction de courbes - type de campagne 501

 efluid 

 Echange GRD-F côté GRD - type de campagne 11
 Campagne workflow de contentieux - type de campagne 12
 Contrat TPN - type de campagne 17
 DI sans déplacement - type de campagne 18
 Supprimée - Relance des propositions commerciales - type de campagne 19
 Alerte relève - type de campagne 20
 Campagne de relance relève - type de campagne 22
 Campagne workflow de raccordement de production - type de campagne 23
 Suivi diagnostic gaz - type de campagne 24
 Suivi des autorisations de prélèvement - type de campagne 25
 Suivi des demandes diverses - type de campagne 26
 Supprimée - Suivi des reconductions des contrats - type de campagne 27
 Suivi des contrats AEL - type de campagne 28
 Campagne workflow de MDE - type de campagne 29
 Campagne workflow de marketing - type de campagne 30
 Désactivée - Campagne workflow de raccordement - type de campagne 31
 Campagne workflow de suivi de traitement sur anomalies de relèves - type de campagne 32
 Campagne workflow de suivi des installations inactives - type de campagne 33
 Supprimée - Campagne workflow de suivi du recrutement - type de campagne 34 (suite à l'extinction d'etineraire)
 Désactivée - Echanges GRD-F côté F - type de campagne 36
 Suivi des modifications de RIB  - type de campagne 37
 Suivi des mandats SEPA  - type de campagne 39
 Campagne workflow de suivi des affaires génériques  - type de campagne 40
 Campagne workflow de suivi des interventions EP  - type de campagne 41
 Campagne workflow d'intervention  - type de campagne 42
 Campagne workflow de suivi des abonnements cycliques de relève  - type de campagne 44
 Campagne workflow d'analyse de recevabilité dans les demandes de prestations  - type de campagne 46
 Campagne workflow de suivi des affaires de correction de factures - type de campagne 48
 Campagne workflow de suivi des affaires de vente d'énergie - type de campagne 49
 Campagne workflow de suivi des demandes de cotation - type de campagne 51
 Campagne workflow de suivi des propositions commerciales - type de campagne 52
 Campagne workflow de simulation des chiffres d'affaire futurs - type de campagne 53
 Campagne workflow de calcul des bilans globaux d'énergie - type de campagne 54
 Campagne workflow de suivi d'import de barèmes - type de campagne 55
 Campagne workflow de gestion de la période mobile - type de campagne 56
 Campagne workflow de gestion des données climatiques - type de campagne 57
 Campagne workflow de suivi des services souscrits de mesure - type de campagne 58
 Campagne workflow de gestion du caractère communicant des compteurs - type de campagne 59
 Campagne workflow de suivi des affaires de prestation - type de campagne 60
 Campagne workflow de suivi des actions - type de campagne 61
 Campagne workflow de programmation relève - type de campagne 62
 Campagne workflow de suivi des échanges avec des GRDs externes - type de campagne 63
 Campagne workflow de suivi des demandes d'intervention réseau - type de campagne 64
 Campagne workflow de gestion des raccordements - type de campagne 65
 Campagne workflow des services souscrits - type de campagne 66
 Campagne workflow de calcul de la CAR - type de campagne 67
 Campagne workflow de suivi des éditions personnalisées sur publication - type de campagne 68
 Campagne workflow de suivi des PDS objet de DI - type de campagne 70
 Campagne workflow de gestion des demandes groupées - type de campagne 71
 Campagne workflow d'import de la CAR - type de campagne 72
 Campagne workflow de suivi des opérations bancaires - type de campagne 74
 Campagne workflow de suivi des relevés bancaires - type de campagne 75
 Campagne workflow de suivi des acteurs (RGPD) - type de campagne 76
 Campagne workflow de suivi des demandes d'opération financière - type de campagne 77
 Campagne de suivi du déploiement - type 83
 Campagne d'intégration des données de mesure - type 84
 Campagne de suivi comptes clients - type 85
 Campagne de suivi des signatures - type 86
 Campagne de contrôle cohérence d'offre - type 87
 Campagne de valorisation de périmètre autoconso collective - type 88
 Campagne de suivi des engagements contractuels - type 93

 travaux (efluid) 

 Campagne workflow de suivi des affaires de travaux - type de campagne 200

 enercom 

 Campagne workflow d'affectations entrées/sorties de périmètre - type de campagne 1
 Campagne workflow des demandes de cotation - type de campagne 2
 Campagne workflow de calcul de bilans global énérgie agrégés  - type de campagne 3
 Campagne workflow de calcul des barèmes - type de campagne 4
 Campagne workflow de gestion des données climatiques - type de campagne 5

 eot

 Campagne workflow d'exécution des téléaction - type de campagne 1

 Technique 
 La plupart des guides et tutos sont dorénavant consultables dans la doc archi => https://wikefluid.efluid.uem.lan/docInstalleur/archi/develop/documentation/composants_metiers/workflow/workflow.html
 Processus de création des EDP
 Scripts sql récurrents du domaine workflow
 Paramètre technique batch workflow
 Créer un traitement d'alimentation
 Créer une anomalie
 Type de chargement pour les batchs
 Traitement fil de l'eau
 WKF997
 Cache Workflow
 Cas particuliers d'appel au moteur Workflow<!!!>Category:domaine
 Présentation 

Le domaine grille contient le concept d'ensemble. Les ensembles permettent non seulement de retourner une quantité mais aussi n’importe quel autre type d’objet tel qu’un énuméré dynamique/statique, un booléen ou une sélection d'énuméré. Ainsi ce concept peut être utilisé n’importe où dans l’application afin de définir un résultat conditionné par une grille de valeurs.

 Utilisation 

Les ensembles sont pour le moment utilisés par les applications efluid et efluid.net.

 Risques 

Pour pouvoir créer des ensembles et critères sur une application, l’utilisateur doit posséder a minima les habilitations suivantes :
 pour avoir le lien dans le menu: risque R1
 pour pouvoir créer modifier les critères : R1006, R1007, R1008
 pour pouvoir enregistrer la grille de valeur : R21: 'Création, modification et suppression offre produit, service et bareme de prix'

Les risques R428 et R4299 (créer/modifier les ensembles) ne sont pas utilisés dans l'application et le risque R21 ne devrait pas être utilisé à cet endroit. Pour rétablir une situation correcte, un événement suivefluid qualité a été créé: 64289.

 Tables utilisées 

De plus ce concept utilise une multitude de table dont le détail est contenu dans les fichier ddl:
TENSEMBLE
TCRITERE
TVALEUR
TMATRICEDEVALEURS
MATRICEDEVALEURS_CRITERES
TCOMBINAISONDEVALEURS
COMBINAISONDEVALEURS_OPERANTES
TVALEURBOOLEEN
TVALEURENUMERE
TVALEURCRITERE
TVALEUROBJET
TVALEURPERIODE
TVALEURTEXTE
TVALEURINTERVALLEQUANTITE
TCOMBINAISONCOMPLEXE
COMBINAISONCOMPLEXE_OPERANTES
TCOMBINAISONCOMPLEXELIEE
COMBINAISONCOMPLEXELIEE_OP<!!!>Category:domaine
Plan de progression

 Aide mémoire 
 Détermination du responsable d'équilibre (RE), du fournisseur, des profils RDF
 Contrat de regroupement
 Dates sur les objets du domaine contrat
 Annotations des attributs des services souscrits
 CA Futur
 Feuillet de gestion
 Requeteur
 GRD Externe
 Commande Arthas

 Documentation Générale 
Analyse fonctionnelle
eRoom
 AFD - CTR - contrat.doc
 Concepts
 Cas d'utilisation : Souscrire et reprendre un contrat standard (reliquat de ce qui n'est pas encore reporté dans AFD - CTR - souscription de contrat.doc)
 Cas d'utilisation : Changement de fournisseur (reliquat de ce qui n'est pas encore reporté dans AFD - CTR - souscription de contrat.doc) 
 Cas d'utilisation : Gérer une condition de paiement par bordereau
 Cas d'utilisation : Gérer un envoi groupé 
 Cas d'utilisation : Gérer un contact facture 
 Cas d'utilisation : Gérer un créancier SEPA
 AFD - CTR - souscription de contrat.doc (en cours de rédaction)
 Cas d'utilisation : Souscrire et reprendre un contrat standard
 Cas d'utilisation : Valider souscription contrat
 Cas d'utilisation : Valider cessation contrat
 Cas d'utilisation : Suspendre un contrat standard
 Cas d'utilisation : Éditer proposition commerciale
 Cas d'utilisation : Optimiser un contrat standard
 Cas d'utilisation : Gérer date effet souhaitée
 Classification des services
 Paramètres de l'application
 AFD - CTR - contrat deuxième partie.doc
 Cas d'utilisation : consulter contrat
 Cas d'utilisation : administrer contrat
 Cas d'utilisation : modifier service régulier
 Cas d'utilisation : administrer service régulier
 Cas d'utilisation : modifier PACR
 Cas d'utilisation : cesser contrat / service
 Cas d'utilisation : mettre à jour contrat
 Cas d'utilisation : réactiver contrat
 Cas d'utilisation : annuler contrat
 Cas d'utilisation : renouveler contrat
 Cas d'utilisation : éditer liste de contrats en suivi
 Cas d'utilisation : éditer liste des contrats pour bilan qualité
 Cas d'utilisation : éditer feuiller de gestion
 Cas d'utilisation : ajouter nouvelle activité
 Cas d'utilisation : détecter les opportunités
 Cas d'utilisation : créer un mandat
 Cas d'utilisation : gérer un mandat
 Étude des statuts
 Gestion de la mise à jour suite à intervention
 Gestion des dates d'effet au niveau des services souscrits

Conception technique
Composant action trace
Composant Combinaison Complexe Liée
Service Souscrit Chauffage Urbain
Implémentation Services Optionnels
Implémentation Mettre A Jour Contrat

 Supports de développement 
 Coder une évolution
 Créer des contextes
 Créer des énumérés persistants<!!!> ecore 

 Documentation 

efluid - Documentation Fonctionnelle > AFD suite efluid > AFD ecore > TMP - Domaine temps

 efluid

Les classes EntityProcess et DAO des classes StructuresHorosaisonniere et PosteHorosaisonnier sont surchargés dans le domaine consommation efluid afin d'ajouter des contrôles sur leur utilisation vis à vis des concepts efluid.

 enercom 

TODO

 Liens externes 

Category:ecore
Category:domaine<!!!>Category:domaine
 AFDs 
eRoom
Les AFDs d'interface ne sont pas représentées ci-dessous.

 AFD - REL - Campagnes de relève et télérelève.doc
 Pré-études et réflexions connexes
 Cas d'utilisation "Gérer le paramétrage du domaine Relève"
 Cas d'utilisation "Gérer la liste des releveurs"
 Cas d'utilisation "Paramétrer un modèle de campagne de relève / télé-relève"
 Cas d'utilisation "Planifier un modèle de campagne de relève / télé-relève"
 Cas d'utilisation "Suivre une campagne de relève / télé-relève"
 Cas d'utilisation "Traiter un lot de relève par TSP"
 Cas d'utilisation "Traiter un lot de télé-relève"
 Cas d'utilisation "Traiter un lot d'isolés de relève / télé-relève"
 Cas d'utilisation "Traiter un élément de population de relève / télé-relève"
 Cas d'utilisation "Gérer les absences de la relève d'une campagne de relève"
 Cas d'utilisation "Réinitialiser un lot de relève / télé-relève"
 Concepts
 Indicateurs

 AFD - REL - Foliotage.doc
 Concepts
 Cas d'utilisation "Mettre à jour la table EDL - Tournée"
 Cas d'utilisation "Refolioter une tournée"
 Cas d'utilisation "Modifier le folio d'un PDS"
 Cas d'utilisation "Refolioter au retour relève"
 Impacts

 AFD - REL - Tournées.doc
 Cas d'utilisation "Gérer une tournée"
 Cas d'utilisation "Gérer les consignes d'une tournée"
 Cas d'utilisation "Gérer le foliotage depuis une tournée"
 Cas d'utilisation "Contrôler l’affectation des données géographiques et des EDL par PDS"
 Cas d'utilisation "Gérer le rattachement à une tournée de relève depuis un PDS"
 Concepts
 Indicateurs

 Requêtes utiles 
 Sélection des relèves de la PACM courante d'un PDS 

SELECT *
FROM treleve
WHERE pacm_id =
  (SELECT id
  FROM tpacm
  WHERE pointdeservice_id =
    (SELECT id FROM tpointdeservice WHERE REFERENCE = 'REFPDS'
    )
  AND datefin IS NULL
  ) order by datereleve desc;

 Supprimer une relève 
Il faut aussi penser à :
supprimer les grandeurs physiques générales
passer les élément de population relève au statut "absent relève" et suppression du lien vers la relève
supprimer les liens entre les factures et la relève
Si nécessaire :
supprimer les grandeurs physiques mensuelles ?
gérer les événements facturation ?
define RELEVE_ID = 36393001
 
-- suppression relève
UPDATE TRELEVE                   SET ETATOBJET = 1, ACTEURSUPPRESSION = 'evt XXXXX', DATESUPPRESSION = CURRENT_DATE  WHERE ID = '&RELEVE_ID';
-- suppression grandeur physique générale
UPDATE TGRANDEURPHYSIQUEGENERALE SET ETATOBJET = 1, ACTEURSUPPRESSION = 'evt XXXXX', DATESUPPRESSION = CURRENT_DATE  WHERE releve_id = '&RELEVE_ID';
-- passage des edp relève au statut "absent relève"
UPDATE TELEMENTDEPOPULATIONRELEVE SET statut = 4, releve_id = NULL WHERE releve_id = '&RELEVE_ID';
-- suppression du lien entre les factures et la relève
DELETE FROM facture_releves where dest = '&RELEVE_ID';

 Purge d'une campagne de relève 
Purge pour script client
define CAMPAGNE_ID =XXXXX;
define EVT = 'evt XXXXX';

-- suppression des relèves
UPDATE tgrandeurphysiquegenerale
SET    etatobjet = 1,
       acteursuppression = '&EVT',
       datesuppression = current_date
WHERE  id IN (SELECT GPG.id
              FROM   tgrandeurphysiquegenerale GPG,
                     treleve R,
                     telementdepopulationreleve E,
                     tlot L
              WHERE  GPG.RELEVE_ID = R.ID
                     AND ( r.id = e.releve_id
                            OR r.id = e.relevecomplementaire_id )
                     AND l.id = e.lot_id
                     AND l.campagne_id = '&CAMPAGNE_ID'
                     AND MOD(r.etatobjet, 2) = 0)
       AND MOD(etatobjet, 2) = 0;

UPDATE treleve
SET    etatobjet = 1,
       acteursuppression = '&EVT',
       datesuppression = current_date
WHERE  id IN (SELECT edp.releve_id
              FROM   telementdepopulationreleve edp,
                     tlot l
              WHERE  edp.lot_id = l.id
                     AND l.campagne_id = '&CAMPAGNE_ID'
                     AND edp.releve_id IS NOT NULL
                     AND MOD(edp.etatobjet, 2) = 0
              UNION
              SELECT edp.relevecomplementaire_id
              FROM   telementdepopulationreleve edp,
                     tlot l
              WHERE  edp.lot_id = l.id
                     AND l.campagne_id = '&CAMPAGNE_ID'
                     AND edp.relevecomplementaire_id IS NOT NULL
                     AND MOD(edp.etatobjet, 2) = 0)
       AND MOD(etatobjet, 2) = 0;

-- suppression des inforelèves
UPDATE tgrandeurphysiquegeneraleinfor
SET    etatobjet = 1,
       acteursuppression = '&EVT',
       datesuppression = current_date
WHERE  id IN (SELECT GPG.id
              FROM   tgrandeurphysiquegeneraleinfor GPG,
                     tinforeleve R,
                     telementdepopulationreleve E,
                     tlot L
              WHERE  gpg.releve_id = r.id
                     AND r.id = e.inforeleve_id
                     AND l.id = e.lot_id
                     AND l.campagne_id = '&CAMPAGNE_ID'
                     AND MOD(r.etatobjet, 2) = 0)
       AND MOD(etatobjet, 2) = 0;

UPDATE tinforeleve
SET    etatobjet = 1,
       acteursuppression = '&EVT',
       datesuppression = current_date
WHERE  id IN (SELECT EDP.inforeleve_id
              FROM   telementdepopulationreleve EDP,
                     tlot L
              WHERE  EDP.lot_id = L.id
                     AND L.campagne_id = '&CAMPAGNE_ID'
                     AND EDP.inforeleve_id IS NOT NULL
                     AND MOD(EDP.etatobjet, 2) = 0)
       AND MOD(etatobjet, 2) = 0; 
-- suppression des anomalies
UPDATE tanomalie SET etatobjet = 1, acteursuppression = '&EVT', datesuppression = CURRENT_DATE
WHERE ID IN (SELECT A.ID FROM tanomalie A, telementdepopulationreleve edp, tlot l
             WHERE A.elementpopulation_id = edp.ID AND edp.lot_id = l.ID AND l.campagne_id = '&CAMPAGNE_ID' AND mod(edp.etatobjet,2)=0  )
AND mod(etatobjet,2) = 0;

-- Suppression des tâches LDG
update ttache set etatobjet = 1, 
       acteursuppression = '&EVT',
       datesuppression = CURRENT_DATE
 where mod(etatobjet,2) = 0
   and elementdetravail_id in ( SELECT EDP.id
              FROM   telementdepopulationreleve EDP,
                     tlot L
              WHERE  EDP.lot_id = L.id
                     AND L.campagne_id = '&CAMPAGNE_ID'
                     AND MOD(EDP.etatobjet, 2) = 0
              UNION SELECT ano.id
              FROM   tanomalie ano,
                     telementdepopulationreleve EDP,
                     tlot L
              WHERE  ano.elementpopulation_id = edp.id
                     AND EDP.lot_id = L.id
                     AND L.campagne_id = '&CAMPAGNE_ID'
                     AND MOD(EDP.etatobjet, 2) = 0
              UNION SELECT top.id 
              FROM TTOURNEEOPERATIONNELLE top where exists (select 1 
                from tlot l 
                where l.id = top.LOTDERELEVE_ID 
                and l.campagne_id = '&CAMPAGNE_ID')
             );

-- suppression des edp
UPDATE telementdepopulationreleve SET etatobjet = 1, acteursuppression = '&EVT', datesuppression = CURRENT_DATE
WHERE lot_id IN ((SELECT l.ID FROM tlot l WHERE l.campagne_id = '&CAMPAGNE_ID' )
          UNION (SELECT c.lotisole_id FROM tcampagne c WHERE c.id = '&CAMPAGNE_ID' and lotisole_id is not null))
AND mod(etatobjet,2) = 0;

-- mise à jour des statistiques
UPDATE tlot SET nbelements=0, nbelementstraites=0,selectionrealisee=0, acteurmodification = '&EVT', datemodification = CURRENT_DATE
WHERE campagne_id = '&CAMPAGNE_ID';
UPDATE tlotdereleve SET ETATTRAITEMENTSELECTION=0 WHERE id in (select id from tlot where campagne_id = '&CAMPAGNE_ID');

UPDATE tcampagne SET nombreelements=0, nombreelementstraites=0, acteurmodification = '&EVT', datemodification = CURRENT_DATE
WHERE id = '&CAMPAGNE_ID';

UPDATE tstatistique SET quantite = 0, acteurmodification = '&EVT', datemodification = CURRENT_DATE
WHERE (campagne_id = '&CAMPAGNE_ID' 
       OR lot_id IN (SELECT l.ID FROM tlot l  WHERE l.campagne_id = '&CAMPAGNE_ID'));

UPDATE TTOURNEEOPERATIONNELLE top
SET etatobjet = 1, acteursuppression = '&EVT', datesuppression = CURRENT_DATE
WHERE EXISTS (SELECT 1 
FROM tlot l 
WHERE l.id = top.LOTDERELEVE_ID 
AND l.campagne_id = '&CAMPAGNE_ID');

undef CAMPAGNE_ID;
undef EVT;

Purge pour développement (suppression définitive)
SELECT id, libelle FROM tcampagne WHERE libelle LIKE '%Campagne LANILDUT (Steph) - 14/08/2009%' AND statut = 0;

-- id de la campagne à traiter
define CAMPAGNE_ID = '21237226010';

BEGIN
   EXECUTE immediate 'DROP TABLE suppression_reinit_lot_releve';
EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE != -942 THEN
         RAISE;
      END IF;
END;
/

CREATE TABLE suppression_reinit_lot_releve (
     edp_id VARCHAR2(25),
     anomalie_id VARCHAR2(25),
     tache_id VARCHAR2(25),
     releve_id VARCHAR2(25),
     gpg_id VARCHAR2(25),
     inforeleve_id VARCHAR2(25),
     gpginforeleve_id VARCHAR2(25),
     echange_id VARCHAR2(25)
);

INSERT INTO suppression_reinit_lot_releve (edp_id, releve_id, inforeleve_id)
SELECT edp.id, edp.releve_id, edp.inforeleve_id
FROM tlot l
     JOIN telementdepopulationreleve edp  ON edp.lot_id = l.id
WHERE l.campagne_id = '&CAMPAGNE_ID'
union all
SELECT null, edp.relevecomplementaire_id, null
FROM tlot l
     JOIN telementdepopulationreleve edp  ON edp.lot_id = l.id
WHERE l.campagne_id = '&CAMPAGNE_ID';

INSERT INTO suppression_reinit_lot_releve (anomalie_id)
SELECT ano.id 
FROM tlot l
     JOIN telementdepopulationreleve edp  ON edp.lot_id = l.id
     JOIN tanomalie ano  ON ano.elementpopulation_id = edp.id
WHERE l.campagne_id = '&CAMPAGNE_ID';

INSERT INTO suppression_reinit_lot_releve (tache_id)
SELECT tache.id 
FROM ttache tache where exists (select 1 from suppression_reinit_lot_releve tmp where tache.elementDeTravail_id = coalesce(tmp.edp_id, '§§'))
union all 
SELECT tache.id 
FROM ttache tache where exists (select 1 from suppression_reinit_lot_releve tmp where tache.elementDeTravail_id = coalesce(tmp.anomalie_id, '§§'));

INSERT INTO suppression_reinit_lot_releve (gpg_id)
SELECT gpg.id 
FROM tgrandeurphysiquegenerale gpg where exists (select 1 from suppression_reinit_lot_releve tmp where gpg.releve_id = coalesce(tmp.releve_id, '§§'));

INSERT INTO suppression_reinit_lot_releve (gpginforeleve_id)
SELECT gpg.id 
FROM tgrandeurphysiquegeneraleinfor gpg where exists (select 1 from suppression_reinit_lot_releve tmp where gpg.releve_id = coalesce(tmp.inforeleve_id, '§§'));

INSERT INTO suppression_reinit_lot_releve (echange_id)
SELECT pub.id 
FROM techange pub where exists (select 1 from suppression_reinit_lot_releve tmp where PUB.OBJETMAITRE_ID = coalesce(tmp.releve_id, '§§'))
UNION ALL
SELECT pub.id 
FROM techange pub where exists (select 1 from suppression_reinit_lot_releve tmp where pub.OBJETMAITRE_ID = coalesce(tmp.inforeleve_id, '§§'));

DELETE FROM techange pub WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE pub.id = tmp.echange_id);
DELETE FROM tgrandeurphysiquegenerale g WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE g.id = tmp.gpg_id);
DELETE FROM tgrandeurphysiquegeneraleinfor g WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE g.id = tmp.gpginforeleve_id);
DELETE FROM treleve r WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE r.id = tmp.releve_id);
DELETE FROM tinforeleve r WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE r.id = tmp.inforeleve_id);
DELETE FROM ttache t WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE t.id = tmp.tache_id);
DELETE FROM tanomaliereleve ano WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE ano.id = tmp.anomalie_id);
DELETE FROM tanomalie ano WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE ano.id = tmp.anomalie_id);
DELETE FROM telementdepopulationreleve edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE edp.id = tmp.edp_id);

DELETE
FROM ttache
WHERE MOD(etatobjet,2)   = 0
AND elementdetravail_id IN
  (SELECT top.id
  FROM TTOURNEEOPERATIONNELLE top
  WHERE EXISTS
    (SELECT 1
    FROM tlot l
    WHERE l.id        = top.LOTDERELEVE_ID
    AND l.campagne_id = '&CAMPAGNE_ID'
    )
  );

delete from TTOURNEEOPERATIONNELLE top
where exists (select 1 
from tlot l 
where l.id = top.LOTDERELEVE_ID 
and l.campagne_id = '&CAMPAGNE_ID');

UPDATE tcampagne
SET    nombreelements = 0,
       nombreelementstraites = 0
WHERE  id = '&CAMPAGNE_ID'; 

UPDATE tlot
SET    nbelements = 0,
       nbelementstraites = 0
WHERE  campagne_id = '&CAMPAGNE_ID';

UPDATE tstatistique s
SET quantite        = 0
WHERE ( s.campagne_id = '&CAMPAGNE_ID'
OR EXISTS
     (SELECT 1 FROM TLOT L WHERE s.lot_id=l.id AND l.campagne_id = '&CAMPAGNE_ID'
     )); 
 
-- ajouts v13
UPDATE tlot l SET selectionrealisee = 0 WHERE campagne_id = '&CAMPAGNE_ID';
UPDATE tlotdereleve SET ETATTRAITEMENTSELECTION=0 WHERE id in (select id from tlot where campagne_id = '&CAMPAGNE_ID');

UPDATE tlignestatistique ligne SET quantite = 0
WHERE EXISTS 
  (SELECT 1 FROM tstat s WHERE s.AXESTATISTIQUE_ID = '&CAMPAGNE_ID' AND s.id = ligne.statistique_id);

UPDATE tlignestatistique ligne SET quantite = 0
WHERE EXISTS 
  (SELECT 1 FROM tstat s 
    JOIN tlot l ON l.id = s.axestatistique_id
    WHERE l.campagne_id = '&CAMPAGNE_ID' AND s.id = ligne.statistique_id);

-- attention ça fait le commit!
TRUNCATE TABLE suppression_reinit_lot_releve;
DROP TABLE suppression_reinit_lot_releve;

Purge pour développement (suppression définitive) en PostGreSQL (A tester et/ou vérifier)
do
$$
    declare
        -- id de la campagne à traiter
        ID_CAMPAGNE CONSTANT varchar := '?';
    begin
        DROP TABLE suppression_reinit_lot_releve;
        CREATE TABLE suppression_reinit_lot_releve
        (
            edp_id           VARCHAR(25),
            anomalie_id      VARCHAR(25),
            tache_id         VARCHAR(25),
            releve_id        VARCHAR(25),
            gpg_id           VARCHAR(25),
            inforeleve_id    VARCHAR(25),
            gpginforeleve_id VARCHAR(25),
            echange_id       VARCHAR(25)
        );

        INSERT INTO suppression_reinit_lot_releve (edp_id, releve_id, inforeleve_id)
        SELECT edp.id, edp.releve_id, edp.inforeleve_id
        FROM tlot l
                 JOIN telementdepopulationreleve edp ON edp.lot_id = l.id
        WHERE l.campagne_id = ID_CAMPAGNE
        UNION ALL
        SELECT NULL, edp.relevecomplementaire_id, NULL
        FROM tlot l
                 JOIN telementdepopulationreleve edp ON edp.lot_id = l.id
        WHERE l.campagne_id = ID_CAMPAGNE;

        INSERT INTO suppression_reinit_lot_releve (anomalie_id)
        SELECT ano.id
        FROM tlot l
                 JOIN telementdepopulationreleve edp ON edp.lot_id = l.id
                 JOIN tanomalie ano ON ano.elementpopulation_id = edp.id
        WHERE l.campagne_id = ID_CAMPAGNE;

        INSERT INTO suppression_reinit_lot_releve (tache_id)
        SELECT tache.id
        FROM ttache tache
        WHERE EXISTS(SELECT 1
                     FROM suppression_reinit_lot_releve tmp
                     WHERE tache.elementDeTravail_id = COALESCE(tmp.edp_id, '§§'))
        UNION ALL
        SELECT tache.id
        FROM ttache tache
        WHERE EXISTS(SELECT 1
                     FROM suppression_reinit_lot_releve tmp
                     WHERE tache.elementDeTravail_id = COALESCE(tmp.anomalie_id, '§§'));

        INSERT INTO suppression_reinit_lot_releve (gpg_id)
        SELECT gpg.id
        FROM tgrandeurphysiquegenerale gpg
        WHERE EXISTS(SELECT 1
                     FROM suppression_reinit_lot_releve tmp
                     WHERE gpg.releve_id = COALESCE(tmp.releve_id, '§§'));

        INSERT INTO suppression_reinit_lot_releve (gpginforeleve_id)
        SELECT gpg.id
        FROM tgrandeurphysiquegeneraleinfor gpg
        WHERE EXISTS(SELECT 1
                     FROM suppression_reinit_lot_releve tmp
                     WHERE gpg.releve_id = COALESCE(tmp.inforeleve_id, '§§'));

        INSERT INTO suppression_reinit_lot_releve (echange_id)
        SELECT pub.id
        FROM techange pub
        WHERE EXISTS(SELECT 1
                     FROM suppression_reinit_lot_releve tmp
                     WHERE PUB.OBJETMAITRE_ID = COALESCE(tmp.releve_id, '§§'))
        UNION ALL
        SELECT pub.id
        FROM techange pub
        WHERE EXISTS(SELECT 1
                     FROM suppression_reinit_lot_releve tmp
                     WHERE pub.OBJETMAITRE_ID = COALESCE(tmp.inforeleve_id, '§§'));

        DELETE
        FROM techange pub
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE pub.id = tmp.echange_id);
        DELETE
        FROM tgrandeurphysiquegenerale g
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE g.id = tmp.gpg_id);
        DELETE
        FROM tgrandeurphysiquegeneraleinfor g
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE g.id = tmp.gpginforeleve_id);
        DELETE FROM treleve r WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE r.id = tmp.releve_id);
        DELETE
        FROM tinforeleve r
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE r.id = tmp.inforeleve_id);
        DELETE FROM ttache t WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE t.id = tmp.tache_id);
        DELETE
        FROM tanomaliereleve ano
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE ano.id = tmp.anomalie_id);
        DELETE
        FROM tanomalie ano
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE ano.id = tmp.anomalie_id);
        DELETE
        FROM telementdepopulationreleve edp
        WHERE EXISTS(SELECT 1 FROM suppression_reinit_lot_releve tmp WHERE edp.id = tmp.edp_id);

        UPDATE tcampagne
        SET nombreelements        = 0,
            nombreelementstraites = 0
        WHERE id = ID_CAMPAGNE;

        UPDATE tlot
        SET nbelements        = 0,
            nbelementstraites = 0
        WHERE campagne_id = ID_CAMPAGNE;

        UPDATE tstatistique s
        SET quantite = 0
        WHERE (s.campagne_id = ID_CAMPAGNE
            OR EXISTS
                   (SELECT 1 FROM TLOT L WHERE s.lot_id = l.id AND l.campagne_id = ID_CAMPAGNE
                   ));

        -- ajouts v13
        UPDATE tlot l SET selectionrealisee = 0 WHERE campagne_id = ID_CAMPAGNE;
        UPDATE tlotdereleve
        SET ETATTRAITEMENTSELECTION = 0
        WHERE id in (select id from tlot WHERE campagne_id = ID_CAMPAGNE);

        UPDATE tlignestatistique ligne
        SET quantite = 0
        WHERE EXISTS
                  (SELECT 1 FROM tstat s WHERE s.AXESTATISTIQUE_ID = ID_CAMPAGNE AND s.id = ligne.statistique_id);

        UPDATE tlignestatistique ligne
        SET quantite = 0
        WHERE EXISTS
                  (SELECT 1
                   FROM tstat s
                            JOIN tlot l ON l.id = s.axestatistique_id
                   WHERE l.campagne_id = ID_CAMPAGNE
                     AND s.id = ligne.statistique_id);

        -- attention ça fait le commit!
        TRUNCATE TABLE suppression_reinit_lot_releve;
        DROP TABLE suppression_reinit_lot_releve;
    exception
        -- Ne pas jeter d'exception si la table suppression_reinit_lot_releve n'existait pas déjà
        when sqlstate '42P01' then
    end;
$$
language plpgsql;

 Nombre d'éléments par statut d'EDP des lots d'une campagne donnée 
Cette requête sert notamment à contrôler que les statistiques d'une campagne et des lots correspondent à la réalité (nombre d'éléments et nombre d'éléments traités)
define CAMPAGNE_ID = 38375002 /* Campagne BT PERF - 14/06/2010 */
select c.id as "id cmp"
, c.libelle as "lib cmp"
, l.id as "id lot"
, l.libelle as "lib lot"
, c.nombreelements as "nb éléments cmp"
, c.nombreelementstraites as "nb éléments traités cmp"
, l.nbelements as "nb éléments lot"
, l.nbelementstraites as "nb éléments traités lot"
, elt.statut as "statut edp"
, count(*) as "nb par statut"
from telementdepopulationreleve elt, tlot l, tcampagne c
where elt.lot_id = l.id
and l.campagne_id = c.id
and c.id = &CAMPAGNE_ID
group by c.id, c.libelle, l.id, l.libelle
, c.nombreelements 
, c.nombreelementstraites
, l.nbelements 
, l.nbelementstraites 
, elt.statut 
order by c.id, l.id;

 Liste des EDP + leurs statuts & anomalies des campagnes 
define CAMPAGNES_ID = in ('6431096003','6431096001','6431096002','6431096004','6431096005')
select pds.reference, elt.statut, elt.enanomalie, ano.typeanomalie, ano.commentaire, ano.donneecontextuelle, c.libelle, l.libelle
from tcampagne c, tlot l, telementdepopulationreleve elt, tpointdeservice pds, tanomalie ano
where c.id = l.campagne_id and l.id = elt.lot_id and pds.id = elt.pointdeservice_id
and c.statut = 0 and l.statut = 0
and ano.elementpopulation_id(+) = elt.id
and c.id &CAMPAGNES_ID
order by c.libelle, l.libelle, pds.reference, ano.typeanomalie, ano.commentaire, ano.donneeContextuelle;

 Requêtes préparant les requêtes de sélection du batch REL001MT 
Ces requêtes sont exécutées en début de batch REL001MT, avant les requêtes de sélection des PDS dans la table BATCH_PDS à proprement parler. Il faut préalablement passer ce script pour que les requêtes pour avoir une chance de sélectionner des PDS dans la table avec les requêtes de sélection des PDS.
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (1, 1, 1) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (1, 4, 4) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (1, 3, 3) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (1, 5, 5) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (1, 2, 2) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (2, 1, 1) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (2, 4, 4) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (2, 3, 3) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (2, 5, 5) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (2, 2, 2) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (5, 1, 1) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (5, 4, 4) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (5, 3, 3) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (5, 5, 5) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (5, 2, 2) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (7, 1, 1) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (7, 4, 4) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (7, 3, 3) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (7, 5, 5) ;
 insert into TMP_BATCH_ENUMSOUSETAT (activite, SousEtatCampagne, SousEtatPDS) values (7, 2, 2) ;

 insert into TMP_BATCH_LOT (ID, ROLE, MODELELOT_ID, CAMPAGNE_ID, MODELECAMPAGNE_ID, TYPELOT, TYPEMODELELOTPERMANENT, CATEGORIESTOURNEE, ACTIVITES, SOUSETATSPDS, AVEC_CONF_MAT, AVEC_TOURNEE_LOT, AVEC_TOURNEE_CAMP, ESTIMATIVE) 	select l.id, l.role, l.modeleDeLot_id, camp.id, camp.modeleCampagne_id, ml.typelot, ml.typemodelelotpermanent , ';'||nvl(ml.categoriestournee, mc.categoriestournee)||';' as categoriestournee , ';'||nvl(ml.activites, mc.activites)||';' as activites , ';'||mc.SOUSETATSPDS||';' as SOUSETATSPDS , (select count(1) from MDLCMPRELEVE_CONFIGMATERIELLE MDLCONF     where  MDLCONF.SOURCE = mc.id and rownum < 2 ) as avecConfigMat , (select count(1) from MdlLotReleveTSP_Tournee MDLTO     where  MDLTO.SOURCE = ml.id and rownum < 2 ) as avecTourneesLot, (select count(1) from MDLCMPRELEVE_TOURNEE MDLTO      where  MDLTO.SOURCE = mc.id and rownum < 2 ) as avecTourneesCampagne , decode(camp.echeancedeclenchee_id, null, 0, (select e.estimative from techeancedereleve e     where e.id = camp.echeancedeclenchee_id)) as estimative 	from tlot l 	inner join tcampagne camp on camp.id = l.campagne_id   AND mod(camp.ETATOBJET,2)=0 	inner join tmodeledelotreleve ml on ml.id = l.modeledelot_id 	inner join tmodeledecampagnereleve mc on mc.id = camp.modelecampagne_id  where  (ml.typelot = 7   or ( ml.typelot = 6        and ml.TYPEMODELELOTPERMANENT = 5   )  or ( ml.typelot IN (0, 1)       and nvl(l.nbelements, 0) = 0 )   )	and l.statut = 0  AND mod(l.ETATOBJET,2)=0;

 Requête de comparaison des EDP Relève 
Cette requête est utile lorsqu'on souhaite comparer l'exécution de batchs de relève (notamment REL001MT et REL005MT) avant et après des développements pour vérifier s'il n'y a pas de regressions.
En plus des EDP des campagnes servant de test, la requête renvoie la réf. du PDS et les anomalies qui ont pu être créées.
Cela donne une bonne indication de la manière dont les EDP de relève ont été traités et permet de détecter rapidement les plus grosses regressions.
select pds.reference, elt.statut, elt.enanomalie, ano.typeanomalie, ano.commentaire, ano.donneecontextuelle, c.libelle, l.libelle
from tcampagne c, tlot l, telementdepopulationreleve elt, tpointdeservice pds, tanomalie ano
where c.id = l.campagne_id and l.id = elt.lot_id and pds.id = elt.pointdeservice_id
and c.statut = 0 and l.statut = 0
and ano.elementpopulation_id(+) = elt.id
and c.id in (
<mettre les id des campagnes qui servent au test>
)
order by c.libelle, l.libelle, pds.reference, ano.typeanomalie, ano.commentaire, ano.donneeContextuelle;
 Simulation d'un retour TSP pour toutes les relèves de campagnes 
Les requêtes suivantes permettent de simuler l'injection de fichiers de relève (DVO, SaturneRL...) :
 passage des EDP du statut 'sélectionné' au statut 'relevé'
 mise à jour de la relève à calculer : date = date de la relève précédente + 1jour
 mise à jour des index : valeur = valeur de l'index précédent + 10

Il est ensuite possible de passer le batch REL005MT pour qu'il calcule les consommations des relèves.
Les scripts doivent être exécutés après sélection des EDP de relève (REL001MT).

 Script 1 : sélection des id des campagnes dont on souhaite traiter les relèves (exemple de requête). 
Les id des campagens récupérées doivent être mis dans la variable ID_CAMPAGNES défini comme dans l'exemple ci-dessous.
select id, libelle from tcampagne where libelle in 
('Relève BT 02 - 30/01/2012','Relève BT 08 - 31/01/2012')
or datecreation > to_date('06/08/2012','dd/mm/yyyy') 
and statut = 0
and modelecampagne_role = 'com.hermes.itv.releve.businessobject.ModeleDeCampagneDeReleve';

-- déclaration de la variable contenant tous les id de campagnes dont on va metre à jour les relèves
define ID_CAMPAGNES = ('5796407002','5799223003','5799223008','5797105001','5799223007','5799223012','5799223005','5799223010','5799223004','5799223009','5799223001','5799223006','5799223011','5799223002')
;

 Script 2 : Mise à jour des EDP, Relèves et grandeurs physiques de type relevé :
 temps moyen requête 1 : 12 sec.
 temps moyen requête 2 : 16 sec.
 temps moyen requête 3 : 500 sec.
-- Passage des EDP au statut 'relevé'
update telementdepopulationreleve e set statut = 3 
where statut = 0 
and e.lot_id in (select l.id from tlot l , tcampagne c
where l.campagne_id in &ID_CAMPAGNES );

-- Mise à jout de la date de la relève à calculer : date relève = date relève précédente + 1 jour
update treleve rel set datereleve = (select relprec.datereleve +1
from treleve relprec where rel.releveprecedente_id = relprec.id)
where rel.statutreleve = 3 -- en cours de traitement
and exists (select * from telementdepopulationreleve elt, tlot l
where elt.releve_id = rel.id
and elt.statut = 3 -- statut relevé
and elt.lot_id = l.id
and l.campagne_id in &ID_CAMPAGNES
);

-- Mise à jour des grandeurs physiques de type index : valeur = valeur index précédent + 10
update tgrandeurphysiquegenerale gp set valeur = (select gpprec.valeur +10
from tgrandeurphysiquegenerale gpprec, treleve rel, treleve relprec
where gp.releve_id = rel.id
and rel.releveprecedente_id = relprec.id
and gpprec.releve_id = relprec.id
and gpprec.modelegrandeurphysique_id = gp.modelegrandeurphysique_id)
where exists (select * from telementdepopulationreleve elt, tlot l, tmodelegrandeurphysique mgp, tgrandeurphysiquegenerale gp1
where elt.releve_id = gp1.releve_id
and elt.statut = 3 -- relevé
and elt.lot_id = l.id
and mgp.id = gp1.modelegrandeurphysique_id
and mgp.releveoucalcule = 0 -- relevé uniquement
and mgp.typegrandeur = 1 -- energie
and mgp.soustype = 1 -- active
and mgp.structureinformation = 1 --index
and l.campagne_id in &ID_CAMPAGNES
and gp1.id = gp.id
);

 Validation des releves en anomalies 
 Dans le cas ou certaines releves sont non conformes, il est possible de faire une "validation manuelle" via le script :
-- validation manuelle des anomalies
UPDATE tanomalie
SET statutanomalie=2
WHERE id         IN
(SELECT a.id
FROM tanomalie a,
  telementdepopulationreleve e
WHERE a.elementpopulation_id=e.id
AND e.lot_id                 IN (SELECT id from tlot where campagne_id IN &ID_CAMPAGNES)
AND a.typeanomalie         IN ('810','800','801','1210','820') and a.statutanomalie=0
);
Il faut ensuite repasser REL005MT.

 Détection des PDS avec plusieurs modalités de relève 
Requête utile pour détecter des cas en anomalie avec REL001MT 

SELECT PDS.REFERENCE
FROM TPOINTDESERVICE PDS
	JOIN TSERVICESOUSCRIT SS 	ON SS.POINTDESERVICE_ID = PDS.ID
	JOIN TCONTRAT CTR 	ON CTR.ID = SS.CONTRAT_ID
	JOIN TMODALITERELEVE MREL	ON MREL.ID = CTR.MODALITERELEVE_ID
	JOIN TOFFREPRODUIT OFP	ON OFP.ID = CTR.OFFREPRODUIT_ID
	JOIN TMODALITEFACTURATION MPAY	ON MPAY.ID = OFP.MODALITEFACTURATION_ID
	JOIN TCONDITIONPAIEMENT CP 	ON CP.ID = CTR.CONDITIONPAIEMENT_ID
	LEFT JOIN TECHEANCIERPMTPARMENSU ECH ON (ech.id = cp.echeancierpaiement_id and mod(ech.etatobjet,2) = 0)
WHERE PDS.activite              != 4
	AND SS.TYPESERVICESOUSCRIT       = 0
	AND SS.STATUT                   IN (1, 3)
	AND SS.DIRECTEUR                 = 1
	AND CTR.STATUTEXTRAIT           IN (1, 2, 3)
	AND OFP.FOURNITUREOUACHEMINEMENT = 2
	AND mod(ss.etatobjet,2)          =0
	AND mod(pds.etatobjet,2)         =0
	AND mod(ctr.etatobjet,2)         =0
	AND mod(mrel.etatobjet,2)        =0
	AND mod(ofp.etatobjet,2)         =0
	AND mod(mpay.etatobjet,2)        =0
GROUP BY PDS.REFERENCE,	PDS.ID
HAVING COUNT(*)>1;

 Statistiques de durée de relève et du nombre d'estimation 
La requête suivante se base sur la dernière relève présente sur un PDS élec bt actif. Elle permet se base toujours sur la dernière relève du PDS
 ANC_DERNIERE_RELEVE : ancienneté de la relève (moyenne en nb de jours)
 ANC_RELEVE_PREC : durée moyenne par rapport à la relève précédente
 ANC_RELEVE_REELLE_PREC : durée moyenne par rapport à la relève réelle précédente
 PRCT_RELEVE_REELLE : pourcentage de relèves réelles
 PRCT_RELEVE_PREC_REELLE : pourcentage de relèves précédentes réelles 
 PRCT_RELEVE_PREC_NON_ESTIMEE : pourcentage de relèves pour lesquelles la relève précédente et la relève réelle précédente sont identiques
 PRCT_LINKY : pourcentage de relève utilisant une PACalendrier
 NB_PDS : nb de PDS traités

with PACM as (
  select pacm.id as PACM_ID, dernierereleve.ID as DERNIERERELEVE_ID,
  DERNIERERELEVE.RELEVEPRECEDENTE_ID as RELEVE_PRECEDENTE_ID,
  DERNIERERELEVE.RELEVEREELLEPRECEDENTE_ID as RELEVE_REELLE_PRECEDENTE_ID,
  dernierereleve.DATERELEVE as DATEDERNIERERELEVE 
  from tpointdeservice pds
  join tpacm pacm on pds.id = pacm.pointdeservice_id
    and mod(pacm.etatobjet,2) = 0
    and pacm.datefin is null
    and pacm.statutvalide = 1
  join treleve dernierereleve on dernierereleve.pacm_id = pacm.id
    and mod(dernierereleve.etatobjet,2)=0
    and not exists (select 1 from treleve rsuivante 
        where mod(rsuivante.etatobjet,2) = 0 
        and rsuivante.datereleve > dernierereleve.datereleve 
        and rsuivante.pacm_id = dernierereleve.pacm_id)
  where mod(pds.etatobjet,2) = 0
  and pds.etat = 4
  and PDS.NIVEAUDETENSION=3
  and pds.activite=0),
stats as (select current_date - TMP.DATEDERNIERERELEVE as ANCIENNETE_DERNIERE_RELEVE,
  TMP.DATEDERNIERERELEVE - (select datereleve from treleve r where r.id = tmp.RELEVE_PRECEDENTE_ID) as DUREERELEVE,
  TMP.DATEDERNIERERELEVE - (select datereleve from treleve r where r.id = tmp.RELEVE_REELLE_PRECEDENTE_ID) as DUREERELEVEREELLE,
  (select nvl2(r.pacalendrier_id,1,0) from treleve r where r.id = tmp.DERNIERERELEVE_ID) as AVEC_CALENDRIER,
  (select decode(r.naturereleve, 1,1,41,1,51,1, 6,1,0) from treleve r where r.id = tmp.DERNIERERELEVE_ID) as RELEVE_REELLE,
  (select decode(r.naturereleve, 1,1,41,1,51,1, 6,1,0) from treleve r where r.id = tmp.RELEVE_PRECEDENTE_ID) as RELEVE_PRECEDENTE_REELLE,
  case when TMP.RELEVE_PRECEDENTE_ID = TMP.RELEVE_REELLE_PRECEDENTE_ID THEN 1 else 0 end as SANS_RELEVE_ESTIMEE_PRECEDENTE
from PACM TMP)
select 
  (select avg(ANCIENNETE_DERNIERE_RELEVE) from stats where ANCIENNETE_DERNIERE_RELEVE > 0) as ANC_DERNIERE_RELEVE,
  (select avg(DUREERELEVE) from stats where DUREERELEVE > 0)  as ANC_RELEVE_PREC,
  (select avg(DUREERELEVEREELLE) from stats where DUREERELEVEREELLE > 0) as ANC_RELEVE_REELLE_PREC,
  (select 100*avg(RELEVE_REELLE) from stats where RELEVE_REELLE is not null) as PRCT_RELEVE_REELLE,
  (select 100*avg(RELEVE_PRECEDENTE_REELLE) from stats where RELEVE_PRECEDENTE_REELLE is not null) as PRCT_RELEVE_PREC_REELLE,
  (select 100*avg(SANS_RELEVE_ESTIMEE_PRECEDENTE) from stats where SANS_RELEVE_ESTIMEE_PRECEDENTE is not null) as PRCT_RELEVE_PREC_NON_ESTIMEE,
  (select 100*avg(AVEC_CALENDRIER) from stats where AVEC_CALENDRIER is not null) as PRCT_LINKY,
  (select count(*) from stats) as NB_PDS
  from dual;

Interface/releve
LKY06
Utilisation des classes de simulation pour LKY06

Historique de mesures (SMO)
Historique de mesures (ou SMO, Systèmes de Mesures Optionnels)

Fonctionnellement, un client peut s'abonner à un service optionnel de mesure pour récupérer le détail de ses consommations à une période donnée.
Pour cela, efluid met à disposition deux mécanismes, dits "historiques de mesures" :
 AEL-GRD qui appelle le service Rest "historique de mesures" pour récupérer les consommations (relèves et courbes) 
 une campagne de Workflow de publication en masse des historique de mesure (passe notamment par le batch CNT038MT)

Selon du paramétrage défini dans efluid, les données de consommations peuvent être récupérées de différents sources : 
 données stockées dans Efluid (résultant de recherches dans la Bdd efluid)
 données demandées au SEC (SmartEnergyCore) : appel de l'interface EF02
 données demandées au SI Linky : appel de l'interface LKY02
 données demandées au SI EOT : appel de l'interface de consultation des relèves / courbe d'EOT

efluid sert donc de passerelle d'un SI Tiers (visible du client, comme l'AEL) vers un autre SI Tiers (stockage des données de consommations) pour aller chercher les données de consommations.

Les natures d'action permettent de définir quel SI Tiers interroger pour récupérer les données de consommations.

Le processus de récupération des historiques de mesure peut être représenté par le schéma ci-dessous :

Fichier:HistoriqueDeMesureSynthèse.png

Paramétrage et outils nécessaires pour tester un appel Rest historique de mesures
Historique des mesures : Paramétrage et outils nécessaires pour tester un appel REST

Description des traitements codés
Historique de mesures : Description des traitements codés<!!!> ecore 

 Documentation 

 efluid - Documentation Fonctionnelle > AFD suite efluid > AFD ecore > PRV - Domaine prévision

 Classification 

 Généralités 

La classification est effectuée par la société Metnext (météo France) via leur service Decide. Voir la documentation sur eroom 

 efluid - Documentation Technique > DCT suite efluid > DCT ecore > Conception > PRV - Domaine prévision > ENERCOM_DECIDE_WEBSERVICES_V7.docx

 Tests du service avec SoapUI 

Il est possible de tester le service de classification via SoapUI de la manière suivante :

 Télécharger et installer SoapUI.
 Trouver la définition du wsdl de Decide dans ecore, normalement "wsdl/decide/Decide.wsdl".
 Copier l'URL du wsdl (targetNamespace), normalement "http://lrsrvmxt2/en/mxtws/".
 Dans SoapUI, faire "File -> New Project", puis dans la fenêtre entrer
 Project Name : ecore decide
 Initial WSDL/WASL : <URL du wsdl>
 Cela va créer un template. Remplir les éléments avec l'enveloppe ici-bas.
 Envoyer la demande, le résultat reçu va s'afficher à droite.

<soapenv:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:mxt="http://lrsrvmxt1/en/mxtws/">
   <soapenv:Header/>
   <soapenv:Body>
      <mxt:classify soapenv:encodingStyle="http://schemas.xmlsoap.org/soap/encoding/">
         <obj_classification_in xsi:type="mxt:obj_classification_in">
            <!-- Entrer le dossier de sortie, il doit être disponible en écriture pour Metnetx (voir avec IT) -->
            <URI_in xsi:type="xsd:string">/smb/batchsenercom/DEV/files/fichierIN/decide/</URI_in>
            <!-- Entrer la liste de fichier à envoyer -->
            <array_cdc_in xsi:type="soapenc:Array" xmlns:soapenc="http://schemas.xmlsoap.org/soap/encoding/">
               <!-- Fichier de consommation exporté en csv -->	
               <item>UEM_METZ_CONSO_6320_1338466279648.csv</item>
            </array_cdc_in>
         </obj_classification_in>
      </mxt:classify>
   </soapenv:Body>
</soapenv:Envelope>

 Paramétrage 

Le paramétrage doit correspondre à ce qui est dans l'enveloppe (voir ici haut). Il est possible d'utiliser le serveur de batch d'enercom dans le dossier DEV, mais il faut s'assurer que les droits d'écritures sont valides. Voir Enercom - Emplacements serveurs et fichiers.

# Répertoire de dépôt de fichiers pour l'interface enercom -> decide, vu depuis le serveur batch enercom
REPERTOIRE_ENTREE_DECIDE=\\\\172.16.1.7\\batchsenercom\\DEV\\files\\fichierIN\\decide\\
 
# Répertoire de dépôt de fichiers pour l'interface decide -> enercom, vu depuis le serveur batch enercom
REPERTOIRE_SORTIE_DECIDE=\\\\172.16.1.7\\batchsenercom\\DEV\\files\\fichierOUT\\decide\\
 
# Répertoire de dépôt de fichiers pour l'interface enercom -> decide, vu depuis le serveur decide (doit se terminser par un slash)
REPERTOIRE_ENTREE_DECIDE_METNEXT=/smb/batchsenercom/DEV/files/fichierIN/decide/
 
# Répertoire de dépôt de fichiers pour l'interface decide -> enercom, vu depuis le serveur decide (doit se terminser par un slash)
REPERTOIRE_SORTIE_DECIDE_METNEXT=/smb/batchsenercom/DEV/files/fichierOUT/decide/

 Decide 
L'application Decide, utilisée pour les calculs de classification et de prévision, est installée sur 4 serveurs:
- serveur de développement / intégration : lrsrvmxt2
- serveur de recette : lrsrvmxt1
- serveur de validation / pré-production : lvsrvmxt1
- serveur de production : lpsrvmxt1

Il arrive que les prévisions soient bloquées sur l'application Decide, sur une ou plusieurs famille Decide. Il convient alors de demander à S&R d'effectuer les opérations suivantes sur le serveur Decide concerné :
 Relancer la machine : shutdown –r now
 Indiquer les jobs restants comme terminés
 psql –d decide –U postgres
 update job_list set job_status_id = 4 where job_status_id != 4;
 update job_unit set processing = 0 where processing != 0;
 update enercom_jobs set status = 4 where status != 4;

 enercom 

TODO

 Liens externes 

Category:ecore
Category:domaine<!!!> ecore 

 Documentation 

 efluid - Documentation Fonctionnelle > AFD suite efluid > AFD ecore > COU - Domaine courbe

 Algorithmes 

La brique ecore fournit une liste d'algorithme pour traiter les courbes et les grandeurs. Ces traitement sont définis dans le document ici haut et ici sont résumés les différents appels possibles aux algorithmes.

 Généralités 

Tous les appels d'algorithme se font avec un AlgorithmeContext et retournent un AlgorithmeContext qui n'est pas nécessairement le même que le premier. Si c'est le même, il est garanti que l'original n'a pas subit de suppression. Le contexte d'algorithme contient un algorithme ; c'est le contexte qui est envoyé au process afin de procéder à l'exécution. L'algorithme est récupérer en base à chaque fois, il est interdit de faire l'appel new Algorithme(), qui est deprécié. Il faut faire l'appel suivant :

	// Récupération de l'algorithme clone
	Algorithme algorithme = AlgorithmeFactory.getInstance().get(new AlgorithmeFactory.ParAlgorithme().traitement(EAlgorithme.ALGORITHME_CLONE));

Ensuite, il faut construire le contexte :

	// Création d'un contexte vide avec l'algorithme clone comme traitement
	AlgorithmeContext algorithmeContext = new AlgorithmeContext();
	algorithmeContext.setTraitement(algorithme);

Puis faire l'appel au process :

	// Exécution de l'algorithme clone
	TraitementBusinessProcess traitementBusinessProcess = new TraitementBusinessProcess();
	AlgorithmeContext algorithmeContextRetour = (AlgorithmeContext) traitementBusinessProcess.doExecuter(algorithmeContext);

Dans le cas ici haut, l'algorithme clone n'a pas suffisamment d'élément pour s'exécuter, il lance donc une TraitementException avec le détail des éléments manquants du contexte.

 Algorithme clone 

Entrée 
 Une liste de courbe OU une liste de grandeurs

Sortie
 Une liste de courbe clonées OU une liste de grandeurs clonées, dépendant de l'appel

Exemple d'appel :

	AlgorithmeContext algorithmeContext = new AlgorithmeContext();
	algorithmeContext.setTraitement(AlgorithmeFactory.getInstance().get(new AlgorithmeFactory.ParAlgorithme().traitement(EAlgorithme.ALGORITHME_CLONE)));
	algorithmeContext.setCourbes(courbes);

	TraitementBusinessProcess traitementBusinessProcess = new TraitementBusinessProcess();
	AlgorithmeContext algorithmeContextRetour = (AlgorithmeContext) traitementBusinessProcess.doExecuter(algorithmeContext);

        Collection<Courbe> courbesClonees = algorithmeContextRetour.getCourbes();

 Gestion des valeurs 

 Réduction du nombre de valeurs 

Lorsqu'il y a chargement d'une courbe ou d'une grandeur, on charge toujours plus de point que ce qui est demandé, puisque le mode de stockage en base (BLOB) implique que charger une année n'est pas plus lent que de charger un seul point (ou presque). Il convient donc de faire deux opérations afin de réduire le nombre de point, sans modifier la courbe existante (et risquer de la persister avec moins de points et perdre ses valeurs), soit (1) faire un clone de la courbe (2) réduire les points.

L'algorithme clone est décrit ici haut. Pour réduire le nombre de point, l'appel suivant fonctionne, mais est assez lent.

	// Récupération des valeurs entre une dateDebut et une dateFin
        Collection<Valeur> valeurs = new ArrayList<Valeur>();
        for (Valeur val : grandeur.getValeurs().subMap(dateDebut, dateFin).values()) {
          valeurs.add(val);
        }

	// Suppression des valeurs courantes et rajout des valeurs
	grandeur.clearValeurs();
	grandeur.addAllToValeurs(valeurs);

 efluid 

TODO

 enercom 

TODO

 Liens externes 

category:domaine
category:ecore<!!!> ecore 

 Documentation  

efluid - Documentation Fonctionnelle > AFD suite efluid > AFD ecore > COA - Domaine courbe agrégée

 enercom 

TODO

 Liens externes 

category:ecore
category:domaine<!!!>category:Domaine
category:Domaine consommation
 Modèle simplifié 
C'est un modèle simplifié qui n'a pas vocation à remplacer l'AFD.

Image:Modèle consommation.jpg
 AFDs 
eRoom

Documentation logigramme moteur estimation : http://wperoom3.uem.lan/eRoomReq/Files/Prod11/DocFonctionnelleSuiteEfluid/0_2bbed

Les AFDs "archivées" ne sont pas représentées ci-dessous.

 AFD - affichage historique conso sur facture.doc
 Cas d'utilisation "Gérer historique de consommation sur facture"
 Cas d'utilisation "Afficher la nature de la facture réalisée"
 Cas d'utilisation "Afficher la nature de chaque index"
 Cas d'utilisation "Afficher une consommation annuelle sur le contrat"
 Cas d'utilisation "Afficher des chiffres d'affaire HT et TTC sur le contrat"

 AFD - CNS - Calcul à la courbe de charge V3.doc
 Cas d'utilisation "Gérer les fonctions de paramétrage des installations de comptage"
 Cas d'utilisation "Gérer les grandeurs physiques"
 Cas d'utilisation "Calculer les grandeurs physiques"
 Gestion des totaliseurs
 Définition du Cas "PSA METZ-BORNY – DLX N° 29031"
 Définition du Cas "PSA TREMERY – DLX N° 29030 1"
 Définition du Cas "PSA TREMERY – DLX N° 29030 1"
 Définition du Cas "PSA TREMERY – DLX N° 29030 2"
 Définition du Cas "GASTEC – DLX N° 21828"
 Définition du Cas "BELLE ISLE - DLX N° 20000"
 Définition du Cas "FC METZ – DLX N° 21829"
 Définition du Cas "HIA LEGOUEST – DLC32 N°00055"

 Références courbes (Saturne)
 Cas d'utilisation "Pose d'un compteur"
 Cas d'utilisation "Échange d'un compteur"
 Cas d'utilisation "Changement de programme d'un compteur"
 Cas d'utilisation "Déplacement d'un compteur"
 Cas d'utilisation "Déplacement d'un compteur"

 AFD - CNS - Consommation.doc
 Exemples de configurations matériels types
 Concepts
 Cas d'utilisation "Gérer les fonctions de paramétrage des installations de comptage"
 Cas d'utilisation "Saisir la relève et les grandeurs physiques"
 Cas d'utilisation "Gérer les instances de relève et les grandeurs physiques dans le cadre d’une saisie de carte T ou pour régularisation"
 Cas d'utilisation "Calculer les grandeurs physiques"
 Cas d'utilisation "Calculer les grandeurs physiques"
 Cas d'utilisation "Estimer des grandeurs physiques"
 Activité Gaz

 AFD - PDS en décompte validée V1.doc
 Concepts
 Création des configurations matérielles en décompte
 Paramétrage de la configuration matérielle lors du CRI
 Paramétrage de la configuration matérielle lors du CRI
 Consultation des paramètres d'une configuration matérielle en décompte
 Calcul des consommations d'une instance de relève de matérielle en décompte
 Présentation de la facture
 Description des cas en électricité

 AFD - SSC- gestion des PDS sans comptage V3.doc
 Domaine EDL - PDS
 Domaine Consommation
 Domaine Intervention
 Domaine Relève
 Domaine Offre Produit, Contrat et Moteur
 Reconstitution des flux

 AFD Calcul énergie en compteur.doc
 Impacts sur le domaine Campagne
 Impacts sur le domaine Offre Produit
 Impacts sur le domaine EDL - PDS
 Impacts sur le domaine Relève et Facturation
 Détails des traitements

 AFD complément adaptation consommation juin2006.doc
 Contrôle des séquences d'Index
 Contrôle de cohérence sur grandeur physique
 Dépendance des attributs de la relève

 Campagnes du domaine 
Campagne d'énergie en compteur
Campagne de calcul de la CAR
Campagne d'import de la CAR
 Documentation sur les notions de circuit électrique 

 Notions physiques 

 Notions de base sur les circuits électriques et l'énergie réactive
L'électricité : production et réseau

 Informations sur les transformateurs
Les transformateurs

A quoi sert un transformateur?

 Comptage 
Cahier des charges de la CRE sur le comptage électrique : http://www.cre.fr/documents/deliberations/communication/comptage-electrique

 Requêtes utiles 
 Purge campagne de valorisation de l'énergie en compteur (CNS001MT) 
define CAMPAGNE_ID =6466432001

DELETE FROM tanomalie
WHERE elementpopulation_id IN
  (SELECT edp.id FROM telementdepopulationconso edp, tlot l WHERE l.campagne_id='&CAMPAGNE_ID' AND edp.lot_id=l.id
  );

DELETE
FROM telementdepopulationconso
WHERE lot_id IN
  (SELECT l.id FROM tlot l WHERE l.campagne_id='&CAMPAGNE_ID'
  );
 
UPDATE tcampagne c SET c.nombreelements=0 WHERE id='&CAMPAGNE_ID'; 
UPDATE tcampagne c SET c.nombreelementstraites=0 WHERE id='&CAMPAGNE_ID'; 
UPDATE tcampagneenergieencompteur
SET montanttotal=0, energietotale=0,energietotalerelevee=0,montantenergietotale=0,
montantenergietotalerelevee=0, montantprimefixetotale=0, energietotalenonrelevee=0, mtprimefixegrd=0,
mtenergiefournisseur=0, mtenergiegrd=0, mtenergienonrelevegrd=0, mtenergienonrelevefournisseur=0,
mtenergierelevefournisseur=0, mtenergierelevegrd=0, mtprimefixefournisseur=0
WHERE id='&CAMPAGNE_ID';
UPDATE tlot l SET l.nbelements=0, l.nbelementstraites=0, l.selectionrealisee=0 WHERE l.campagne_id='&CAMPAGNE_ID'; 
DELETE tresultatdetailenergieoffre WHERE campagne_id='&CAMPAGNE_ID';
DELETE tresultatdetailenergieservice WHERE campagne_id='&CAMPAGNE_ID';
DELETE tresultatenergieoffre WHERE campagne_id='&CAMPAGNE_ID';
DELETE tresultatenergieservice WHERE campagne_id='&CAMPAGNE_ID';

 Recherche des points de service avec des historiques de conso mensuelles manquantes 
-- recherche des PDS avec un historique de consommation mensuelles incomplet
with DOUBLONS as (select h.pds_id, count(*) as nombre from thistoriqueconsomensuelle h
where mod(h.etatobjet,2) = 0
group by h.pds_id
having count(*) != 12)
select pds.reference, d.nombre, pds.activite from tpointdeservice pds, doublons d where pds.id = d.pds_id
order by pds.activite, pds.reference;

-- création des historiques manquant
insert into THISTORIQUECONSOMENSUELLE (id, acteurcreation, datecreation, codegrd, codefournisseur, etatobjet, mois, pds_id, objetmaitre_id, objetmaitre_role)
with pds2 as (select  H.PDS_ID as pdsId, H.codeGRD, H.codeFournisseur, H.objetmaitre_role from THISTORIQUECONSOMENSUELLE h where mod(h.etatobjet,2) = 0 
  group by h.pds_id, H.codeGRD, H.codeFournisseur, objetmaitre_role having count(*) < 12)
  ,numbers as ( select level as nb from dual connect by level <= 12)
select uniqueid.nextval, 'evtXXXXXX', current_date, pds2.codeGRD,pds2.codeFournisseur, 0,  numbers.nb, pds2.pdsId,  pds2.pdsId, objetmaitre_role
from pds2, numbers 
where not exists (select h.id from THISTORIQUECONSOMENSUELLE h where mod(h.etatobjet,2) = 0 and h.mois = numbers.nb and h.pds_id = pds2.pdsId);

 Recherche des points de service avec plus de 12 valeurs de consommation mensuelle 
-- recherche PACM avec plus de 12 conso mensuelles
select COUNT(*) "nb conso mensuelle", CM.PACM_ID, PDS.REFERENCE
from TCONSOMMATIONMENSUELLE CM, TPACM PACM, TPOINTDESERVICE PDS 
where CM.PACM_ID = PACM.id and PACM.POINTDESERVICE_ID = PDS.id and MOD(CM.ETATOBJET, 2)=0 
group by CM.PACM_ID, PDS.REFERENCE having COUNT(*) > 12;

=== Recherche des points de service avec plus de 12 valeurs de consommation mensuelle ===
--recherche PACA avec plus de 12 conso mensuelles
SELECT COUNT(*) "nb conso mensuelle", CM.PACALENDRIER_ID, PDS.REFERENCE
FROM TCONSOMMATIONMENSUELLE CM, TPACALENDRIER PACA, TPOINTDESERVICE PDS, TPACM PACM
WHERE CM.PACALENDRIER_ID = PACA.id AND PACA.PACM_ID = PACM.id and PACM.POINTDESERVICE_ID=PDS.ID AND MOD(CM.ETATOBJET, 2)=0 group by CM.PACALENDRIER_ID, PDS.REFERENCE HAVING COUNT(*) > 12;

--recherche PDS avec plus de 12 histo conso
select count(*) "nb histoconsomens", PDS.REFERENCE
from THISTORIQUECONSOMENSUELLE H, TPOINTDESERVICE PDS
where H.PDS_ID = PDS.ID and mod(H.ETATOBJET,2)=0
group by H.PDS_ID, PDS.REFERENCE having count(*) > 12;

-- suppression des historiques en doublon - on garde en priorite les historiques modifies le plus récemment, sinon ceux avec la plus grande valeur
merge into THISTORIQUECONSOMENSUELLE histoModifies
using (
select hdoublon.id, hdoublon.pds_id, hdoublon.mois,  
row_number() over(partition by hdoublon.pds_id, hdoublon.mois order by hdoublon.datemodification desc nulls last, hdoublon.valeur desc nulls last, hdoublon.id desc) as rang
from THISTORIQUECONSOMENSUELLE hdoublon
where mod(hdoublon.etatobjet,2) = 0
and (hdoublon.pds_id, hdoublon.mois) in (
SELECT h.pds_id, h.mois
FROM THISTORIQUECONSOMENSUELLE H
WHERE MOD(H.ETATOBJET,2)=0
GROUP BY H.PDS_ID, h.mois  HAVING COUNT(*) > 1)) tmp
on (tmp.id = histoModifies.id)
when matched then
  update set histoModifies.etatobjet = 1,
    histoModifies.datesuppression = current_timestamp,
    histoModifies.acteursuppression='evt163540'
    where tmp.rang > 1;

 Suppression des historiques conso mensuelles et conso mensuelles en double 
Penser à modifier la valeur de 'EVT' avant de passer le script (pour que le acteursuppression corresponde à une réf. d'évt)

DEFINE EVT='evt SRO-TEST'
set serveroutput on

DECLARE

TYPE REC_HISTO is record (
id THISTORIQUECONSOMENSUELLE.id%type
, pds_id THISTORIQUECONSOMENSUELLE.pds_id%type
, mois THISTORIQUECONSOMENSUELLE.mois%type
);
histo rec_histo;

-- historique conso
CURSOR C_REC_HISTO is 
select histo.id, histo.pds_id, histo.mois
from thistoriqueconsomensuelle histo, 
(select count(*), pds.id as pds_id from thistoriqueconsomensuelle h, tpointdeservice pds
where pds.id = h.pds_id
and mod(pds.etatobjet,2) = 0
and mod(h.etatobjet,2) = 0
group by pds.id
having count(*) > 12) doublons
where histo.pds_id = doublons.pds_id
and mod(histo.etatobjet,2) = 0
order by histo.pds_id, histo.mois;

TYPE REC_CMENS is record (
id TCONSOMMATIONMENSUELLE.id%type
, pacm_id TCONSOMMATIONMENSUELLE.pacm_id%type
, mois TCONSOMMATIONMENSUELLE.mois%type
);
cmens REC_CMENS;

-- conso mensuelles
CURSOR C_REC_CMENS is 
select cmens.id, cmens.pacm_id , cmens.mois
from tconsommationmensuelle cmens, (
  select count(*), h.pacm_id from tconsommationmensuelle h
  where mod(h.etatobjet,2) = 0
  group by h.pacm_id
  having count(*) > 12) doublons 
where cmens.pacm_id = doublons.pacm_id 
and mod(cmens.etatobjet,2) = 0
order by cmens.pacm_id, mois;

pdsPrec VARCHAR2(100);
pacmPrec VARCHAR2(100);
moisPrec number;
cpt number;

BEGIN

open C_REC_HISTO;
open C_REC_CMENS;

pdsPrec := null;
moisPrec := null;
cpt := 0;
loop
  fetch C_REC_HISTO into HISTO;
  exit when C_REC_HISTO%NOTFOUND;
  IF (pdsPrec = HISTO.PDS_ID AND moisPrec = HISTO.MOIS ) THEN
    cpt := cpt + 1;
    dbms_output.put_line('HISTO a supprimer no:'||cpt||' [id='||HISTO.ID||' / pds='||HISTO.PDS_ID||' / mois='||HISTO.mois||' ]');
    update thistoriqueconsomensuelle set etatobjet = 1, datesuppression = current_date, acteursuppression='&EVT' where id=HISTO.ID;
  END IF;
  pdsPrec := HISTO.PDS_ID;
  moisPrec := HISTO.MOIS;
end loop;

pacmPrec := null;
moisPrec := null;
cpt := 0;
loop
  fetch C_REC_CMENS into CMENS;
  exit when C_REC_CMENS%NOTFOUND;
  IF (pacmPrec = CMENS.PACM_ID AND moisPrec = CMENS.MOIS ) THEN
    cpt := cpt + 1;
    dbms_output.put_line('CMENS a supprimer no:'||cpt||' [id='||CMENS.ID||' / pacm='||CMENS.PACM_ID||' / mois='||CMENS.mois||' ]');
    update tconsommationmensuelle set etatobjet = 1, datesuppression = current_date, acteursuppression='&EVT' where id=CMENS.ID;
    update tgrandeurphysiquemensuelle set etatobjet = 1, datesuppression = current_date, acteursuppression='&EVT' where releve_id=CMENS.ID;
  END IF;
  pacmPrec := CMENS.PACM_ID;
  moisPrec := CMENS.MOIS;
end loop;

END;
/

 copie des données d'un PDS sur un autre (apres fusion EDL) 
Le script qui suit transfère d'une PDS source à un PDS destination les PACM (donc relève et conso mensuelle), histo conso mensuelle, PARE, PAProfil et recopie certaines info du PDS
Attention à s'assurer que ces concepts n'existent pas déjà sur le PDS destination, ou on risque d'avoir des doublons!

define PDS_SOURCE = '84911GC1';
define PDS_DEST = '84990GC1';
define EVT = 'evt 122208';

update tpacm set pointdeservice_id = (select id from tpointdeservice pds where reference ='&PDS_DEST')
,datemodification= current_date, acteurmodification = '&EVT'
where pointdeservice_id = (select id from tpointdeservice pds where reference ='&PDS_SOURCE');

update thistoriqueconsomensuelle histo set etatobjet = 1, acteursuppression = 'evt XXX' 
where pds_id = (select id from tpointdeservice pds where reference ='&PDS_DEST');

update thistoriqueconsomensuelle set pds_id = (select id from tpointdeservice pds where reference ='&PDS_DEST')
,datemodification= current_date, acteurmodification = '&EVT'
where pds_id = (select id from tpointdeservice pds where reference ='&PDS_SOURCE');

update tperiodeactiviteprofil set pointdeservice_id = (select id from tpointdeservice pds where reference ='&PDS_DEST')
,datemodification= current_date, acteurmodification = '&EVT'
where pointdeservice_id = (select id from tpointdeservice pds where reference ='&PDS_SOURCE');

update tperiodeactivitere set pointdeservice_id = (select id from tpointdeservice pds where reference ='&PDS_DEST')
,datemodification= current_date, acteurmodification = '&EVT'
where pointdeservice_id = (select id from tpointdeservice pds where reference ='&PDS_SOURCE');

update tpointdeservice pds 
set (pds.dateprochainerelevereelle, pds.consoannuellereference_unit, pds.consoannuellereference_value)= 
(select dateprochainerelevereelle, consoannuellereference_unit,  consoannuellereference_value 
from tpointdeservice where reference ='&PDS_SOURCE')
,datemodification= current_date, acteurmodification = '&EVT'
where reference ='&PDS_DEST';
 
undef PDS_SOURCE;
undef PDS_DEST;
undef EVT;

 contrôle de cohérence du paramétrage 
 vérification des MGP de dépassement quadratique 
-- le service de calcul de regroupement S10 n'a pas de sens pour une dépassement quadratique, vu que la fonction de dépassement quadratique n'est pas linéaire.
-- utiliser le S21 à la place.
select id, configurationmaterielle_id, libelle from tmodelegrandeurphysique 
where servicedecalcul = 10
  and ordredecalcul is not null
  and typegrandeur=2 and soustype=6
  and mod(etatobjet,2) = 0;

-- vérifie qu'il ne manque de MGP lors d'un regroupement de dépassement quadratique. S'il manque des poste, cela peut-être signe d'un mauvais paramétrage 
-- (on pointe vers les mauvais postes)
select cm.reference "ref config", 
mgp.id "ID mgp", 
mgp.libelle "mgp", 
ph.libelle "poste regroupement", 
ph2.libelle "poste regroupé sans mgp"
from tconfigurationmaterielle cm
	join tmodelegrandeurphysique mgp on mgp.configurationmaterielle_id = cm.id
	join tpostehorosaisonnier ph on ph.id = mgp.postehorosaisonnier_id
	join postehoros_postesregrpes lien on lien.source = ph.id
	join tpostehorosaisonnier ph2 on lien.dest = ph2.id
where mgp.servicedecalcul = 21
	and mgp.typegrandeur=2 
	and mgp.soustype=6
	and mod(mgp.etatobjet,2) = 0
	and mgp.ordredecalcul is not null
	and not exists (select 1 
		from tmodelegrandeurphysique mgp2
		where ph2.id = mgp2.postehorosaisonnier_id
			and mgp.brutounet = mgp2.brutounet
			and mgp.sensdemesure = mgp2.sensdemesure
			and mgp.configurationmaterielle_id = mgp2.configurationmaterielle_id
			and mgp2.typegrandeur=2 
			and mgp2.soustype=6
			and mgp2.releveoucalcule in (0,1,6)
			and mod(mgp2.etatobjet,2) = 0
	);

 vérification de la cohérences des profils de consommation 
Ce script retourne les part par poste et coef de répartition de sous profils incohérents (segments ou sous profils conso incompatibles avec le profil de consommation
select 'part par poste' "role", ppp.id, ppp.etatobjet, ppp.sousprofilconsommation_id, ppp.segment_id 
from tpartparposte ppp 
where mod(ppp.etatobjet,2)=0
	and not exists 
	(select 1 from tsousprofilconsommation sp, tsegment seg, tprofilconsommation prof
		where  mod(sp.etatobjet,2) = 0
                and mod(seg.etatobjet,2) = 0
                and mod(prof.etatobjet,2) = 0
                and seg.profilconsommation_id = prof.id
                and seg.id = ppp.segment_id
                and sp.profilconsommation_id = prof.id
                and sp.id = ppp.sousprofilconsommation_id)
union all
select 'coef de répartition', coef.id, coef.etatobjet, coef.sousprofilconsommation_id, coef.segment_id 
from tcoefficientrepartitionenergie coef
where mod(coef.etatobjet,2)=0
	and not exists 
	(select 1 from tsousprofilconsommation sp, tsegment seg, tprofilconsommation prof
		where  mod(sp.etatobjet,2) = 0
                and mod(seg.etatobjet,2) = 0
                and mod(prof.etatobjet,2) = 0
                and seg.profilconsommation_id = prof.id
                and seg.id = coef.segment_id
                and sp.profilconsommation_id = prof.id
                and sp.id = coef.sousprofilconsommation_id);

Cette rêquête retourne les couples segment-sous profil consommation avec un nombre incorrect de part par poste et de coef de répartition
select 'part par poste' "role", s.id, sp.id
from tsegment s, tsousprofilconsommation sp
where mod(sp.etatobjet,2) = 0 and mod(s.etatobjet,2)=0
	and s.profilconsommation_id = sp.profilconsommation_id
	and 1 <> (select count(*) from tpartparposte ppp where mod(ppp.etatobjet,2) = 0 and ppp.segment_id = s.id and ppp.sousprofilconsommation_id = sp.id)
union all
select 'coef de répartition', s.id, sp.id 
from tsegment s, tsousprofilconsommation sp
where mod(sp.etatobjet,2) = 0 and mod(s.etatobjet,2)=0
	and s.profilconsommation_id = sp.profilconsommation_id
	and 12 <> (select count(*) from tcoefficientrepartitionenergie coef where mod(coef.etatobjet,2) = 0 and coef.segment_id = s.id and coef.sousprofilconsommation_id = sp.id)

 vérification de la cohérences des calendriers 
Cette rêquête retourne les profils jour (rattachés à des profils jour génériques) dont le poste horosaisonnier ne correspondant pas à la structure horosaisonnière du calendrier.
select pj.id, TH.POSTEHOROSAISONNIER_ID from tstructuretemporelle st
join STRUCTURETEMPORELLE_PROFILJOUR stpj on stpj.source = st.id
join TPROFILJOUR pjg on pjg.id = stpj.dest
join TLIENDECLINAISONGEO lien on LIEN.PROFILJOURGENERIQUE_ID = pjg.id
join TPROFILJOUR pj on pj.id = LIEN.PROFILJOUR_ID
join TTRANCHEHORAIRE th on pj.id= TH.PROFILJOUR_ID
join TPOSTEHOROSAISONNIER ph on ph.id = TH.POSTEHOROSAISONNIER_ID
where st.reference = '9'
and PH.STRUCTHOROS_ID != ST.STRUCTUREHOROSAISONNIERE_ID;

 Requêtes mise à jour données 

 Suppression d'une relève 

Avec &id_releve = identifiant de la relève à supprimmer, &num_evenement = numéro de l'évènement

 Avant la mise à jour, vérifier que la relève à supprimer ne précède pas une autre relève. La requête suivante doit toujours renvoyer 0 avant la suppression :
SELECT COUNT(*) FROM treleve WHERE (RELEVEPRECEDENTE_ID = '&id_releve' OR RELEVEREELLEPRECEDENTE_ID = '&id_releve')
 AND MOD(etatobjet,2) = 0;

1. Mettre à jour la table de relèves (TRELEVE) champs etatobjet, datesuppression, acteursuppression
UPDATE TRELEVE set ETATOBJET = 1, DATESUPPRESSION = current_date, ACTEURSUPPRESSION = '&num_evenement'
 WHERE id = '&id_releve' and mod(etatobjet,2) = 0;

2. Mettre à jour la table de grandeurs physiques (TGRANDEURPHYSIQUEGENERALE) champs etatobjet, datesuppression, acteursuppression
UPDATE TGRANDEURPHYSIQUEGENERALE set ETATOBJET = 1, DATESUPPRESSION = current_date,
 ACTEURSUPPRESSION = '&num_evenement' WHERE releve_id= '&id_releve' and mod(etatobjet,2) = 0;

3. Supprimer la liaison dans la table FACTURE_RELEVES
DELETE FROM FACTURE_RELEVES WHERE dest = '&id_releve';

4. Mettre à jour la table d'éléments de population relève (TELEMENTDEPOPULATIONRELEVE) champs etatobjet, acteurmodification, datemodification, statut, releve_id

 Avant la mise à jour des éléments de population, vérifier que l'EDP n'a pas de relèves complémentaires.
La requête suivante doit toujours renvoyer 0 avant la suppression :
select COUNT(*) from telementdepopulationreleve WHERE relevecomplementaire_id= '&id_releve'
 and mod(etatobjet,2) = 0;

Puis mettre à jour la table d'éléments de population relève 
UPDATE telementdepopulationreleve SET ACTEURMODIFICATION = '&num_evenement',
 DATEMODIFICATION = current_date, STATUT = 5, RELEVE_ID = null
 WHERE (releve_id = '&id_releve' OR relevecomplementaire_id = '&id_releve') and mod(etatobjet,2) = 0;

5. Mettre à jour la table d'échange (TECHANGE) champs etatobjet, datesuppression, acteursuppression
UPDATE TECHANGE SET ETATOBJET = 1, DATESUPPRESSION = CURRENT_DATE,
 ACTEURSUPPRESSION = '&num_evenement' where OBJETMAITRE_ID = '&id_releve' and mod(etatobjet,2) = 0;

6. Mettre à jour la table des interventions (TINTERVENTION) champs indexcontresigne_id, releveestimativecomplement_id, releveexistant_id, relevenouveaucomptage_id

UPDATE TINTERVENTION itv SET INDEXCONTRESIGNE_ID = NULL WHERE INDEXCONTRESIGNE_ID = '&id_releve'
 AND EXISTS ( SELECT 1 FROM TAFFAIRE aff WHERE aff.id = itv.id
 AND mod(aff.etatobjet,2) = 0);

UPDATE TINTERVENTION itv SET RELEVEESTIMATIVECOMPLEMENT_ID = NULL WHERE RELEVEESTIMATIVECOMPLEMENT_ID = '&id_releve'
 AND EXISTS ( SELECT 1 FROM TAFFAIRE aff WHERE aff.id = itv.id
 AND mod(aff.etatobjet,2) = 0);

UPDATE TINTERVENTION itv SET RELEVEEXISTANT_ID = NULL WHERE RELEVEEXISTANT_ID = '&id_releve'
 AND EXISTS ( SELECT 1 FROM TAFFAIRE aff WHERE aff.id = itv.id
 AND mod(aff.etatobjet,2) = 0);

UPDATE TINTERVENTION itv SET RELEVENOUVEAUCOMPTAGE_ID = NULL WHERE RELEVENOUVEAUCOMPTAGE_ID = '&id_releve'
 AND EXISTS ( SELECT 1 FROM TAFFAIRE aff WHERE aff.id = itv.id
 AND mod(aff.etatobjet,2) = 0);

 Fichiers sources 
 Modèle consommation.VSD (Visio 2003)

 Loggers 
 Moteur d'estimation 
 Nom spécifique 
MOTEUR_ESTIMATION
 Afficher les logs dans un fichier dédié 
Pour obtenir les logs qui correspondent au logger du moteur d'estimation, il faut ajouter le fichier xml suivant dans le dossier properties2 et le nommer log4j2-test.xml
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">
  <Properties>
    <Property name="logDir">${sys:user.home}/log</Property>
  </Properties>

  <Appenders>
    <RollingFile name="RollingFile" filePattern="${logDir}/efluid_moteur_estimation_%d{yyyy-MM-dd}.log" includeLocation="true">
      <PatternLayout>
        <Pattern>%-5level|%thread|%mdc{User}|%mdc{Event}|%logger{1}|%class{1.}.%method : %line|%d{HH:mm:ss.SSS}|%msg%n</Pattern>
      </PatternLayout>
      <Policies>
        <TimeBasedTriggeringPolicy />
        <SizeBasedTriggeringPolicy size="500 MB" />
        <OnStartupTriggeringPolicy />
      </Policies>
    </RollingFile>
  </Appenders>

  <Loggers>
    <Logger name="MOTEUR_ESTIMATION" level="debug">
      <AppenderRef ref="RollingFile" />
    </Logger>
  </Loggers>
</Configuration>
Le fichier de log obtenu ira dans le dossier log dans votre home.
 Changer la configuration 
 Choix du dossier 
Le changement se passe à ce niveau :
<Property name="logDir">${sys:user.home}/log</Property>
Il suffit de changer le chemin indiqué.
 Choix du nom du fichier 
Le changement se passe à ce niveau :
<RollingFile name="RollingFile" filePattern="${logDir}/efluid_moteur_estimation_%d{yyyy-MM-dd}.log" includeLocation="true">
Il suffit de changer le pattern du nom après "${logDir}/".
 Choix du pattern 
Le changement se passe à ce niveau :
<Pattern>%-5level|%thread|%mdc{User}|%mdc{Event}|%logger{1}|%class{1.}.%method : %line|%d{HH:mm:ss.SSS}|%msg%n</Pattern>
Pour modifier le pattern à votre convenance, je vous invite à vous rendre sur log4j2 layouts et aller dans la partie Pattern Layout.
 Choix de l'appender 
Le changement se passe à ce niveau :
<AppenderRef ref="RollingFile" />
Pour changer la destination des logs, il suffit de changer "ref" de "AppenderRef", ou d'ajouter un autre "AppenderRef".
Si vous voulez créer une autre Appender il faut aller voir sur log4j2 appenders.
Sinon, voici un exemple d'Appender pour la console :
<Console name="Console" target="SYSTEM_OUT">
  <PatternLayout pattern="%-5level|%t|%X{User}|%X{Event}|%logger{36}|%d{HH:mm:ss.SSS}|%msg%n" />
</Console>
Vous copier cela dans la partie "Appenders" de votre fichier xml.
 Utilisation 
il faut créer le logger en tant qu'attribut de la classe :
private static final Logger LOGGER_MOTEUR_ESTIMATION = LoggerFactory.getLogger(LoggersConso.MOTEUR_ESTIMATION);
Puis on utilise l'API associée, exemple :
LOGGER_MOTEUR_ESTIMATION.debug("log avec paramètres : {} et {}", param1, param2);
 Méthode d'estimation utilisée 
 Nom spécifique 
METHODE_ESTIMATION_APPLIQUEE
 Changer la configuration 
voir loggers partie moteur d'estimation
 Afficher les logs dans un fichier dédié 
voir loggers partie moteur d'estimation
 Utilisation 
il faut créer le logger en tant qu'attribut de la classe :
private static final Logger LOGGER_METHODE_ESTIMATION_APPLIQUEE = LoggerFactory.getLogger(METHODE_ESTIMATION_APPLIQUEE);
Puis on utilise l'API associée, exemple :
LOGGER_METHODE_ESTIMATION_APPLIQUEE.debug("log avec paramètres : {} et {}", param1, param2);<!!!>Category:domaine
 AFD 
eRoom

 Batchs 
 Edition des déclarations des taxes locales sur l'électricité (E-0077MT)
 EditionEtatAvancesMensuEmisesProgram (MNS004MT)
 EditionEtatAvancesMensuDeduitesProgram (MNS003MT)
 EditionEtatAvancesMensuNonSoldeesProgram (MNS005MT)
 EditionEtatAvancesMensuImpayeesProgram (MNS007MT)
 Calcul de dérive d'échéancier de mensualisation (MNS009MT)

 Statuts Echéancier 
image:diagEtatSeqEcheancier.png

 Statuts Mensu 
image:diagEtatSeqMensu.png

 Déroutage régularisation de mensu 

image:deroutage regularisation.png

NB: Si le traitement n'est pas interrompu (indication "fin du traitement"), alors efluid calcule une échéancier de paiement (à une ou deux échéances) qui sera présenté sur la facture.
Cette création d'échéancier donc lieu uniquement avec un montant sans solde inférieur au seuil 2...<!!!> Introduction  
Le domaine intervention est le domaine relatif à tout ce qui concerne les interventions ou les planning intervention. Les éléments ci-dessous, sont alors des éléments fonctionnels ou du code permettant de mieux appréhender et comprendre toute l'étendue de ce domaine. 

Le domaine intervention possède aussi, pour l'instant un dossier relatif à la documentation du domaine, dans eRoom. Ce dossier documentation intervention permet ainsi de stocker les différents fichiers de documentation qui ne peuvent pas être directement téléversés dans le wike.

 Eléments documentés  

 AFDs 
Les AFD se trouvent dans eRoom et il en existe plusieurs types :
 On retrouve la documentation fonctionnelle Suite Efluid qui correspond aux fonctionnalités génériques de l'application. Parmi ces documents, il y a à la fois les AFD initialles et les écarts qui correspondent aux fonctionnalités qui ont été rajoutées par la suite.
 On retrouve aussi la documentation fonctionnelle Projets Clients qui elle, diffère en fonction des clients. L'application possède des différences en fonction des clients et toutes les AFD correspondant à ces différences sont des Projets Clients. 

Les AFD contiennent les éléments suivants :
 Concepts
 Cas d'utilisation : 
 Classification des services
 Paramètres de l'application

 Coder une évolution 

droite|640px|vignette|Schéma décrivant les interactions entre les différentes DI contractuelles

 Gestion des planifications
 Pour tout moteur de planification qui store des objets en cours de route comme c'est le cas pour Displanis, displanisWeb et planification par Créneaux, ne pas oublier de voir ce qui se passe en suspension de contrat:
 en effet, l'intervention n'est pas stockée en base, mais il faut prévoir de supprimer les objets stockés en bases tels que les déplacements et/ou les réservations

 3 points d'entrée pour l'Edition d'un Bon d'Intervention
 Affaire / bouton "éditer BI"
 EDL / affaire / bouton "éditer BI"
 Intervention (comptage) (déselectionner le Responsable si besoin) / sélectionner l'intervention puis bouton "éditer sélection"

 Gestion des DI dupliquée/miroir/reprise/fourniture/acheminement
 liste des cas de tests à faire pour vérifier le bon fonctionnement global.

 Tests relatifs aux créations de DIs 

 Les DIs contractuelles 

Les différents tests d'intégration relatifs à la création de DIs contractuelles ont tous été répertoriés dans un fichier excel stocké dans eRoom. Il contient un schéma explicatif du cas, le fichier contenant le test et sa place dans le code ainsi que la description complète : type de compteur, générateur de contrat utilisé, contexte de départ du test. 

Vous pouvez retrouver ce fichier grâce au lien suivant :
Lien vers le fichier sur eRoom

À noter : 
 Cela ne répertorie que les tests de création de DI. 
 Ils sont tous plus ou moins complétés par des tests qui portent presque le même nom mais avec l’acronyme CRI dedans et qui s’appuie sur un contexte préCRI (donc une DI), qui fait appel au test de fabrication de la DI correspondante.

 Ce principe est à la base de tous nos TI : 
 Le contexte répondant à la question : quelle est la situation à partir de laquelle on veut tester quelque chose
 La réponse à cette question étant : sur quel test existant je m’appuie -> GIVEN
 Puis on n’a plus qu’à dérouler les actions de notre test -> WHEN
 Et on contrôle -> THEN

 Les DIs non contractuelles 

Il existe différents tests basés sur la création de DI techniques sur des espaces de livraison avec ou sans contrat et dont le principe peut être exploité pour vérifier des mises à jours sur le domaine référentiel à l'issue d'un CRI.

Exemple : la mise à jour par le CRI du caractère HRV sur le PDS (Haut Risque Vital).

 Les différents webservices du domaine intervention 

Il existe deux types de webservices : les webservices SOAP et REST.

Concernant le développement des interfaces, une documentation avaient été effectuée à ce propos. Cette documentation est bien sûre à compléter. 
Concernant le fonctionnement un peu plus techniques des interfaces dans eFluid on retrouve aussi le document suivant : Webservices.

Category:Domaine
Category:Domaine Intervention<!!!>Category:domaine
 AFDs 
eRoom

 AFDs éléctricité 
 AFD - REC - Reconstitution des flux.doc
 Pré-études gestion des profils et RE
 Concepts et modélisation
 Cas d'utilisation "Gérer les profils et les RE"
 Cas d'utilisation "Gérer la spécialisation du composant campagne pour la RDF par le GRD"
 Cas d'utilisation "Gérer les traitements propres à la RDF par le GRD"
 Cas d'utilisation "Gérer la spécialisation du composant campagne pour la RDF par le fournisseur"
 Cas d'utilisation "Visualiser des courbes"

 AFD - reco flux - RE agrégés.doc
 Modélisation
 Cas d'utilisation "Valider des éléments de population"

 AFDs gaz 
 AFD - RDF - GAZ.doc
 Paramétrage des profils et des températures
 Modélisation
 Cas d'utilisation "Gérer les profils et les sous profils"
 Cas d'utilisation "Gérer les stations météo"
 Cas d'utilisation "Gérer manuellement les coefficients par station météo"
 Cas d'utilisation "Intégrer automatiquement les données profil et T°C"
 Cas d'utilisation "Intégrer les températures réelles journalières"
 Cas d'utilisation "Actualiser la CAR et le profil du PDS"
 Bilans de consommation J+1 et M+1
 Modélisation
 Cas d'utilisation "Sélectionner la population"
 Cas d'utilisation "Exécuter la RDF à J+1"
 Cas d'utilisation "Exécuter la RDF à M+1"
 Cas d'utilisation "Contrôler les bilans"
 Cas d'utilisation "Valider les bilans"
 Cas d'utilisation "Fermer la campagne"
 Cas d'utilisation "Recalculer les quantités"
 Gestion des comptes d'écarts
 Modélisation

 Liens utiles 
 [CRM] Détermination du responsable d'équilibre (RE), du fournisseur, des profils RDF

 Requêtes utiles 
 Sélectionner un PDS gaz en modereleve téléreleve 
SELECT pds.reference
FROM tpointdeservice pds,tperiodeactiviteprofil paprofil
WHERE pds.id = paprofil.pointdeservice_id
AND pds.activite = 2
AND paprofil.moderelevepds = 5
AND mod(pds.etatobjet,2) = 0
AND mod(paprofil.etatobjet,2) = 0;

 Purger une campagne RDFGaz 

define CAMPAGNE_ID = 'IDCAMPAGNE'
DELETE
FROM tanomalie
WHERE ELEMENTPOPULATION_ID IN
  (SELECT id
  FROM telementdepopulationrdfgaz
  WHERE lot_id IN
    (SELECT l.id FROM tlot l WHERE l.campagne_id='&CAMPAGNE_ID'
    )
  );
DELETE
FROM tbilanquantite
WHERE EDP_ID IN
  (SELECT id
  FROM telementdepopulationrdfgaz
  WHERE lot_id IN
    (SELECT l.id FROM tlot l WHERE l.campagne_id='&CAMPAGNE_ID'
    )
  );
DELETE
FROM telementdepopulationrdfgaz
WHERE lot_id IN
  (SELECT l.id FROM tlot l WHERE l.campagne_id='&CAMPAGNE_ID'
  );
UPDATE tcampagne c SET c.nombreelements=0 WHERE id='&CAMPAGNE_ID';
UPDATE tcampagne c SET c.nombreelementstraites=0 WHERE id='&CAMPAGNE_ID';
UPDATE tlot l SET l.nbelements=0 WHERE l.campagne_id='&CAMPAGNE_ID';
UPDATE tlot l SET l.nbelementstraites=0 WHERE l.campagne_id='&CAMPAGNE_ID';

 Purger une campagne RDFGaz MENSUELLE en fonction de la date d'éxécution 
 Pour une campagne JOURNALIERE il suffit de mettre e.frequencequotidienne = 1
 Ne pas oublier de mettre l'acteur suppression/modification
 
define DATEDEBUT = &5;
define DATEDEFIN = &6;

UPDATE tanomalie ano
SET ano.etatobjet           = 1,
    ano.datesuppression = CURRENT_DATE,
    ano.acteursuppression = 'Evenement xxx'
WHERE ELEMENTPOPULATION_ID IN
  (SELECT id
  FROM telementdepopulationrdfgaz
  WHERE lot_id IN
    (SELECT l.id
FROM techeance e, tcampagne c, tlot l
WHERE e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id
    )
  );

UPDATE tbilanquantite b
SET b.etatobjet = 1,
  b.datesuppression = CURRENT_DATE,
  b.acteursuppression = 'Evenement xxx'
WHERE EDP_ID IN
    (SELECT id
  FROM telementdepopulationrdfgaz
  WHERE lot_id IN
    (SELECT l.id
FROM techeance e, tcampagne c, tlot l
WHERE  e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id
    )
  );

update tcollectionBilanQuantite 
SET etatobjet = 1,
  datesuppression = CURRENT_DATE,
  acteursuppression = 'Evenement xxx'
where id in (
SELECT l.source
FROM collection_bilansquantite l,telementdepopulationrdfgaz edp,tbilanquantite b
  WHERE b.edp_id = edp.id
  AND b.id = l.dest
  AND edp.lot_id IN
    (SELECT l.id
FROM techeance e, tcampagne c, tlot l
WHERE  e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id
    )
);

update techange 
set  etatobjet = 1,
  datesuppression = CURRENT_DATE,
  acteursuppression = 'Evenement xxx'
where id in (select id from techange 
where objetmaitre_role='com.hermes.itv.reconstitutiongaz.businessobject.CollectionBilanQuantite' 
and objetmaitre_id in (SELECT l.source
FROM collection_bilansquantite l,telementdepopulationrdfgaz edp,tbilanquantite b
  WHERE b.edp_id = edp.id
  AND b.id = l.dest
  AND edp.lot_id IN
    (SELECT l.id
FROM techeance e, tcampagne c, tlot l
WHERE  e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id
    )));

update TVALEURCRITEREPUBLICATION 
set etatobjet = 1,
  datesuppression = CURRENT_DATE,
  acteursuppression = 'Evenement xxx'
where id in (select v.id 
from TVALEURCRITEREPUBLICATION v, ECHANGE_VALEURSDECRITERE l, TECHANGE e 
where l.source=e.id 
and l.dest=v.id 
and e.id in (select id from techange 
where objetmaitre_role='com.hermes.itv.reconstitutiongaz.businessobject.CollectionBilanQuantite' 
and objetmaitre_id in (SELECT l.source
FROM collection_bilansquantite l,telementdepopulationrdfgaz edp,tbilanquantite b
  WHERE b.edp_id = edp.id
  AND b.id = l.dest
  AND edp.lot_id IN
    (SELECT l.id
FROM techeance e, tcampagne c, tlot l
WHERE  e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id
    ))));

UPDATE telementdepopulationrdfgaz edp
SET edp.etatobjet = 1,
   edp.datesuppression = CURRENT_DATE,
   edp.acteursuppression = 'Evenement xxx'
WHERE lot_id IN
    (SELECT l.id
FROM techeance e, tcampagne c, tlot l
WHERE e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id);
  
UPDATE tcampagne c 
SET c.nombreelements=0,
c.nombreelementstraites=0,
c.statut = 0,
c.acteurmodification = 'Evenement xxx',
c.datemodification = CURRENT_DATE
WHERE c.id IN 
    (SELECT c.id
FROM techeance e, tcampagne c, tlot l
WHERE   e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id);

UPDATE tlot l
SET l.nbelements=0,
    l.nbelementstraites=0,
    l.statut = 0,
    l.acteurmodification = 'Evenement xxx',
    l.datemodification = CURRENT_DATE
WHERE l.campagne_id IN 
  (SELECT c.id
FROM techeance e, tcampagne c, tlot l
WHERE   e.dateexecutionprevue >= TO_DATE('&DATEDEBUT','DD/MM/YYYY')
AND e.dateexecutionprevue < TO_DATE('&DATEDEFIN','DD/MM/YYYY')+1
AND e.role = 'com.hermes.itv.reconstitutiongaz.businessobject.EcheanceRDFGaz'
AND mod(e.etatobjet,2) = 0
AND l.campagne_id = c.id
AND e.frequencequotidienne = 0
AND c.echeancedeclenchee_id = e.id);

commit;

UNDEF DATEDEBUT;
UNDEF DATEDEFIN;

 Purger les PAProfil gaz pour le batch RFG001 AffecterProfilBatch 
 modifier les dates
 
update tperiodeactiviteprofil pap 
set etatobjet = 1, acteursuppression='DEV', datesuppression=current_timestamp
where pap.role = 'com.hermes.itv.reconstitution.businessobject.PeriodeActiviteProfilGaz'
and trunc(pap.datedebut) = to_date('01042016');

update  tperiodeactiviteprofil pap set datefin = null, acteurmodification='DEV', datemodification=current_timestamp
where pap.role = 'com.hermes.itv.reconstitution.businessobject.PeriodeActiviteProfilGaz'
and trunc(pap.datefin) = to_date('01042016')-1;

 Insertion des données journalière gaz (coef PCS) 
 Crée les valeurs de coefficient PCS jusqu'à la date du jour depuis la dernière valeur trouvée sur chaque table PCS ouverte.
 
-- date jusqu'à laquelle on crée des valeurs
define DATE_FIN = current_date;

DECLARE 
                 
  cursor c_tablesOuvertes is
              select t.id, t.datedebut, t.libelle
              from TTABLECOEFFICIENTPCS t 
              where mod(t.etatobjet,2) = 0 
                  and t.datefin is null and t.periodicite in (1,2);                         
    
  r_table             c_tablesOuvertes%rowtype;
  totalLignesInserees INTEGER;
BEGIN 
  totalLignesInserees := 0;
  FOR r_table IN c_tablesOuvertes LOOP

    DECLARE
      type VALEURPCS    is record(id tvaleurcoefficientpcs.id%type, 
                                    datevaleur tvaleurcoefficientpcs.datevaleur%type);
      derniereValeur    VALEURPCS;
    BEGIN
      select id, datevaleur  into derniereValeur
      from
        (select vpcs.id, vpcs.datevaleur, row_number() 
            over(partition by tablecoefficientpcs_id order by vpcs.datevaleur desc) as rang
        from tvaleurcoefficientpcs vpcs
          join ttablecoefficientpcs tablepcs on tablepcs.id = vpcs.tablecoefficientpcs_id
        where tablepcs.id = r_table.id
          and mod(vpcs.etatobjet,2) = 0)
      where rang = 1;

      insert into tvaleurcoefficientpcs (id, acteurcreation, datecreation, etatobjet, datevaleur, tablecoefficientpcs_id, valeur, statut)
        with nouvellesDates as  
          (select trunc(&DATE_FIN +1 - level, 'DD') as nouvelleDate from dual 
              WHERE  trunc(&DATE_FIN,'DD') > trunc(derniereValeur.datevaleur, 'DD') 
            connect by trunc(&DATE_FIN - level, 'DD') > trunc(derniereValeur.datevaleur, 'DD') )
      select '§PCS' || to_char(current_date, 'DDMMRRHH24MI') || '_' || (totalLignesInserees + rownum), 'scriptCreationCoefPCS', 
              current_date ,0, nv.nouvelleDate, v.tablecoefficientpcs_id, v.valeur, 0
      from tvaleurcoefficientpcs v, nouvellesDates nv
      where v.id = derniereValeur.id;

      dbms_output.put_line(SQL%ROWCOUNT || ' valeurs insérées depuis le ' || derniereValeur.datevaleur 
                          || ' pour la table '|| r_table.libelle || ' (id = ''' || r_table.id || ''')' );
      totalLignesInserees := totalLignesInserees + SQL%ROWCOUNT;
    EXCEPTION
      WHEN NO_DATA_FOUND THEN 
        dbms_output.put_line('pas de valeur sur la table '|| r_table.libelle || ' (id = ''' || r_table.id || ''')' );
    END;

  END LOOP;
  

END;
/

undef DATE_FIN;

commit;<!!!>category:domaine
category:ecore

 ecore 

 Documentation 

TODO

 businessobject 

 Traitement 

TODO

 log 

 PeriodicLogManager 

Cette classe de log s'exécute à intervalle régulier via le paramètre PeriodicLogManagerParameters.LOG_MANAGER_POLL_INTERVAL_MS et log le retour de la liste de callback ajoutés via addCallback(PeriodicLogCallback). 

Afin d'utiliser ce log, la configuration suivante dans le framework2.properties du projet est nécessaire. 

 LOG_MANAGER = com.efluid.ecore.commun.utils.PeriodicLogManager

Voir le "DCT - Méthodologie tests de mémoire" pour plus d'infos.

 PerformanceLogManager 

Cette classe permet de faire des tests de performance, typiquement à partir des tests unitaires de JUnit. Afin d'utiliser la classe, la configuration suivante dans le framework2.properties du projet est nécessaire. 

 LOG_MANAGER=com.efluid.ecore.commun.log.PerformanceLogManager

Si la mesure de performance doit aussi contenir le temps SQL (non nécessaire), ajouter : 

 JDBC_CONNECTION_MANAGER=com.hermes.arc.commun.jdbc.PerfDetailConnectionManager

Le fichier de sortie du log est, par défaut, le répertoire "target" du projet dans lequel la classe est exécutée. Pour changer cette valeur, il suffit de créer un fichier "testperformance.properties" qui contient le paramétrage suivant. Voir TestPerformanceConstantes pour tout le paramétrage possible.

 CHEMIN_DOSSIER_SORTIE=D:\\java\\workspaces\\developpement_dev\\ecore\\target\\
 NOM_FICHIER_SORTIE_PAR_DEFAUT=test-performance

Afin d'améliorer le temps total pour les tests volumineux, il est fortement recommandé de désactiver la sortie console avec

 LOG_SEVERITY=1

dans le framework.properties. Par contre, seuls les messages d'erreurs vont sortir dans le fichier, il faut donc utiliser Log.error(String) (ou la méthode correspondante si le paramètre LOG_SEVERITY est moins restrictif).

Voir le DCT tests performance pour plus d'infos.

 utils 

 BBCodeProcesseur 

TODO

 BlocChamps 

TODO

 BlocDonneesTechniquesMailChamps 

TODO

 BlocPositionnableChamps 

TODO

 BlocTexteChamps 

TODO

 BlocUtils 

TODO

 CelluleLabelMgr 

TODO

 CouleurChamps 

TODO

 CouleurUtils 

TODO

 EditionPersonnaliseeConstantes 

TODO

 EditionPersonnaliseeStringUtils 

TODO

 EditionPersonnaliseeUtils 

TODO

 EditionPersonnaliseeXMLParameters 

TODO

 EnteteTableauMgr 

TODO

 ExpressionReguliereSimple 

TODO

 FiltreClasse 

TODO

 FiltreDonneesTechniquesMailOuConditionAffichageVerifiee 

TODO

 FiltreTagXML 

TODO

 FiltreTagXMLObjetMetier 

TODO

 FluxXMLChamps 

TODO

 LigneRadioLabelMgr 

TODO

 RomanUtils 

TODO

 TagXMLChamps 

TODO

 TagXMLUtils 

TODO

 Liens externes <!!!>Category:domaine

AFD
eroom

Aides Mémoires
Statuts des objets principaux
Facturation
 - 012345678101112Factureen cours de constitutionconstituéevalidéeémiseen cours d'annulationen cours de rectificationannuléerectifiéeabandonnéeacceptérefuséproforma
 - 01234567891011EDPselectionnéfacturablenon facturablecalculécontrôlévalidééditéémisà intégrer à bordereauabandonnéà intégrer facture multi fluidesfacture vide - 121314151617181920212223EDP(suite)mensualisé hors facture de régularisationécartédécompte de lissage à calculerdécompte de lissage à éditerdécompte de lissage émisà recalculerdérive à estimeréchéancier à éditerdérive éditéeà intégrer à envoi groupéproformaà réferencer - 24EDP(suite)à intégrer à facture agrégée

Mensualisation
 - 018Facturecrééecalculéeabandonnée
 - 0121214151617181920EDPselectionnéfacturablenon facturablemensualisédécompte de lissage à calculerdécompte de lissage à éditerdécompte de lissage émisà recalculerdérive à estimeréchéancier à éditerdérive éditée

Regroupement

Batchs

Ligne de valorisation<!!!> Introduction 

Il s’agit de revoir le fonctionnement de l’actuel CJP en intégrant les contraintes suivantes :
Intégrer toutes les activités, électricité, gaz et CU, quelque soit la taille du client
Intégrer la gestion des courbes de charge
Pouvoir faire :
Une vente d’énergie, dans un cadre de marché ouvert, en gaz ou en CU
Un diagnostic sur de l’existant
Pour les courbes de charge :
Une étude d’optimisation tarifaire
Une demande de cotation
Ces fonctions devront pouvoir être traitées :
Depuis la création d’une affaire ad hoc ; dans ce cas de figure il faudra pouvoir depuis l’affaire lancer la demande de création d’un contrat sur la base d’un scénario retenu par le client
Ou en partant d’un contrat, d’où l’on pourra créer une affaire
L’affaire sera associée à un workflow qui permettra de paramétrer des fonctions d’édition, de validation etc..
L’affaire spécialisera une affaire générique de sorte à permettre une souplesse d’utilisation par l’ajout de nouveaux attributs et onglets par les clients, tout en permettant un haut niveau de paramétrage du workflow associé.
La simulation et l’étude tarifaire pourront être lancées depuis efluid et depuis énercom pour les courbes de charge, avec des fonctions supplémentaires depuis efluid, ce qui implique qu’une partie des développements devra être réalisé dans ecore.

 Documentation 
TODO : vérifier les liens ci-dessous
 Ecore 
 efluid - Documentation Fonctionnelle > AFD suite efluid > AFD ecore > SIM - Domaine simulation tarifaire > AFD - SIM - Simulation tarifaire - ecore.doc

 Efluid 
 efluid - Documentation Fonctionnelle > AFD suite efluid > AFD efluid > SIM - Simulation tarifaire > AFD - SIM - Simulation tarifaire - efluid.doc

 Enercom 
 enercom > Système d'information > AFD énercom > SIM - Domaine simulation tarifaire > AFD - SIM - Simulation tarifaire - enercom.docx

 Autre 
 Proto performance simulation tarifaire
 DCT simulation tarifaire
 DCT objets dynamiques (archi)
 Etudes préalables

Category:domaine
Category:simulation<!!!>Category:domaine

 Documentation 
 Qu'est-ce que la MDE ?
 Plan de progression
 Page spécifique au portail MDE

 AFDs 
 AFD du composant affaire générique
 AFD de paramétrage du modèle d'affaire MDE

 DCTs 
 DCT - AFG - affairegenerique - concept - affaire MDE
 DCT - AFG - affairegenerique - concept - operation MDE
 DCT affaires génériques

 Paramétrage 
 Note de paramétrage pour créer WKF et modèles d'affaires des lots suivi de l’isolation thermique, suivi de la climatisation performante, suivi du chauffe-eau solaire et suivi des lampes économiques
 Exemple paramétrage critères pour calculs de prime
 Note de paramétrage pour rendre impossible l'ajout d'une opération après une certaine étape

 Lancement Batchs 
 Lancer un batch workflow WKF 999
 Points d'arrêts utiles pour le debug des batchs MDE
 WorkflowBatch.traiterTraitement
 AffaireMDEBatchDAO.fillLoadObject
 CritereUtils.getValeurRetournerParMethode

 Architecture technique  

 Les process liés à la maitrise de l'énergie sont codés dans efluid dans le package : MDE
 L'affaire MDE est pilotée par un workflow dans efluid : CAMPAGNE_WORKFLOW_MAITRISE_DEMANDE_ENERGIE = 29
 Le portail partenaire est codé dans l'AEL dans un package : MDE
 Paramétrage spécifique à la MDE dans l'AEL : Parametrage portail partenaire
 Installation du portail en développement : Installation du portail en développement

 FAQ efluid  

 Lien "créer un compte partenaire" inexistant 
 Si le lien n'apparait pas en dessous de l'e-mail du contact c'est que le paramètre entreprise "activerCreationComptePartenaireEnModificationContact" n'est pas à TRUE. 
Celui-ci est par défaut à FALSE, il faut donc penser à le modifier sur l'environnement en question.

 Aucun mail n'est envoyé après validation de la création de compte partenaire 
 Si aucun mail n'est envoyé, c'est sans doute qu'aucun modèle d'édition n'a été créé pour ça.
Vérifier dans la recherche de modèle d'édition si "mail portail partenaire" existe ou s'il est correctement paramétré grâce à la note de paramétrage pour la création d'un compte partenaire et mails personnalisés associés.

 Traitements ou onglets non visibles 
 Vérifier le paramétrage :
- Le traitement ou l'onglet doit être correctement associé au type de role d'operateur en question dans le modèle d'affaire/d'opération..
A vérifier dans efluid par exemple au niveau de l'écran modèle d'affaire, dans le tableau des zones actives de type consultation (en zoomant sur le type de rôle opérateur on a la liste des traitements possible par étape)

- Le user connecté doit avoir le bon type de rôle opérateur : à vérifier dans efluid via le lien info en haut à droite puis tout en bas sur l'onglet utilisateur

 Pour utiliser une version de dev de l'edk 
 récupérer branche develop de l'edk
 faire les modifs si nécessaire
 regénérer les jar EDK : mvn clean install -P add-java-sources-in-archive
 sur le projet efluid sous Eclipse, mettre le profil maven edk-jar-DEV puis faire un maven update dependancies et un build

 Pour utiliser une version de dev d'ecore 
Faire de même
 récupérer branche develop d'ecore
 faire les modifs si nécessaire
 regénérer les jar ecore
 sur le projet efluid sous Eclipse, mettre le profil maven ecore-jar-DEV puis faire un maven update dependancies et un build

 Pour utiliser une version de dev de l'archi 
Sur le projet efluid sous Eclipse, mettre le profil maven archi-jar-DEV puis faire un maven update dependancies et un build<!!!>Category:application
Category:etinéraire
Category:domaine

 Configuration 
 utilisateur : superadmin / superadmin
 maven : portail-recrutement
 Fichiers ael 
hermes2.properties
efluid_BCCLIENT_SOAP_URL=http://localhost:8080/efluid/bcclientaelsoap
BACKUP_SOAP_MESSAGES=true
BACKUP_SOAP_MESSAGES_DIR=D:/java/workspaces/Developpement_dev/ael/logs
LOG_SEVERITY=10
ADDITIONAL_BUNDLE=DEV_AEL
hermes2.properties
HERMES_DEBUG=true

EXCEPTION_MGR_PATTERN_ID={0,date,yyyyMMdd_HHmmssSSS}_{1}
EXCEPTION_MGR_DIRECTORY=D:\\java\\logErreursInternes\\ael
EXCEPTION_MGR_DELETE_DIRECTORY=D:\\java\\logErreursInternes\\ael\\delete

UTILISATEUR_MOT_DE_PASSE_TAILLE=0;3
UTILISATEUR_MOT_DE_PASSE_CONTROLES=false;false;false;false
UTILISATEUR_MOT_DE_PASSE_REGLES=0;0
UTILISATEUR_MOT_DE_PASSE_DICTIONNAIRE_MOTS_INTERDITS=21212121
UTILISATEUR_MOT_DE_PASSE_DATE_PARAM=03/12/2015
ADMIN_MOT_DE_PASSE_TAILLE=2;0
ADMIN_MOT_DE_PASSE_CONTROLES=false;false;false;false
ADMIN_MOT_DE_PASSE_REGLES=2;0
ADMIN_MOT_DE_PASSE_DICTIONNAIRE_MOTS_INTERDITS=password!123;motdepasse!123;21212121
ADMIN_MOT_DE_PASSE_DATE_PARAM=03/12/2015

HERMES_HELP_ROOT=http://localhost:8887

 Documentation  

 Plan de progression

 AFDs 
 AFD Recrutement
 AFD - GRH - paramétrage de suivi des affaires de recrutement

 DCTs 
 DCT - GRH - affairerecrutement - Enchaînement - Gérer un candidat
 DCT - GRH - affairerecrutement - Enchaînement - Gérer une annonce

 DCT - GRH - affairerecrutement - Enchaînement - Gérer la connexion à l'espace candidat.doc
 DCT - GRH - affairerecrutement - Enchaînement - Gérer une candidature depuis un portail.doc
 DCT - AEL - Authentification

 Etudes 

 Etude de la séparation de la séparation du code entre le projet etinéraire & efluid : evt 123525

 Architecture technique  

 La candidature est une affaire de recrutement (affaire générique) codée dans efluid dans un package : GRH
 L'affaire de recrutement est pilotée par un workflow dans efluid : CAMPAGNE_WORKFLOW_SUIVI_RECRUTEMENT = 34
 Le portail de recrutement est codé dans l'AEL dans un package : GRH
 Paramétrage spécifique à étinéraire dans l'AEL : Parametrage etineraire
 Documents d'architecture : http://wperoom1/eRoom/Production/DocTechniqueEfluid/0_96b4e
 Installation du portail en développement : Installation du portail en développement

 Historique du projet  

 Demande de la part d'UEM de posséder une gestion du recrutement par workflow / affaire générique (spec : FG)
 Intégration du domaine etinéraire dans le groupe workflow

 FAQ  

 Lancement tests selenium  : 
- Utiliser le profil maven test-selenium dans l'AEL

- Ne pas tenir compte de l'erreur de compilation "Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-dependency-plugin:2.3:copy (execution: copy-dependencies, phase: process-test-resources)" 

- Les tests sont codés dans la classe TestsPortailRecrutement, ils se lancent comme les JUnit (clic droit Run As JUnit test)

- Le selenium2.properties est nécessaire pour faire tourner les tests, il faut notamment y spécifier les paramètres suivants :

# Utilisation du driver IE (Pop-up IE pour les tests)
FENETRE_IE=true

# Chemin pour l''application
APPLICATION_CHEMIN=/ecoreWar/jsp/arc/commun/frame.jsp

# Chemin du driver IE de sélénium
DRIVER_IE_CHEMIN=D:\\java\\workspaces\\resources\\IEDriverServer.exe

BASE_URL=http://localhost:8090/ael/

# Utilisé pour la création de compte
NOM_UTILISATEUR_PAR_DEFAUT={login}
MOT_DE_PASSE_PAR_DEFAUT={mdp}

TEMPS_ATTENTE_APPLICATION_SECONDES=1
MODE_SANS_CONTROLE_SUR_SUITE=true

IMPLEMENTATION_DRIVER=com.efluid.ecore.selenium.ie.InternetExplorerDriverEcore

- A lire aussi : Le Guide du développeur Selenium 

  Menu de gauche inexistant  : 
Bien vérifier que la valeur du paramètre HERMES_MENU_ID dans ael/properties/hermes2.properties (en V11) ou du ael-webapp/classes/Hermes.properties (en V12)
Pour Etineraire HERMES_MENU_ID=menuPortailRecrutement<!!!>Category:domaine

Documentation générale
 [AFD du domaine]
...

Batchs 
État des lieux
Dans le domaine règlement nous avons aujourd’hui 7 grandes familles de batch, à savoir :
 
1.	Création des encaissements à partir d’un fichier en entrée
2.	Création des encaissements à partir des lignes de comptes débitrices (dettes)
3.	Création des décaissements à partir des lignes de comptes créditrices (avoirs)
4.	Gestions de retours banque (impayés bancaires, VNE et DCD)
5.	Génération des flux sortants :
a.	Génération des ordres de prélèvements
b.	Génération des ordres de virements
c.	Génération des flux remboursements divers
6.	Créations des opérations diverses
7.	Editions
 
Orientation des nouveaux développement lot 12
Dans le cadre des nouveaux développements ERDF (et pour lesquels vous êtes concernés)  nous allons nous intéresser aux familles 1, 2, 3, 4 et 5 uniquement.  Ci-dessous mes remarques :
Remarque n° 1
Pour certaines de ces 5 familles il existe aujourd’hui plusieurs versions de  batch (V1, V2, V3). La questions que l’on se pose donc aujourd’hui et de savoir vers où on va aller ? Et comment on va structurer nos batch pour qu’ils soient évolutifs (à moindre cout)  et  que l’on puis développer facilement des nouvelles interfaces et/ou batch de création des opération financières de manière générale.  Aujourd’hui nous avons deux solutions mises en place, à savoir :
 
Solution 1 
A chaque nouveau format de fichier on crée un nouveau code batch qui s’appuie sur une structure commune. 
A chaque nouvelle méthode de création des encaissements à partir d’une ligne débitrice (par code banque, par offre produit, SEPA) on crée un nouveau code batch qui s’appuie sur une structure commune 
 
Solution 2
A chaque nouveau format de fichier on crée un mappeur spécifique mais le code batch reste le même. Ils instancie des classes filles qui s’appuie sur des classes mères 
A chaque méthode de création des décaissements à partir d’une ligne créditrice (par code banque, par offre produit, SEPA) on crée un JobDAO spécifique mais le code batch reste le même.
 
Chaque solution possède  ses avantages et ses inconvénients mais celle que l’on veut  privilégier aujourd’hui est la solution 2. En effet elle permet de ne gérer qu’un seul batch qui, selon le type de format à gérer ou selon la méthode d’affectation (paramètre en entré du batch), le batch  va instancier les bonnes classes filles. Ainsi aux yeux des tous les clients et de la recette il n’existera plus qu’un seul batch à gérer par famille.  Côté conception technique il y aura des classes filles de petite taille à créer à chaque nouveau développement.

Remarque n° 2 
Les batch, quelques soient leur familles, ils ont besoin de deux socles communs , à savoir :
Un socle commun qui est purement batch 
Un socle commun qui relève des traitements
 
Socle commun batch : dans ce socle commun nous plaçons les requêtes communes de type SELECT, MAJ et/ou INSERT car il s’agit presque toujours de faire la même chose : MAJ du solde du compte client, MAJ du solde de la ligne lettre, création d’une ligne lettrante, création d’une transaction, etc.  Il existe aujourd’hui un socle commun batch V3 pour les batch de la famille numéro 1.  Pour les autres familles des classes mères ont été crées mais il reste encore à fignoler et/ou renforcer certaines parties. 
 
Socle commun traitement : il s’agit ici de factoriser les traitements communs entre le TP et le batch tout en mettant une couche « proceses » intermédiaire entre les batch les  « process » d’imputation comptable. Ce socle « traitement » permet au système non seulement de réaliser les imputations comptable CAC et IC mais aussi de gérer les méthodes d’affectation pour les batch de la famille 1.  Ce socle commun centralise l’intelligence (ou les règles) de la comptabilité. Il est disponibles pour toutes les familles de batch.  La seule chose qui manque pour la famille numéro 4 est la couche intermédiaire.

Remarque n° 3 
Comment on voit le futur ?  Avec  l’arrivé du SEPA nous souhaiterons n’avoir qu’un seul bath par famille de batch qui géreront les trois spécificités suivantes : 
Règles SEPA 
Règles EDF 
Règles ERDF

CONCLUSION
Les batch d’ERDF devront donc être conçus en se basant sur la solution 2. Ainsi :

Pour la création d’encaissements à partir des lignes de compte débitrices on doit arriver à la même conception de ce que CGU a fait pour le REG024MT : le REG024MT peut gérer plusieurs type de formats de fichiers. Il suffit de lui passer  en paramètre du batch le format du fichier que l’on veut gérer. Dans notre cas il faudra lui passer en paramètre du batch la méthode d’affectation : Par code banque ? Par  offre produit ? SEPA ? ERDF ? . Puis le batch, en fonctionne de la méthode, va instancier les bonnes classes filles  Cette remarque concerne Tarik (evt 86445). Tarik une fois que nous aurions conçu ainsi le batch ERDF, ce batch deviendra, dans le futur, le seul batch de création des encaissements à partir d’une ligne débitrice. Nous pourrons donc nous séparer de autres vieux batch.

Pour la création des décaissements à partir des lignes de compte créditrices : même remarque que la précédente. Cette remarque concerne Tarik (evt 86445)

Pour la gestion des retours banque à partir de fichiers en entrée : même remarque que la remarque précédente. Dans cet cas de figure on devra passer en paramètre du batch le format du fichier (SEPA, EDF, ERDF) cette remarque concerne Jérôme et Xavier (evt 89665 et 89667).

Pour la génération des flux de prélèvements et virements même remarque que la remarque précédente. Dans ce cas de figure on devra passer en paramètre du batch le format du fichier de sortie (SEPA, EDF, ERDF) cette remarque concerne Catherine et Tarik (evt 89663 et 89664).

Pour la création d’encaissements via un fichier en entrée on doit arriver à la même conception de ce que CGU a fait pour le REG024MT : le REG024MT peut gérer plusieurs type de formats de fichiers. Il suffit de lui passer  en paramètre du batch le format. Cette remarque ne concerne pour le moment personne<!!!>Category:domaineCategory:Liste de gestionCategory:pole composants transverses
 Documentation générale 
 AFD produit
 AFD client
 DCT

 Concepts 
 type de rôle : "compétences" de l'agent

 poste de travail : différent du poste dans l'annuaire mais même notion

 Une tâche => Affecter du travail aux personnes
 avoir une vue globale des travaux dans plusieurs campagnes, plutôt que de devoir consulter chaque campagne, lot, relève ou intervention...

 Tâche perdue : une tâche n'a pas pu se faire
 la raison pour laquelle elle n'a pas pu se faire y est indiquée
 à la différence d'un EDP où s'il y a un problème à la création, rien n'est créé, pour une tâche on essaie toujours de la créer et si ça ne marche pas elle va dans une sorte de "poubelle" (=> les tâches perdues)

 Nature de tâche
 => nature du travail à faire (avec des durées, des échéances, ...)
 paramétrée sur une étape workflow, en fonction du paramètre entreprise LDG
 on y affecte un type de rôle
 liée à un type d'anomalie, plutôt dans les campagne classiques : soit on l'affecte dans une grille via la nature sur l'étape, soit sur le type anomalie.

 Périmètres métiers
 sur un portefeuille, sur un modèle de campagne / de lot, ... (d'abord on regarde le lot et ensuite la campagne)
 ex : regarder l'objet traité de l'EDP pour déterminer le portefeuille
 Type de périmètre instancié via le même écran que le type de rôle métier
objectif => créer la tâche au bon endroit (dans le bon groupe, etc.)

 Une seule tâche pour 1 élément de travail : si on essaie de créer 1 tâche sur 1 élt de travail qui a déjà 1 tâche au statut à traiter (= en cours ou à faire) alors celle-ci est clôturée

 Le "faux" concept de tâche bloquante ou non bloquante
 une tâche bloquante est une tâche liée à une anomalie bloquante et idem pour une non bloquante.
 si plusieurs ano bloquantes : utiliser la priorité sur la nature de tâche

 Liens entre tâches et EDP
 une tache bloquante est liée directement à un EDP
 une tâche non bloquante est liée à l'ano non bloquante qui est liée à l'EDP, pour laisser vivre l'EDP en parallèle (et potentiellement avoir une tâche bloquante)

 Quand un edp change d'étape ou ano est levée => la tâche est clôturée automatiquement

 Ecrans
 lien "vérifier la cohérence" sur un périmètre => pour vérifier la cohérence du paramétrage
 bandeau des traitements non affiché si tâche à faire sur un EDP uniquement en consultant l'EDP depuis la tâche
 mes tâches (depuis menu) : pour voir les tâches affectées à l'agent connecté à un instant t
 Poste de travail : avoir le statut actif pour le trouver

 affectation de tâches
 Rôle : gestionnaire => gère un groupe de travail)
 Rôle Superviseur => supervise un rôle métier

 moteur LDG (= moteur des tâches) : spécifique à chaque domaine (ex. notable : intervention)

 Dans un évt d'ano qui arriverait : savoir quel utilisateur a effectué les tâches (ou se mettre en gestionnaire)

 A la création et au placement d'une tâche au bon endroit , le type de rôle par défaut quand on ne trouve pas ce qu'il faut

 Carnets de travail
 plutôt côté domaine Intervention (MSI, CDI, BDA)
 en lien avec mobefluid
 un carnet regroupe des tâches d'intervention ou de relève => matérialiser un regroupement de ses tâches pour une période donnée (comme agrafer un paquet de feuilles)

 type de tâche sur Type de rôle métier sur tâche
 le type de tâche de programmation concerne les carnets de travail

 modèle de carnet

 règle d'affectation sur groupe de travail : "avec ou sans carnet ..." => si pas de carnet :
 avec : crée le carnet automatiquement
 sans : pas créé

 Mobefluid
 la tâche représente la validation du chef et pas le boulot "unitaire" des agents

 Dans le code
 une Navig spécifique (via SPI) dans les différents domaines pour débrancher vers tout "type" d'objet rattaché à une tâche dans la recherche de tâche et dans le zoom d'une tâche (onglet traitement)
 nécessite un type de chargement pour charger cet objet<!!!>category:domaine
 Modèle simplifié 
C'est un modèle simplifié qui n'a pas vocation à remplacer l'AFD.

 Requêtes utiles 
 Purge campagne de construction des courbes pour suppression définitive 
Le commit est fait par les instruction de fin (drop)

define LOT_ID = '30473164049';
 
BEGIN
   EXECUTE immediate 'DROP TABLE suppression_reinit_lot_cc';
EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE != -942 THEN
         RAISE;
      END IF;
END;
/
 
CREATE TABLE suppression_reinit_lot_cc (
	edp_id VARCHAR2(25),
	anomalie_id VARCHAR2(25),
	courbe_id VARCHAR2(25),
	grandeur_id VARCHAR2(25),
	partitionvaleurs_id VARCHAR2(25),
	diagnostic_id VARCHAR2(25),
	valeurcalcul_id VARCHAR2(25),
	anomalievaleurcalcul_id VARCHAR2(25),
	echange_id VARCHAR2(25));
 
INSERT INTO suppression_reinit_lot_cc
SELECT edp.id, ano.id, courbe.id, g.id, pv.id, d.id, val.id, anov.id, pub.id
FROM tlot l
INNER JOIN telementdepopulationwkf edp  ON (edp.lot_id = l.id)
LEFT JOIN tanomalie ano ON (ano.elementpopulation_id = edp.id)
LEFT JOIN tcourbeecore courbe ON (edp.objettraite_id = courbe.id AND objettraite_role = courbe.ROLE)
LEFT JOIN tgrandeur g ON (courbe.id = g.courbe_id)
LEFT JOIN tpartitionvaleurs pv ON (pv.grandeur_id = g.id)
LEFT JOIN tdiagnostic d ON (d.objetdiagnostique_id = g.id)
LEFT JOIN tvaleurcalcul val ON (val.diagnostic_id = d.id)
LEFT JOIN tanomalievaleurcalcul anov ON (anov.valeurcalcul_id = val.id)
LEFT JOIN techange pub ON (pub.objetmaitre_id=courbe.id AND pub.objetmaitre_role=courbe.role)
WHERE l.id = '&LOT_ID' AND l.modeledelot_role='com.efluid.ecore.courbe.workflow.constructeurcourbe.businessobject.ModeleDeLotConstructeurCourbeWorkflow'
UNION ALL
SELECT edp.id, NULL, courbefille.id, g.id, pv.id, NULL, NULL, NULL, NULL
FROM tlot l
JOIN telementdepopulationwkf edp  ON (edp.lot_id = l.id)
JOIN tcourbeecore courbe ON (edp.objettraite_id = courbe.id AND edp.objettraite_role = courbe.ROLE)
JOIN courbeagregee_courbes lienmerefille ON lienmerefille.SOURCE = courbe.id
JOIN tcourbeecore courbefille ON lienmerefille.dest = courbefille.id
LEFT JOIN tgrandeur g ON (courbefille.id = g.courbe_id)
LEFT JOIN tpartitionvaleurs pv ON (pv.grandeur_id = g.id)
WHERE l.id = '&LOT_ID' AND l.modeledelot_role='com.efluid.ecore.courbe.workflow.constructeurcourbe.businessobject.ModeleDeLotConstructeurCourbeWorkflow';
 
DELETE FROM techange pub WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE pub.id = echange_id);
DELETE FROM tanomalievaleurcalcul anov WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE anov.id = anomalievaleurcalcul_id);
DELETE FROM tvaleurcalcul val WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE val.id = valeurcalcul_id);
DELETE FROM tdiagnostic diag WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE diag.id = diagnostic_id);
DELETE FROM tpartitionvaleurs pv WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE pv.id = partitionvaleurs_id);
DELETE FROM tgrandeur g WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE g.id = grandeur_id);
DELETE FROM tcourbeecore courbe WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE courbe.id = courbe_id);
DELETE FROM tanomalie ano WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE ano.id = anomalie_id);
DELETE FROM tedpconstructeurcourbewkf edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE edp.id = edp_id);
DELETE FROM telementdepopulationwkf edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_cc WHERE edp.id = edp_id);
UPDATE tlot l SET l.selectionrealisee= 0, l.STATUT = 0 WHERE l.id = '&LOT_ID';
 
-- le drop fait le commit!
TRUNCATE TABLE suppression_reinit_lot_cc;
DROP TABLE suppression_reinit_lot_cc;

 Purge campagne de BGE pour suppression définitive 

Le commit est fait par les instruction de fin (drop)

script v14
define LOT_ID = '23087009023';
 
BEGIN
   EXECUTE immediate 'DROP TABLE suppression_reinit_lot_bge';
EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE != -942 THEN
         RAISE;
      END IF;
END;
/
 
CREATE TABLE suppression_reinit_lot_bge (
	edp_id VARCHAR2(25),
	anomalie_id VARCHAR2(25),
	courbebge_id VARCHAR2(25),
	grandeur_id VARCHAR2(25),
	partitionvaleurs_id VARCHAR2(25),
	diagnostic_id VARCHAR2(25),
	valeurcalcul_id VARCHAR2(25),
	anomalievaleurcalcul_id VARCHAR2(25),
	echange_id VARCHAR2(25));
 
INSERT INTO suppression_reinit_lot_bge
SELECT edp.id, ano.id, bge.id, g.id, pv.id, d.id, val.id, anov.id, pub.id
FROM tlot l
INNER JOIN telementdepopulationwkf edp  ON (edp.lot_id = l.id)
LEFT JOIN telementdepopulationbgewkf edpbge ON (edp.id = edpbge.id)
LEFT JOIN tanomalie ano ON (ano.elementpopulation_id = edp.id)
LEFT JOIN tcourbeecore bge ON (edp.objettraite_id = bge.id AND objettraite_role = bge.ROLE)
LEFT JOIN tgrandeur g ON (bge.id = g.courbe_id)
LEFT JOIN tpartitionvaleurs pv ON (pv.grandeur_id = g.id)
LEFT JOIN tdiagnostic d ON (d.objetdiagnostique_id = g.id)
LEFT JOIN tvaleurcalcul val ON (val.diagnostic_id = d.id)
LEFT JOIN tanomalievaleurcalcul anov ON (anov.valeurcalcul_id = val.id)
LEFT JOIN techange pub ON (pub.objetmaitre_id=bge.id AND pub.objetmaitre_role='com.hermes.itv.bge.businessobject.CourbeBilanGlobalEnergie')
WHERE l.id = '&LOT_ID' AND l.modeledelot_role='com.hermes.itv.bge.businessobject.ModeleDeLotCalculBilanGlobalEnergieWorkflow'
UNION ALL
SELECT edp.id, NULL, bgefils.id, g.id, pv.id, NULL, NULL, NULL, NULL
FROM tlot l
JOIN telementdepopulationwkf edp  ON (edp.lot_id = l.id)
JOIN telementdepopulationbgewkf edpbge ON (edp.id = edpbge.id)
JOIN tcourbeecore bge ON (edp.objettraite_id = bge.id AND edp.objettraite_role = bge.ROLE)
JOIN courbeagregee_courbes lienperefils ON lienperefils.SOURCE = bge.id
JOIN tcourbeecore bgefils ON lienperefils.dest = bgefils.id
LEFT JOIN tgrandeur g ON (bgefils.id = g.courbe_id)
LEFT JOIN tpartitionvaleurs pv ON (pv.grandeur_id = g.id)
WHERE l.id = '&LOT_ID' AND l.modeledelot_role='com.hermes.itv.bge.businessobject.ModeleDeLotCalculBilanGlobalEnergieWorkflow';
 
DELETE FROM ttracetraitementenmasse trace WHERE EXISTS (
SELECT 1 FROM telementdepopulationwkf edp INNER JOIN tlot l  ON edp.lot_id = l.id
WHERE l.id = '&LOT_ID' and edp.id = trace.EDP_ID);

DELETE FROM techange pub WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE pub.id = echange_id);
DELETE FROM tanomalievaleurcalcul anov WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE anov.id = anomalievaleurcalcul_id);
DELETE FROM tvaleurcalcul val WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE val.id = valeurcalcul_id);
DELETE FROM tdiagnostic diag WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE diag.id = diagnostic_id);
DELETE FROM tpartitionvaleurs pv WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE pv.id = partitionvaleurs_id);
DELETE FROM tgrandeur g WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE g.id = grandeur_id);
DELETE FROM tcourbeecore bge WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE bge.id = courbebge_id);
DELETE FROM tanomalie ano WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE ano.id = anomalie_id);
DELETE FROM telementdepopulationbgewkf edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE edp.id = edp_id);
DELETE FROM telementdepopulationwkf edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE edp.id = edp_id);
UPDATE tlot l SET l.selectionrealisee= 0 WHERE l.id = '&LOT_ID'; 

-- le drop fait le commit!
TRUNCATE TABLE suppression_reinit_lot_bge;
DROP TABLE suppression_reinit_lot_bge;

script v13

-- id du lot à traiter
define LOT_ID = '951689321';
 
BEGIN
   EXECUTE immediate 'DROP TABLE suppression_reinit_lot_bge';
EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE != -942 THEN
         RAISE;
      END IF;
END;
/
 
CREATE TABLE suppression_reinit_lot_bge (
	edp_id VARCHAR2(25),
	anomalie_id VARCHAR2(25),
	courbebge_id VARCHAR2(25),
	grandeur_id VARCHAR2(25),
	partitionvaleurs_id VARCHAR2(25),
	diagnostic_id VARCHAR2(25),
	valeurcalcul_id VARCHAR2(25),
	anomalievaleurcalcul_id VARCHAR2(25),
	echange_id VARCHAR2(25),
	trace_id VARCHAR2(25));
 
INSERT INTO suppression_reinit_lot_bge
SELECT edp.id, ano.id, bge.id, g.id, pv.id, d.id, val.id, anov.id, pub.id, edpbge.tracecalcul_id
FROM tlot l
INNER JOIN telementdepopulationwkf edp  ON (edp.lot_id = l.id)
LEFT JOIN telementdepopulationbgewkf edpbge ON (edp.id = edpbge.id)
LEFT JOIN tanomalie ano ON (ano.elementpopulation_id = edp.id)
LEFT JOIN tcourbeecore bge ON (edp.objettraite_id = bge.id AND objettraite_role = bge.ROLE)
LEFT JOIN tgrandeur g ON (bge.id = g.courbe_id)
LEFT JOIN tpartitionvaleurs pv ON (pv.grandeur_id = g.id)
LEFT JOIN tdiagnostic d ON (d.objetdiagnostique_id = bge.id AND d.objetdiagnostique_role = bge.ROLE)
LEFT JOIN tvaleurcalcul val ON (val.diagnostic_id = d.id)
LEFT JOIN tanomalievaleurcalcul anov ON (anov.valeurcalcul_id = val.id)
LEFT JOIN techange pub ON (pub.objetmaitre_id=bge.id AND pub.objetmaitre_role='com.hermes.itv.bge.businessobject.CourbeBilanGlobalEnergie')
WHERE l.id = '&LOT_ID' AND l.modeledelot_role='com.hermes.itv.bge.businessobject.ModeleDeLotCalculBilanGlobalEnergieWorkflow'
UNION ALL
SELECT edp.id, null, bgefils.id, g.id, pv.id, null, null, null, null, null
FROM tlot l
JOIN telementdepopulationwkf edp  ON (edp.lot_id = l.id)
JOIN telementdepopulationbgewkf edpbge ON (edp.id = edpbge.id)
JOIN tcourbeecore bge ON (edp.objettraite_id = bge.id AND edp.objettraite_role = bge.ROLE)
JOIN courbeagregee_courbes lienperefils on lienperefils.source = bge.id
JOIN tcourbeecore bgefils ON lienperefils.dest = bgefils.id
LEFT JOIN tgrandeur g ON (bgefils.id = g.courbe_id)
LEFT JOIN tpartitionvaleurs pv ON (pv.grandeur_id = g.id)
WHERE l.id = '&LOT_ID' AND l.modeledelot_role='com.hermes.itv.bge.businessobject.ModeleDeLotCalculBilanGlobalEnergieWorkflow';
 
DELETE FROM techange pub WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE pub.id = echange_id);
DELETE FROM tanomalievaleurcalcul anov WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE anov.id = anomalievaleurcalcul_id);
DELETE FROM tvaleurcalcul val WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE val.id = valeurcalcul_id);
DELETE FROM tdiagnostic diag WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE diag.id = diagnostic_id);
DELETE FROM tpartitionvaleurs pv WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE pv.id = partitionvaleurs_id);
DELETE FROM tgrandeur g WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE g.id = grandeur_id);
DELETE FROM tcourbeecore bge WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE bge.id = courbebge_id);
DELETE FROM tanomalie ano WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE ano.id = anomalie_id);
DELETE FROM telementdepopulationbgewkf edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE edp.id = edp_id);
DELETE FROM telementdepopulationwkf edp WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE edp.id = edp_id);
DELETE FROM ttracetraitementenmasse trace WHERE EXISTS (SELECT 1 FROM suppression_reinit_lot_bge WHERE trace.id = trace_id);
 
-- le drop fait le commit!
TRUNCATE TABLE suppression_reinit_lot_bge;
DROP TABLE suppression_reinit_lot_bge;

 Besoin fonctionnel 
Un BGE (bilan global énergie) est une courbe qui représente une énergie ou une puissance calculée à partir de données réelles ou prévisionnelles. Ces courbes sont calculées à partir d’un workflow.

On a 3 grands processus :
 Enercom 
Ici le but est de faire des calcul prévisionnel d’énergie (à court terme ou long terme) à renvoyer vers Enercom pour un fournisseur donné, pour qu’Enercom puisse gérer les achat d’énergie. On calcul donc des énergies prévisionnelles (dans le futur), quotidiennement pour publier des données tous les jours.

 Reconstitution des flux 
Le GRD doit faire des calculs de reconstitution des flux sur sa concession. Cela consiste à attribuer, semaine par semaine, le total des consommations ou production de chaque responsable d’équilibre. Ces calculs se font a posteriori (sur une période passée). Pour cela on relève les courbes de charge ou on estime des courbes à partir des relèves (profilage) pour tous les clients au pas de temps donné (30 minutes). L’objectif est de remonter ces infos vers RTE pour qu’il agrège les données au niveau national. Cela a pour but d’inciter les responsable d’équilibre à équilibrer la production et la consommation, puisqu’ils sont pénalisés s’ils ne sont pas à l’équilibre.
Il faut donc pour chaque responsable d’équilibre (RE) attribut la somme des énergies :
	Consommées télérelevées
	Consommées profilées
	Produites télérelevées
	Produites profilées
	Energie des pertes (techniques et non techniques, calculées à partir de l’énergie totale sur le réseau, et attribué à un responsable d’équilibre désigné par le GRD).
 Capacité 
Similaire à la reconstitution des flux, l’objectif est ici de s’intéresser à la puissance appelée en pointe. L’acteur responsable des capacités s’appelle un acteur obligé (chaque fournisseur désigne son acteur obligé) pour lequel on doit calculer une puissance de référence. Cela obligera les acteurs obligés à prouver qu’il aurait été en mesure de fournir suffisamment d’énergie en cas d’hiver rigoureux. (par achat de certificat de capacité, ou en certifiant ses moyen de production, ou en justifiant de capacité d’effacement).
	
Les calculs se font sur une période d’une année de livraison (année civile, AL - 1 ou AL - 2). Après avoir calculé la consommation totale et réelle des clients d’un acteur obligé, on estime la consommation qui aurait eu lieu en cas de température extrême (hiver très froid), et on ne garde que certains jours particuliers (PP1 choisis par RTE). Cela permet de calculer la puissance de référence pour l'acteur obligé.

 Utilisation dans efluid 
Pour paramétrer une campagne BGE, il faut avoir en tête que c’est un workflow récurrent. Il est donc nécessaire de créer un planning et de déclencher les échanges pour créer les campagnes.

En créant un modèle de lot, on définit :
 Le type de processus concernée
 Les paramètres concernant la période à calculer.
 Le type d’objet connexe :
Pour Enercom, on définit sur le modèle de lot la liste exhaustive des modeleBGE à utiliser. 
Le batch de sélection (BGE001MT) créera un EDP et un BGE sera cré par modèle BGE.
Pour les autres traitements, on définit un type d’objet connexe (acteur obligé, RE, fournisseur), une concession, et un unique modèle BGE.
Le batch de sélection sélectionnera tous les PDS actifs de la concession, identifiera les acteurs actifs et créera pour chacun un EDP, rattaché à l’acteur connexe, et créer une grappe de BGE/courbe agrégée à l’image de la grappe de MBGE/MCC rattachée au modèle de lot.  Certains BGE de la grappe ne seront créés que sous certaines conditions (condition d’existence portée par MBGE)

 Prérequis 

Pour les traitements prévisionnels, il faudra notamment :
 pour les courbes de charger, passer le batch PRJ001MT pour calculer les courbe d'énergie prévisionnelles. (utilisation d'un calendrier projection).
 pour les profilés, avoir sur la station météo les températures normales, la température prévisionnelle (si prévision court terme), les courbes de sous-profil brut préparé, les grandients de température préparé, et les sous-profil ajusté prévisionnels.  (calculés par worfklow de gestion des données climatiques GDC). Les profilés utilisent les consommations mensuelles (traitement H) pour définir une énergie, et utilise ces coefficient pour faire le profilage.

Pour les traitements se basant sur les données réel, il faut :
 les courbes de charge réelles sur les PDS actifs. (télérelève, import de courbe)
 pour les profilés, les courbes de températures normales et réelles, les gradients ajustés réels et les sous-profil ajustés réelles (calculée par workflow GDC). On exploite également les consommations des relèves validées sur les PDS.<!!!>REDIRECTION edoc
Category:domaine<!!!>Category:domaineCategory:composant sw-jmsCategory:pole composants transverses
Web Service
 liens 
 interne 
 CR des points d'avancement nouvelle archi des services web<!!!>category:domainecategory:requêteur
Le domaine requêteur est composé de 2 types d’évènements géré par des domaines différents :
 Le domaine composant requêteur : pour assurer la maintenance du composant
 Et le domaine paramétrage requêteur : pour traiter le paramétrage du requêteur dans l’application efluid
(Pour les autres applications : suivefluid, ethaque, eldap, ... ce paramétrage est assuré directement par le responsable de chaque application)<!!!>category:domainecategory:requêteur
Cette page rassemble les informations générales concernant le paramétrage du requêteur pour l’application efluid. Sa maintenance est assurée par chaque domaine, en fonction de la vue principale.
(Pour les autres applications : suivefluid, ethaque, eldap, ... ce paramétrage est assuré directement par le responsable de chaque application).

 Correspondance Vue principale / Domaine en charge de la maintenance 
 Vue principale  Domaine  Développeur acteur (VRP_Acteur.xml)  ref.acteur  Anthony BAINVILLE (ABAI) action (VRP_Action.xml)  ref.affaireaction  Anthony BAINVILLE (ABAI) affaire (VRP_Affaire.xml)  ref.affaireaction  Anthony BAINVILLE (ABAI) compte client (VRP_CompteClient.xml)  rec.comptabilite  Lionel PRYBYLA (LPR) contrat (VRP_Contrat.xml)  crm.contrat  Anthony BAINVILLE (ABAI) courbe (VRP_Courbe.xml) (ecore)  ecore.courbe  demande prestation (VRP_DemandePrestation.xml)  crm.demandeprestation  Anthony BAINVILLE (ABAI) devis (VRP_Devis.xml)  fac.devis  Alexandre HAFFNER (AHA) / Alexandre THIBAULT (ATH) écriture (VRP_Ecriture.xml)  rec.interfacecomptable  Anthony BAINVILLE (ABAI) espace de livraison (VRP_EspaceDeLivraison.xml)  ref.edl  Anthony BAINVILLE (ABAI) facture (VRP_Facture.xml)  fac.facturation  Alexandre HAFFNER (AHA) / Alexandre THIBAULT (ATH) facture principale (VRP_FacturePrincipale.xml)  fac.facturation  Alexandre HAFFNER (AHA) / Alexandre THIBAULT (ATH) matériel (VRP_Materiel.xml)  ref.materiel  Mathieu GULDNER (MGU) / Brice DAMESTOY (BDA) modèle bilan global énergie (VRP_ModeleBilanGlobalEnergie.xml)  itv.bge  Mathieu GULDNER (MGU) / Brice DAMESTOY (BDA) offre produit (VRP_OffreProduit.xml)  ofp.offre  Alexandre HAFFNER (AHA) / Alexandre THIBAULT (ATH) opération MDE (VRP_OperationMDE.xml)  ref.affairemde  Anthony BAINVILLE (ABAI) point de service (VRS_PointDeService.xml)  ref.edl  Anthony BAINVILLE (ABAI) point éclairage public (VRP_PointEclairagePublic.xml)  ref.edl  Anthony BAINVILLE (ABAI) relève (VRP_Relève.xml)  itv.conso  Patrice FRANTZ (PFR) simulation tarifaire (VRP_SimulationTarifaire.xml)  ecore.simulation  Alexandre HAFFNER (AHA) / Alexandre THIBAULT (ATH) suivi connexion portail (VRP_SuiviConnexionPortail.xml)  ref.acteur  Anthony BAINVILLE (ABAI)

 Ressources nécessaires 

 Fichiers DDL de création/modification de vue 
Ils se situent dans le dossier "sql > database > elfuid > ddl > create > view > requeteur > [domaine] > [sousDomaine]".
Pour chaque fichier "VRP/S_NOMDELAVUE.ddl" de création à cet emplacement, il faut en correspondance un fichier "VRP/S_NOMDELAVUE_drop.ddl" de suppression dans le dossier "sql > database > elfuid > ddl > drop > view > requeteur > [domaine] > [sousDomaine]". Ce dernier ne contient que la commande "drop view VRP/S_NOMDELAVUE;"
Si une vue est modifiée via SQLmigrator (voir plus loin), le fichier de création de la vue doit être modifié également.

 Fichiers XML de paramétrage 
Ils se situent dans le dossier "src > main > resources > vuesRequeteur".

 Fichiers XML/SQL de modification de vue 
La modification d'une vue du requêteur (ajout, modification ou suppression d'un attribut) se fait par SQLmigrator, donc via un fichier changeLog.xml. Ce dernier pointera sur un fichier SQL contenant le code pour la création de la vue modifiée.
Les deux fichiers se trouvent dans le dossier "sql > database > elfuid > ddl > upgrade > [versionPrincipale] > changelog > view > requeteur > [domaine] > [sousDomaine]".
Il est important de vérifier que le fichier changeLog.xml de ce sous-dossier apparaît bien dans le fichier "sql/database/efluid/ddl/upgrade/[versionPrincipale]/changeLog.xml", et qu'il se situe à la fin (après les fichier changeLog.xml des domaines autres que le requêteur).

 Principe 
A chaque élément disponible dans la liste déroulante "vue" de l'écran de recherche de requête correspond une vue principale. Cette vue principale est liée à une ou plusieurs vue(s) secondaire(s), disponible(s) dans les onglets "colonnes" et "critères" des écrans de consultation/modification de requête. Une vue secondaire peut avoir elle-même une ou plusieurs vue(s) secondaire(s).
Le paramétrage d'une vue se fait par 2 fichiers :
 
VRP_NOMDELAVUEPRINCIPALE.ddl (ou VRP_NOMDELAVUEPRINCIPALES.ddl)
VRP_NomDeLaVuePrincipale.xml (ou VRS_NomDeLaVueSecondaire.xml)

 Fichier DDL de création/modification de vue 
Il contient la requête de création de la vue. Cette dernière remonte tous les attributs nécessaires à l'affichage des éléments dans les onglets "colonnes" et "critères", ainsi que ceux servant au jointures (par exemple REFERENCE_ID) ou au fonctionnement du requêteur (par exemple ID, CODEGRD, CODEFOURNISSEUR).
Il est nommé VRP_ (pour une vue principale) et VRS_ (pour une vue secondaire) en majuscules, suivie d'un nom explicite et unique en majuscule. Idéalement, il porte le même nom que le fichier XML de paramétrage correspondant.
Ci-dessous, un exemple de fichier DDL :
create or replace view VRP_NOMDELAVUE as (
  select A.ID,
         A.CODEGRD,
         A.CODEFOURNISSEUR,
         A.ATTRIBUT1,
         A.ATTRIBUT2,
         A.NOMDUNATTRIBUT as ATTRIBUT3,
         B.ATTRIBUT1 as ATTRIBUTDEB,
         ...
    from TTABLE A
      left join TAUTRETABLE B on B.ID = A.B_ID
    where ...
);

 Fichier XML de paramétrage 
Le fichier XML décrit la structure de la vue, les caractéristiques générales (habilitations, lien vers la vue SQL, lien vers l'objet JAVA, attributs disponibles pour la clause SELECT et pour la clause WHERE, etc.).
Il sert à générer les pages IHM, soit les onglets "colonne" et "critère" de la page de zoom des requêtes, et à lier entre elles les vues SQL par des jointures.
Il est nommé VRP_ (pour une vue principale) et VRS_ (pour une vue secondaire) en majuscules, suivie d'un nom explicite et unique en "camelCase".
Plusieurs fichiers XML de paramétrage peuvent être liés à un même fichier DDL de vue.
Ci-dessous, un exemple de fichier XML.
<?xml version="1.0" encoding="iso-8859-15"?>
<vue id="VuePrincipale1" >
  <libelle>vue principale 1</libelle>
  <nomSQL>VRP_VUEPRINCIPALE</nomSQL>
  <clePrimaire>ID</clePrimaire>
  <typeJava>com.hermes.[domaine].[sousDomaine].businessobject.MonObject</typeJava>
  <colonneRole>ROLE</colonneRole>
  <selectionURL>[sousDomaine].RechercherMonObjet</selectionURL>
  <confidentialite>true</confidentialite>
  <risqueModifier>[codeRisque]</risqueModifier>
  <risqueExecuter>[codeRisque]</risqueExecuter>
  <colonnesSelectionnables>
    <colonne id="attribut1">
      <libelle>premier attribut</libelle>
      <nomSQL>ATTR1</nomSQL>
      <typeJava>[type]</typeJava>
      <obligatoire>true</obligatoire>
      <affichageObligatoire>true</affichageObligatoire>
    </colonne>
    <colonne id="attribut2">
      <libelle>deuxième attribut</libelle>
      <nomSQL>ATTR2</nomSQL>
      <typeJava>[type]</typeJava>
    </colonne>
    ...
  </colonnesSelectionnables>
  <criteresPossibles>
    <critere id="attribut1">
      <libelle>premier attribut</libelle>
      <nomSQL>ATTR1</nomSQL>
      <typeJava>[type]</typeJava>
    </critere>
    <critere id="attribut2">
      <libelle>deuxième attribut</libelle>
      <nomSQL>ATTR2</nomSQL>
      <typeJava>[type]</typeJava>
      <rechercheValeurColonne>true</rechercheValeurColonne>
    </critere>
    ...
  </criteresPossibles>
  <jointuresPossibles>
    <jointure id="vueSecondaire1" >
      <libelle>première vue secondaire</libelle>
      <dest>VRS_PremiereVueSecondaire</dest>
      <typeJointure>0-1</typeJointure>
      <colonnesSource>REFERENCE_ID</colonnesSource>
      <jointureExterne>true</jointureExterne>
    </jointure>
    <jointure id="vueSecondaire2" >
      <libelle>deuxième vue secondaire</libelle>
      <dest>VRS_DeuxiemeVueSecondaire</dest>
      <typeJointure>1-N</typeJointure>
      <colonnesSource>REFERENCE_ID</colonnesSource>
      <jointureExterne>false</jointureExterne>
    </jointure>
    <jointure id="vueSecondaire3" >
      <libelle>troisièmevue secondaire</libelle>
      <dest>VRS_TroisiemeVueSecondaire</dest>
      <typeJointure>0-N</typeJointure>
      <tableLien id="objet1_objets2">
        <nomSQL>OBJET1_OBJETS2</nomSQL>
        <colonnesSource>DEST</colonnesSource>
        <colonnesDest>SOURCE</colonnesDest>
      </tableLien>
      <jointureExterne>false</jointureExterne>
    </jointure>
    ...
  </jointuresPossibles>
</vue>

 Entête 
Dans /efluid/xml/VRP_Acteur.xml et /efluid/xml/VRP_Affaire.xml.
<vue id="affaire" >                                                        <!-- L'ID de la vue, doit être unique -->
  <!-- ... -->
  <libelle>affaire</libelle>                                               <!-- Le libelle tel qu'il apparaîtra dans les IHM -->
  <nomSQL>VRP_AFFAIRE</nomSQL>                                             <!-- La vue SQL correspondante -->
  <description>affaire</description>                                       <!-- La description telle qu'elle apparaîtra dans le tooltip de l'IHM -->
  <colonneRole>ROLE</colonneRole>                                          <!-- La colonne role dans le cas d'un objet abstrait -->
  <clePrimaire>ID</clePrimaire>                                            <!-- La clef primaire -->
  <typeJava>com.hermes.ref.affaireaction.businessobject.Affaire</typeJava> <!-- Le type Java, abstrait si nécessaire, dans quel cas l'attribut "colonneRole" doit être renseigné -->
  <selectionURL>affaireaction.RechercherAffaire</selectionURL>             <!-- L'URL de recherche, afin de pouvoir "zoomer" sur les objets retournés -->
  <risqueModifier>246</risqueModifier>                                     <!-- Le risque de modifications des requêtes sur cette vue -->
  <risqueExecuter>247</risqueExecuter>                                     <!-- Le risque d'exécution des requêtes sur cette vue -->
  <confidentialite>false</confidentialite>                                 <!-- Activation de la confidentialité sur cette vue (ajoute les clauses par défaut) par défaut à "true" -->
  <suppressionLogique>true</suppressionLogique>                            <!-- Activation de la vérification de la suppression logique sur cette vue (ajoute les clauses par défaut) par défaut à "true" -->
  <!-- ... -->
</vue>

 Colonne 
Dans /efluid/xml/VRP_Acteur.xml.
    <colonne id="intitule" >                                     <!-- L'ID pour l'IHM, doit être unique -->
      <libelle>intitulé</libelle>                                <!-- Le libelle pour l'IHM -->
      <nomSQL>INTITULE</nomSQL>                                  <!-- Nom de la colonne dans la vue -->
      <typeJava>com.hermes.ref.acteur.type.EPIntitule</typeJava> <!-- Type de donnée pour l'IHM, soit (I) type Java (II) nombre (III) string (IV) date (V) datetime -->
      <obligatoire>true</obligatoire>                            <!-- Sélection obligatoire ou non (si oui se retrouve automatiquement à droite dans l'IHM et impossible d'en changer) -->
      <affichageObligatoire>true</affichageObligatoire>          <!-- Comme sélection obligatoire mais pour l'affichage (se retrouve dans l'IHM résultat / l'export Excel) -->
    </colonne>

 Critères 
Dans /efluid/xml/VRP_Acteur.xml.
    <critere id="intitule" >                                      <!-- L'ID du critère, doit être unique -->
      <libelle>intitulé</libelle>                                 <!-- Le libelle qui apparaîtra dans l'IHM -->
      <nomSQL>INTITULE</nomSQL>                                   <!-- La colonne dans la vue SQL -->
      <typeJava>com.hermes.ref.acteur.type.EPIntitule</typeJava>  <!-- Le type pour l'IHM, voir colonne -->
    </critere>

 Jointures 
Les jointures apparaissent dans l'IHM comme des sous-niveaux du tableau hiérarchique. En général, les jointures doivent être déclarées comme "externes" dans le cas où l'objet lié est facultatif. NB : Il existe une limitation du requêteur qui ne permet pas de faire une jointure externe sur la vue courante.

 0-1 
Ce type de jointure correspond aux cas où la clef étrangère est dans la table correspondante à la vue courante. Attention à la jointure externe qui varie en fonction de la nécessité de l'objet lié. De /xml/VRP_Acteur.xml :
    <jointure id="adresse">                               <!-- L'ID pour l'IHM -->
      <libelle>adresse de correspondance</libelle>        <!-- Le libelle qui apparaît dans le sous-niveau du tableau hiérarchique de l'IHM -->
      <dest>VRS_AdressePostale</dest>                     <!-- Le nom de la vue XML pour afficher les éléments dans le sous-tableau -->
      <typeJointure>0-1</typeJointure>                    <!-- Le type de jointure -->
      <colonnesSource>ADRESSEPOSTALE_ID</colonnesSource>  <!-- La clef étrangère dans la table de la vue courante -->
      <jointureExterne>true</jointureExterne>             <!-- Vrai pour la jointure externe -->
    </jointure>

 1-N 
Ce type de jointure correspond aux cas où la clef étrangère est dans la table correspondante à la vue liée. Attention à la jointure externe qui varie en fonction de la nécessité de l'objet lié, donc la vue courante (ici, la majorité du temps, la jointure est externe !). De /xml/VRP_Acteur.xml :
    <jointure id="affaireResponsable">
      <libelle>affaire en tant que responsable</libelle>
      <dest>VRP_Affaire_JExt</dest>
      <typeJointure>1-N</typeJointure>
      <colonnesSource>RESPONSABLE_ACTEUR_ID</colonnesSource> <!-- La clef étrangère dans la table de la vue liée -->
      <jointureExterne>true</jointureExterne>                <!-- Vrai pour la jointure externe (la grande majorité du temps vrai) -->
    </jointure>

 0-N 
Ce type de jointure correspond aux cas où la clef étrangère est dans la table de lien. Attention à la jointure externe qui varie en fonction de la nécessité de l'objet lié, donc la vue courante (ici, la majorité du temps, la jointure est externe !). De /xml/VRP_Contrat.xml :
    <jointure id="espaceDeLivraison" >
      <libelle>espace de livraison</libelle>
      <libelleCourt>edl</libelleCourt>              <!-- Je sais pas à quoi ça sert -->
      <dest>VRP_EspaceDeLivraison_JExt</dest>       <!-- Le nom de la vue XML pour afficher les éléments dans le sous-tableau, voir "nomSQL" en bas pour la vue SQL -->
      <typeJointure>0-N</typeJointure>
      <tableLien id="contrat_edl">                  <!-- L'ID de la table de lien pour l'IHM, ne sert pas à grand-chose -->
        <nomSQL>CONTRAT_ESPACESDELIVRAISON</nomSQL> <!-- Le nom de la vue SQL qui correspond à la table de lien -->
        <colonnesSource>SOURCE</colonnesSource>     <!-- La colonne source dans la table de lien -->
        <colonnesDest>DEST</colonnesDest>           <!-- La colonne destination dans la table de lien -->
      </tableLien>
    </jointure>

 Types Java 
Les types Java sont soit le nom complet de la classe (pour les énumérés en particulier), soit ces valeurs spéciales :
 string : chaîne de caractères
 booleen : booléen
 entier : nombre entier
 nombre : nombre réel
 date : date (sans précision de l’heure)
 dateTime : date avec la précision sur l’heure
 ListEnum : liste d’énuméré

 Stockage en base de données 
Les éléments d'une requête sont stockés dans 3 tables :
TREQUETE
TCOLONNESELECTIONNEE
TREQUETEELEMENTCLAUSEWHERE
TREQUETE contient les attributs de l'objet principal Requete : paramètres visibles dans le bandeau haut, dans l'onglet "généralités", dans l'onglet "complément", etc.
TCOLONNESELECTIONNEE contient, par requête, tous les éléments sélectionnés dans l'onglet "colonne".
TREQUETEELEMENTCLAUSEWHERE contient, par requête, tous les critères sélectionnés dans l'onglet "critère", avec leurs valeurs.

 Opérations de base 
 Ajout d'un attribut 
Ajout de l'attribut à la vue DDL
Modifier du fichier sql > database > sql > database > efluid > ddl > create > view > requeteur > [domaine] > [sousDomaine] > VR[P-S]_NOMDELAVUE.ddl
Créer le fichier sql > database > efluid > ddl > upgrade > [version] > changelog > view > requeteur > [domaine] > [sousDomaine] > T_VR[P-S]_NOMDELAVUE_REPLACE_[numEvt].sql
Modifier le fichier sql > database > efluid > ddl > upgrade > [version] > changelog > view > requeteur > [domaine] > [sousDomaine] > changeLog.xml
Vérifier l'existence de la référence au changeLog ci-dessus en fin de fichier sql>database>efluid>ddl>upgrade>[version]>changelog>changeLog.xml
Ajout de l'attribut au fichier XML de paramétrage
Modifier le(s) fichier(s) src > main > ressources > vuesRequeteur > VR[P-S]_NomDeLaVue.xml

 Modification d'un attribut 
Les opérations à réaliser sont les mêmes que pour l'ajout d'un attribut si la vue (DDL) est impactée.
Sinon, modifier uniquement le fichier XML de paramétrage (correction d'un libellé par exemple).
ATTENTION : un script de migration est nécessaire si l'ID de l'attribut dans le fichier XML de paramétrage est modifié, car il correspond à la colonne REF_PARAMETRAGE des tables TCOLONNESELECTIONNEE et TREQUETEELEMENTCLAUSEWHERE.

 Suppression d'un attribut 
Les opérations à réaliser sont les mêmes que pour l'ajout d'un attribut.
ATTENTION : un script de migration est nécessaire pour supprimer toutes les références à l'attribut supprimé dans la colonne REF_PARAMETRAGE des tables TCOLONNESELECTIONNEE et TREQUETEELEMENTCLAUSEWHERE.

 Ajout d'une vue secondaire 
Créer la vue DDL
Créer le fichier sql > database > efluid > ddl > create > view > requeteur > [domaine] >  [sousDomaine] > VRS_NOMDELAVUE.ddl
Créer le fichier sql > database > efluid > ddl > upgrade > [version] > changelog > view > requeteur > [domaine] > [sousDomaine] > T_VRS_NOMDELAVUE_CREATE_[numEvt].sql
Ajouter un changeSet au fichier : sql > database > efluid > ddl > upgrade > [version] > changelog > view > requeteur > [domaine] > [sousDomaine] > changeLog.xml
Vérifier l’existence de la référence au changeLog ci-dessous à la fin du fichier : sql > database >   efluid > ddl > upgrade > [version] > changelog > changeLog.xml
Créer le fichier XML de paramétrage
Créer le fichier de paramétrage : src > main > ressources > vuesRequeteur > VRS_NomDeLaVue.xml
Ajouter la jointure vers la nouvelle vue dans le(s) fichier(s) XML de paramétrage

 Ajout d'une vue principale 
Créer la vue DDL et le fichier XML de paramétrage
Idem vue secondaire, mais avec préfix VRP
Créer éventuellement les codes risque et habilitations nécessaires
Demander un découpage technique
Modifier framework.properties 
Ajouter la référence à la nouvelle vue principale pour l’attribut REQUETEUR_VUES_POSSIBLES

 Documentation 
 Guide utilisateur requêteur : efluid - Documentation Fonctionnelle Suite efluid > Guides Utilisateur > guides de paramétrage > V13 à rédiger > Guide utilisateur - domaines requêteur et exécutions différées.docx

 AFD Requêteur : efluid - Documentation Fonctionnelle Suite efluid > AFD suite efluid > AFD efluid > REQ - Requêteur > AFD > AFD - REQ - Requeteur.doc

 Description des fichiers XML (paramétrage existant) : efluid - Documentation Fonctionnelle Suite efluid > AFD suite efluid > AFD efluid > REQ - Requêteur > AFD > paramétrageRequeteur vXX_X.xls
Les descriptions des fichiers XML (paramétrage existant) sont tenus à jour par l'équipe expertise/recette (Valérie BECKER - VBE).<!!!>Category:domaine
Category:domaine service web SGE

 Présentation 

Présentation des services web SGE

 Liste des services 
 Domaine services web SGE 
 SGE01 : Créer une demande de prestation
 SGE02 : Modifier une demande de prestation
 SGE03 : Annuler une demande de prestation
 SGE05a : Consommer avancement affaire (création d'une action de suivie sur l'EDL d'une affaire SGE dans Efluid)
 SGE06 : Relance distributeur (anomalie sur edp)
 SGE07 (sortant - appel à un WS externe) : Demande de référence externe à SGE d'une affaire
 SGE08 : Renvoie toutes les informations du PDS (getInfo)
 SGE012 (SGE01a) : Récupération des périodes d'historiques de mesure disponible
 KGO01 : Créer ou modifier acteur + rattacher au PASC (SMO)
 KGO02 : Consulter client + PASC (SMO)
 KGO03 : Consulter services SMO et les traces de mesure du point (SMO)

 Domaine échange 
 SGE04 : Flux d'avancement d'affaire (Jalon)
 Domaine consommation 
 SGE05b : Intégrer notification de déclenchement d'ordre de pointe mobile

 Domaine référentiel 
 PRM01 : Création de point
 PRM02 : Raccordement/PMES - Création de branchement
 PRM03 (ex SGE08) : Consultation Mesures et plages Heure Creuse

 Liste des services web exposés (WSDL) 
http://localhost:8080/efluid/services-sge/

 Matrice des versions de services 
Matrice des versions de services web SGE

 Appeler un service SOAP SGE / Soap over JMS 
 Fichier de paramétrage 
 EDK2.properties
 INTERFACE_SGE_INBOUND_ACTIVE=true
 INTERFACE_SGE_OUTBOUND_ACTIVE=true
 NOEUD_WS_URL=http://localhost:8080/efluid
 framework2.properties
 webservices.security.status=NO_SECURITY
 JMS_ACTIVER=true
 INTIAL_CONTEXT_FACTORY=org.jnp.interfaces.NamingContextFactory
 PROVIDER_URL=jnp://localhost:16401
 hermes2.properties
 EMBEDDED_JNDI_PORT=16401
 EMBEDDED_JNDI_RMI_PORT=16402
 EMBEDDED_JNDI_BIND_ADDRESS=localhost
 EMBEDDED_ACTIVER_JORAM=true
 EMBEDDED_ACTIVER_MESSAGE_LISTENER=true
 EMBEDDED_JMS_BIND_ADDRESS=localhost
 EMBEDDED_JMS_PORT=16017
 #### AVANT 14.6 UNIQUEMENT ####
 JMS_ACCESS_LOGIN=sgeUser
 #JMS_ACCESS_PASSWORD=sgePassword$123
 JMS_ACCESS_PASSWORD=GINKO2014

 Entete SOAP
 Header = Echange-ID
 Value = NO-CHECK-DOUBLONS

 Authentification après 14.6 
Encoder USER:PASSWORD (sgeUser:sgePassword) via https://www.base64encode.org/ : c2dlVXNlcjpzZ2VQYXNzd29yZA== 
Puis le transmettre via le header du flux :
 Clé : Authorization
 Valeur : Basic c2dlVXNlcjpzZ2VQYXNzd29yZA==
Fichier:header_authorization_sge.png

 Appel du service 
 Démarrer le serveur d'application
 Lister tous les services disponibles : http://localhost:8080/efluid/services-sge/
 Clic droit copier le raccourci
Fichier:services_sge.jpeg
 Importer le WSDL dans SOAPUI
 Copier le raccourci dans "Initial WSDL" puis OK.
Fichier:WSDL_SOAPUI.jpeg
 Importer le flux dans la request du service SOAP
 Ne pas prendre PortBindingSOAPJMS mais PortBinding (WS sans file)
 ALT + F et ALT + V (pour formater et valider le flux)
 Flèche verte pour exécuter la requête
Fichier:request_SOAPUI.jpeg

 Documents conceptuels techniques 
 http://wperoom2.uem.lan/eRoom/Production/DocTechniqueEfluid/0_e8242
 Développer un service web 
 Générer WebService à partir du wsdl et xsd [EDK] 
 Dans l'AFD interface récupérer le zip des XSD
 Copier dans les projets edk/ws/wsdl (les modifications ou la totalité)
 Faire un draft pour vérifier les modifications
 Ajouter si besoin dans le fichier edk/ws/codegen/cxf/pom.xml la conversion de package pour une nouvelle opération ou service

 Développer service web [Efluid] 
 Ajouter le service dans app/src/main/resources/webServices.xml s'il n'existe pas
 Compléter ou créer le WSMgr (exemple : CalculerRecevabiliteDemandePrestationWSMgr)
Lier le WsMgr avec le fichier wsdl qui se trouve dans le projet edk-ws-wsdl-serveur :
 @WebResult (Retour du WS)
 name = "calculerRecevabiliteDemandePrestationModificationMesures" : nom de l'attribut OUTPUT
 targetNamespace = "http://www.erdf.fr/ginko/calculerrecevabilitemodificationmesures/v3" : namespace de l'attribut OUTPUT
 Idem pour @WebParam mais INPUT
 Créer un mappeur pour la "demande" SGE0xa, la "réponse" SGE0xa et la "demande" SGE0xb
 Référencer chaque mappeur dans MapperFactory qui hérite de GenericMapperFactory
 Créer un test TestMapperFactory qui hérite de TestGenericMapperFactory (cela permet de savoir ce qui n'est pas mappé)
 Pour tester chaque mappeur il faut : 
 Un test qui hérite de TestSGEMapper
 Un Builder pour générer les objets type (objets du WS)
 Une condition qui hérite de AbstractCondition qui permet de tester le mapping

 Générer un contexte depuis un test d'intégration 
 Générer une base from scratch perso => http://usinelogicielle/job/FsuiteEfluid/job/Fefluid/job/efluid.drop-create-bdd-via-sqlmigrator-dynamic/
 Configurer le fichier framework2.properties
JDBC_CONNECT_STRING=jdbc:oracle:thin:@LPBDDEDT4:2483/PTECEDT2
JDBC_USER=VBO
JDBC_PASSWORD=ulonly
 Lancer le test d'intégration avec le VM paramètre :
 -DtestIT.commit=true
 Requêtes de MAJ
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('SGE01_0085',0,1002,0,'SGE01_0085','SGE01_0085','java.lang.String','SGE01_0085',NULL,NULL,'21-06-2017','DEV001|DEV|test',NULL,NULL);
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('VBO_CESS_SOUS_TYPE_AFF',0,1001,0,'CESSATION_SERVICE_MESURE','accueil','com.hermes.ref.affaireaction.type.EPSousTypeAffaire','ACCUEIL',NULL,NULL,'23-06-2017','DEV001|DEV|test',NULL,NULL);
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('SGE01_0104',0,1002,0,'SGE01_0104','SGE01_0104','java.lang.String','SGE01_0104',NULL,NULL,'21-06-2017','DEV001|DEV|test',NULL,NULL);
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('SGE01_0039',0,1002,0,'SGE01_0039','SGE01_0039','java.lang.String','SGE01_0039',NULL,NULL,'21-06-2017','DEV001|DEV|test',NULL,NULL);
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('52003VBO',0,1002,0,'SGE01_0038','SGE01_0038','java.lang.String','SGE01_0038',NULL,NULL,'21-06-2017','DEV001|DEV|test',NULL,NULL);
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('52001VBO',0,1002,0,'SGE01_0048','SGE01_0048','java.lang.String','SGE01_0048',NULL,NULL,'21-06-2017','DEV001|DEV|test',NULL,NULL);
insert into TCODIFICATION (ID, ETATOBJET, TYPECODIFICATION, VALEURPARDEFAUTDECODIFICATION, CODEEXTERNE, LIBELLEEXTERNE, TYPE_CODIFIE, VALEUR, DATEMODIFICATION, ACTEURMODIFICATION, DATECREATION, ACTEURCREATION, DATESUPPRESSION, ACTEURSUPPRESSION) values ('52000VBO',0,1002,0,'SW_0999','SW_0999','java.lang.String','SW_0999',NULL,NULL,'21-06-2017','DEV001|DEV|test',NULL,NULL);
update tacteur set referenceexterne= 'TRISTAN002' where id='2897995122';
update tacteur set EMAIL='v-bouthinon@uem-metz.fr' where id='2897995122';
insert into tpersonnephysique (id) values ('2897995122');
update tservicesouscrit set statut=1, dateeffet=current_date - 1  where reference = 270719773010123;
 Flux de la demande :
<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:v3="http://www.erdf.fr/ginko/calculerrecevabilitemodificationmesures/v3">
   <soapenv:Header/>
   <soapenv:Body>
      <v3:calculerRecevabiliteModificationMesures>
         <demande>
            <donneesGenerales>
               <refFrn>Test AME</refFrn>
               <refFrnRegroupement>Test AME</refFrnRegroupement>
               <objetCode>AME</objetCode>
               <pointId>77770000007777</pointId>
               <affaireOrigineId>1234569</affaireOrigineId>
               <dateHeure>2017-05-07T14:42:28</dateHeure>
               <mediaCode>B2B</mediaCode>
               <initiateur>
                  <loginUtilisateur>tel@energem.fr</loginUtilisateur>
               </initiateur>
               <dateEffetSouhaitee>2017-06-15</dateEffetSouhaitee>
               <canalCode>PCLI</canalCode>
               <acteurCrmId>TRISTAN002</acteurCrmId>
            </donneesGenerales>
            <cessationServiceMesures>
               <serviceSouscritId>270719773010123</serviceSouscritId>
               <dateFin>2017-11-29</dateFin>
               <declarationConsentement>
                  <autorisation>false</autorisation>
                  <personnePhysique>
                     <civilite>M</civilite>
                     <nom>Bouthinon</nom>
                     <prenom>vincent</prenom>
                  </personnePhysique>
               </declarationConsentement>
               <usageDistributeurCode>AZERTY</usageDistributeurCode>
            </cessationServiceMesures>
         </demande>
      </v3:calculerRecevabiliteModificationMesures>
   </soapenv:Body>
</soapenv:Envelope>

 Débug et tests 
 Simuler Batch 997 en déport depuis file JMS 
package com.efluid.itf.ref.edl.businessprocess;

import static com.imrglobal.framework.factory.BusinessIdFactory.getInstance;

import org.junit.Test;

import com.hermes.arc.commun.businessprocess.EfluidBusinessProcess;

import com.hermes.itv.intervention.businessobject.*;
import com.hermes.ref.workflow.batch.businessprocess.TraitementWorkflowDeporteMDBProcess;
import com.hermes.ref.workflow.batch.utils.WorkflowBatchUtils;
import com.hermes.ref.workflow.businessprocess.WorkflowGestionSuiviProcess;
import com.hermes.ref.workflow.context.WorkflowContext;
import com.hermes.ref.workflow.type.EStatutTraitementWorkflow;

import test.hermes.arc.commun.test.AbstractTestIT;

public class TestITTraitementLeverConcurrenceDemandeCessation247134 extends AbstractTestIT {

  @Test
  public void traiterDonneesMessage() {
    WorkflowContext contexte = new WorkflowContext();
    ElementDePopulationInterventionWorkflow edp = (ElementDePopulationInterventionWorkflow) new EfluidBusinessProcess().actionOpen(new ElementDePopulationInterventionWorkflow((getInstance("hs4If"))));
    TraitementCRIAutomatique traitement = (TraitementCRIAutomatique) new EfluidBusinessProcess().actionOpen(new TraitementCRIAutomatique(getInstance("$d$41210")));
    TestTraitementWorkflowDeporteMDBProcess traitementWorkflowDeporteMDBProcess = new TestTraitementWorkflowDeporteMDBProcess();

    contexte.setElementDePopulationWorkflow(edp);
    contexte.setTraitementExecutionEtape(traitement);

    WorkflowGestionSuiviProcess workflowGestionSuiviProcess = new WorkflowGestionSuiviProcess();
    workflowGestionSuiviProcess.startSuiviTraitement(contexte);
    workflowGestionSuiviProcess.getSuivi().setObjetSuivi(edp);
    workflowGestionSuiviProcess.getSuivi().setStatutTraitementWorkflow(EStatutTraitementWorkflow.DEPORTE_ATTENTE);
    new EfluidBusinessProcess().actionSaveIfNotStored(workflowGestionSuiviProcess.getSuivi());

    contexte.setStatutSuivi(EStatutTraitementWorkflow.DEPORTE_ATTENTE);
    contexte.setTraitementExecutionEtape(traitement);
    contexte.setCommentaireSuivi(WorkflowBatchUtils.getCommentaireSuiviDeporte(traitement.getEtape(), true));

    traitementWorkflowDeporteMDBProcess.appelerTraiterDonneesMessage(contexte);
  }

  class TestTraitementWorkflowDeporteMDBProcess extends TraitementWorkflowDeporteMDBProcess {

    public void appelerTraiterDonneesMessage(WorkflowContext ctx) {
      traiterDonneesMessage(null, ctx);
    }
  }
}

select * from TESPACEDELIVRAISON where reference='1200727399';
select * from taffaire where reference='1000109'; --hs4Ia
select edp.STATUT_ID from TELEMENTDEPOPULATIONWKF edp where edp.OBJETTRAITE_ID='hs4Ia' and lot_id='45992468151'; --$d$41110
select * from tlot where id='45992468151';
select * from TTRAITEMENTEXECUTIONETAPE  where role = 'com.hermes.itv.intervention.businessobject.TraitementCRIAutomatique' and etatobjet=0 and ETAPEDUTRAITEMENTUNITAIRE_ID= '$d$41110' -- $d$41210

 Créer une demande de prestation à la volée 
INSERT INTO tdemandeprestation (id, etatobjet, COMMANDEDEMANDEPRESTATION_ID, CODEGRD, CODEFOURNISSEUR, idaffaire, NATUREPROVISOIRE, MOTIFANNULATION) VALUES ('1',0, '1','G-UEM', 'F-UEM', '1', '1' ,'ERRSAISIE');
INSERT INTO tcommandedemandeprestation (id, etatobjet, SOLDERACCORDEMENTPAYEPMS, RACCORDEMENTPROVISOIRE, INTERVENANTRACCORDEMENTPROVIS, MISEENSERVICECOURTEDUREE) VALUES ('1',0, 1, 1, 1, 1);

 Tests unitaires 
mvn clean install -pl :efluid-interfaces -Ptest-unitaire
 Tests d'intégrations 
 Exécuter tous les tests 
mvn clean integration-test -pl :efluid-interfaces -Ptest-integration -DefluidTestsReuseForks=false -DefluidTestsForkCount=1 -DefluidTestsForkMode=always -DuseLocalProperties2=true

 Exécuter un seul test 
mvn clean install -pl :efluid-interfaces -Ptest-integration -DefluidTestsReuseForks=false -DefluidTestsForkCount=1 -DuseLocalProperties2=true -Dit.test=TestITSouscrireContrat
ou 
mvn -f app/pom.xml failsafe:integration-test -Dit.test="TestITGestionViaHibernateJourOuvre" -Ptest-integration

Pour faire fonctionner mes tests sur la BDD des tests avec une application 12.10.*
-- Supression des anomalies suivantes (ne as oublier de les rajouter ensuite) 
update TPARAMTYPEANOMALIE
set etatobjet=0
where id in ('WKFPERMOB001', 'WKFPERMOB002', 'WKFPERMOB003', 'WKFPERMOB004');

update TPARAMETRE
set etatobjet=3
where code='modeExport';

update TVALEURDEPARAMETRE
set etatobjet=3
where parametre_id='modeExport';

 SQL 
 Procédure_livraison_d'un_script_ponctuel
 Script SQL SGE

 Paramétrage du workflow 

 Localiser un traitement de nature d'après son code erreur 

SELECT WKF.LIBELLE                                   AS WORKFLOW,
       ETP.NUMEROETAPE || ' - ' || ETP.LIBELLE       AS ETAPE,
       NAT.CODE                                      AS NATURE,
       ACT.PRIORITEEXECUTION || ' - ' || ACT.LIBELLE AS TRAITEMENT
FROM TNATUREACTION NAT
JOIN TCVCNATURECONTEXTE CVC ON CVC.NATUREACTION_ID = NAT.ID
JOIN TACTIONPREDEFINIE ACT ON ACT.CVCNATURECONTEXTE_ID = CVC.ID
JOIN TTRAITEMENTDEMANDEPRESTATION TDP ON TDP.ID = ACT.ID
JOIN TTRAITEMENTEXECUTIONETAPE EXE ON DBMS_LOB.SUBSTR(EXE.COMPLEMENTJSON, 4000) = '{"natureAction":"' || NAT.ID || '|com.efluid.itf.crm.demandeprestation.businessobject.NaturePrestationControleRecevabilite"}'
JOIN TETAPEWORKFLOW ETP ON ETP.ID = EXE.ETAPEDUTRAITEMENTINIT
JOIN TWORKFLOW WKF ON WKF.ID = ETP.WORKFLOW_ID
WHERE EXE.ROLE = 'com.hermes.ref.workflow.businessobject.TraitementExecuterNatureAction'
AND EXE.ETATOBJET = 0
AND TDP.CODEERREUR = 'SGE01_XXXX';

 Localiser un traitement workflow d'après son code erreur 

SELECT WKF.LIBELLE                             AS WORKFLOW,
       ETP.NUMEROETAPE || ' - ' || ETP.LIBELLE AS ETAPE,
       EXE.PRIORITEEXECUTION                   AS PRIORITE,
       EXE.LIBELLE                             AS TRAITEMENT
FROM TTRAITEMENTEXECUTIONETAPE EXE
JOIN TETAPEWORKFLOW ETP ON ETP.ID = EXE.ETAPEDUTRAITEMENTINIT
JOIN TWORKFLOW WKF ON WKF.ID = ETP.WORKFLOW_ID
WHERE EXE.ETATOBJET = 0
AND EXE.COMPLEMENTJSON LIKE '%"codeErreur":"SGE01_XXXX"%';

 Liens 
 Web service
 schéma 
V13 : Fichier:SGE.png
V12 : 1000px<!!!>Category:domaine
Category:architecture
Category:pole composants transverses

composant Framework
composant Architecture

 Documentation générale 
 écarts du domaine<!!!>Category:domaine
Category:application
Category:edoc
__NOTOC__
L'application edoc est basée sur le projet Open Source Solr.
Elle est divisée en 3 parties :
 Un batch EDC001MT
 Une base de données d'indexes edoc-solr
 Un connecteur efluid ↔ edoc intégré à l'application efluid

400px|néant|vignette|Schéma d'architecture d'edoc 2.X
 versions d'edoc 

le bon de livraison est documenté dans les sources.

Le détail des évènements est disponible sous suivefluid et dans le  bon de livraison. (Version minimale en production 2.3.0)

 documents 
 documents disponible sous eRoom :
 efluid - Documentation Technique > Projets clients > UEM > 010 - dossier d'architecture technique > edoc
 dtr_ArchitectureEdocsUEM.doc (dossier d'architecture technique pour le client uem)
 efluid - Conduite du changement > 02 - formation > 03 - formation interne > contenus > 02 - autres modules > portail / transverse
 présentation efluid_edoc_Archives.pptx (présentation fonctionnelle d'edoc)
 présentation efluid_edoc_Archives_Technique.pptx (présentation orienté technique d'edoc)
 efluid – Analyse des performances > edoc - solr
 tests_performance_EDC001MT.docx
 efluid - Documentation Fonctionnelle Suite efluid > Guides Utilisateur > edoc
 Guide utilisateur application edoc.docx
 Guide utilisateur paramétrage edoc.docx
 documents d'exploitation : 
 documentation\docExploitationBatchs\edoc sous git avec le DEX du batch (en cours de migration vers l'installeur ; évènement )
 aide-mémoire requête edoc-solr

 Mise à jour de solr 

 déposer la nouvelle archive sous artifactory dans https://eartifact.efluid.uem.lan/artifactory/webapp/#/artifacts/browse/tree/General/ext-release-local 
 générer un nouvel rpm, par exemple https://gerrit.efluid.uem.lan/c/etools/+/191855
 mettre à jour la dépendance, par exemple https://gerrit.efluid.uem.lan/c/edoc/+/193238<!!!>Category:domaine
Category:composants batchs
Category:pole composants transverses<!!!>Domaine correspondant à la gestion des risques/habilitations présents dans le code Efluid. Ces habilitations permettent de donner l'accès à certain type d'utilisateur (rôle) en fonction des besoins métiers. 
La mise en place de ces habilitations permet de limiter les failles de sécurité selon lesquelles un utilisateur malveillant pourrait accéder à certaines actions ou page sans avoir pour autant les droits.

Category:domaine
Category:Habilitations
Category:pole composants transverses<!!!>category:Domaine

Page recensant différentes documentations plus ou moins spécifiques au domaine consommation. 

La page Domaine consommation permet notamment de faire un lien entre ces différentes pages.

A compléter nécessairement lors de mise à jour de l'application.<!!!>category:Domaine

Page recensant différentes documentations plus ou moins spécifiques au domaine Intervention. 
La page Domaine Intervention permet ainsi de faire un lien entre ces différentes pages en précisant aussi certains points tels que les lieux de stockages des documents.

A compléter nécessairement lors de mise à jour de l'application.<!!!>